// Sentinel Hub API Server
// Central server for document processing, metrics, and organization management

package main

import (
	"bytes"
	"context"
	"crypto/md5"
	"crypto/rand"
	"crypto/sha256"
	"database/sql"
	"encoding/hex"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"math"
	"net"
	"net/http"
	"os"
	"os/exec"
	"os/signal"
	"path/filepath"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"sync"
	"syscall"
	"time"
	"unicode"

	"sentinel-hub-api/models"
	"sentinel-hub-api/pkg/security"

	"github.com/go-chi/chi/v5"
	"github.com/go-chi/chi/v5/middleware"
	"github.com/go-chi/cors"
	"github.com/google/uuid"
	"github.com/lib/pq"
	_ "github.com/lib/pq"
)

// JSON type aliases for flexible JSON handling
type JSONMap map[string]interface{}
type JSONSlice []interface{}

// Error types are defined in error_handler.go

type NotImplementedError struct {
	Feature  string `json:"feature"`
	Message  string `json:"message"`
	Resource string `json:"resource,omitempty"`
}

func (e *NotImplementedError) Error() string {
	if e.Message != "" {
		return e.Message
	}
	return fmt.Sprintf("feature not implemented: %s", e.Feature)
}

type RateLimitError struct {
	Message    string    `json:"message"`
	Limit      int       `json:"limit"`
	Remaining  int       `json:"remaining"`
	ResetTime  time.Time `json:"reset_time"`
	RetryAfter int       `json:"retry_after"`
}

// isCriticalError classifies errors that should fail fast
func isCriticalError(err error) bool {
	if err == nil {
		return false
	}
	errStr := strings.ToLower(err.Error())
	// Classify errors that should fail fast
	criticalPatterns := []string{
		"database connection",
		"authentication failed",
		"permission denied",
		"unauthorized",
		"forbidden",
	}
	for _, pattern := range criticalPatterns {
		if strings.Contains(errStr, pattern) {
			return true
		}
	}
	return false
}

// =============================================================================
// CONTEXT KEYS
// =============================================================================

// contextKey is a custom type for context keys to prevent collisions
type contextKey string

const (
	requestIDKey contextKey = "request_id"
	projectKey   contextKey = "project"
)

// =============================================================================
// CONFIGURATION
// =============================================================================

type Config struct {
	Port            string
	DatabaseURL     string
	DocumentStorage string
	JWTSecret       string
	OllamaHost      string
	CORSOrigin      string
	HubURL          string
	BinaryStorage   string
	RulesStorage    string
	AdminAPIKey     string
}

func loadConfig() *Config {
	corsOrigin := getEnv("CORS_ORIGIN", "*")

	// Validate CORS origin - warn if using wildcard in production
	if corsOrigin == "*" {
		env := getEnv("ENVIRONMENT", "development")
		if env == "production" {
			log.Printf("⚠️  WARNING: CORS_ORIGIN is set to '*' in production. This is insecure.")
			log.Printf("   Recommendation: Set CORS_ORIGIN to specific allowed origins.")
		}
	} else {
		// Validate origin format (basic check)
		if !strings.HasPrefix(corsOrigin, "http://") && !strings.HasPrefix(corsOrigin, "https://") {
			log.Printf("⚠️  WARNING: CORS_ORIGIN '%s' does not appear to be a valid URL. Expected http:// or https://", corsOrigin)
		}
	}

	return &Config{
		Port:            getEnv("PORT", "8080"),
		DatabaseURL:     getEnv("DATABASE_URL", "postgres://sentinel:sentinel@localhost:5432/sentinel?sslmode=disable"),
		DocumentStorage: getEnv("DOCUMENT_STORAGE", "/data/documents"),
		JWTSecret:       getEnv("JWT_SECRET", "change-me-in-production"),
		OllamaHost:      getEnv("OLLAMA_HOST", "http://localhost:11434"),
		CORSOrigin:      corsOrigin,
		HubURL:          getEnv("HUB_URL", "http://localhost:8080"),
		BinaryStorage:   getEnv("BINARY_STORAGE", "/data/binaries"),
		RulesStorage:    getEnv("RULES_STORAGE", "/data/rules"),
		AdminAPIKey:     getEnv("ADMIN_API_KEY", ""),
	}
}

// validateProductionConfig validates production configuration and fails startup if insecure defaults detected
func validateProductionConfig(config *ServerConfig) {
	env := getEnv("ENVIRONMENT", "development")
	if env != "production" {
		return // Skip validation in non-production
	}

	var errors []string

	// Check CORS
	if config.CORSOrigin == "*" {
		errors = append(errors, "CORS_ORIGIN cannot be '*' in production")
	}

	// Check JWT Secret
	if config.JWTSecret == "change-me-in-production" {
		errors = append(errors, "JWT_SECRET must be changed from default value")
	}

	// Check Database SSL
	if strings.Contains(config.DatabaseURL, "sslmode=disable") {
		errors = append(errors, "Database connection must use SSL (sslmode=require) in production")
	}

	// Check for default password in connection string
	if strings.Contains(config.DatabaseURL, "sentinel:sentinel@") {
		errors = append(errors, "Database password must not be default 'sentinel' in production")
	}

	if len(errors) > 0 {
		log.Fatalf("❌ PRODUCTION CONFIGURATION ERRORS:\n%s\n\nPlease fix these issues before starting in production mode.", strings.Join(errors, "\n"))
	}

	log.Println("✅ Production configuration validated")
}

// =============================================================================
// TIMEOUT CONSTANTS AND HELPERS
// =============================================================================

// Timeout constants
// Deprecated: Use GetConfig().Timeouts.Query instead
func getQueryTimeout() time.Duration {
	return GetConfig().Timeouts.Query
}

// Deprecated: Use GetConfig().Timeouts.Analysis instead
func getAnalysisTimeout() time.Duration {
	return GetConfig().Timeouts.Analysis
}

// Deprecated: Use GetConfig().Timeouts.Context instead
var DefaultContextTimeout = GetConfig().Timeouts.Context

// Deprecated: Use GetConfig().Timeouts.HTTP instead
var DefaultHTTPTimeout = GetConfig().Timeouts.HTTP

// =============================================================================
// DATABASE QUERY HELPERS
// =============================================================================

// Database query helpers with timeout
func queryWithTimeout(ctx context.Context, query string, args ...interface{}) (*sql.Rows, error) {
	timeout := getQueryTimeout()
	ctx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()
	return db.QueryContext(ctx, query, args...)
}

func queryRowWithTimeout(ctx context.Context, query string, args ...interface{}) *sql.Row {
	timeout := getQueryTimeout()
	ctx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()
	return db.QueryRowContext(ctx, query, args...)
}

func execWithTimeout(ctx context.Context, query string, args ...interface{}) (sql.Result, error) {
	timeout := getQueryTimeout()
	ctx, cancel := context.WithTimeout(ctx, timeout)
	defer cancel()
	return db.ExecContext(ctx, query, args...)
}

// =============================================================================
// REQUEST ID MIDDLEWARE
// =============================================================================

func requestIDMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		requestID := uuid.New().String()
		ctx := context.WithValue(r.Context(), requestIDKey, requestID)
		w.Header().Set("X-Request-ID", requestID)
		next.ServeHTTP(w, r.WithContext(ctx))
	})
}

// =============================================================================
// CONTEXT HELPER FUNCTIONS
// =============================================================================

// getOrgFromContext extracts organization from context (stub)
func getOrgFromContext(ctx context.Context) (*models.Organization, error) {
	orgID := ctx.Value("org_id")
	if orgID == nil {
		return nil, fmt.Errorf("organization not found in context")
	}
	return &models.Organization{
		ID:   orgID.(string),
		Name: "Default Organization",
	}, nil
}

// getProjectFromContext safely retrieves project from context
// Returns error if project is missing or invalid (should never happen in protected routes)
func getProjectFromContext(ctx context.Context) (*Project, error) {
	value := ctx.Value(projectKey)
	if value == nil {
		return nil, fmt.Errorf("project not found in context")
	}
	project, ok := value.(*Project)
	if !ok || project == nil {
		return nil, fmt.Errorf("invalid project type in context")
	}
	return project, nil
}

// requireProjectContext validates and returns project from context, writing error response if missing
func requireProjectContext(w http.ResponseWriter, r *http.Request) (*Project, error) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Project context required but missing")
		WriteErrorResponse(w, &ValidationError{
			Field:   "authorization",
			Message: "Project context required",
			Code:    "unauthorized",
		}, http.StatusUnauthorized)
		return nil, err
	}
	return project, nil
}

// getRequestIDFromContext safely retrieves request ID from context
func getRequestIDFromContext(ctx context.Context) string {
	if requestID, ok := ctx.Value(requestIDKey).(string); ok && requestID != "" {
		return requestID
	}
	return "unknown"
}

// =============================================================================
// MODELS
// =============================================================================

type Organization struct {
	ID        string    `json:"id"`
	Name      string    `json:"name"`
	CreatedAt time.Time `json:"created_at"`
}

type Project struct {
	ID        string    `json:"id"`
	OrgID     string    `json:"org_id"`
	Name      string    `json:"name"`
	APIKey    string    `json:"api_key,omitempty"`
	CreatedAt time.Time `json:"created_at"`
}

type Document struct {
	ID            string     `json:"id"`
	ProjectID     string     `json:"project_id"`
	Name          string     `json:"name"`
	OriginalName  string     `json:"original_name"`
	Size          int64      `json:"size"`
	MimeType      string     `json:"mime_type"`
	Status        string     `json:"status"` // queued, processing, completed, failed
	Progress      int        `json:"progress"`
	FilePath      string     `json:"-"`
	ExtractedText string     `json:"extracted_text,omitempty"`
	Error         string     `json:"error,omitempty"`
	CreatedAt     time.Time  `json:"created_at"`
	ProcessedAt   *time.Time `json:"processed_at,omitempty"`
}

// =============================================================================
// Phase 16: Organization Features
// =============================================================================

type Team struct {
	ID          string    `json:"id"`
	OrgID       string    `json:"org_id"`
	Name        string    `json:"name"`
	Description string    `json:"description,omitempty"`
	LeadUserID  string    `json:"lead_user_id,omitempty"`
	Settings    JSONMap   `json:"settings,omitempty"`
	CreatedAt   time.Time `json:"created_at"`
	UpdatedAt   time.Time `json:"updated_at"`
}

type TeamMember struct {
	ID      string    `json:"id"`
	TeamID  string    `json:"team_id"`
	UserID  string    `json:"user_id"`
	Role    string    `json:"role"` // lead, senior, member
	AddedBy string    `json:"added_by,omitempty"`
	AddedAt time.Time `json:"added_at"`
}

type RegisteredAgent struct {
	ID           string    `json:"id"`
	OrgID        string    `json:"org_id,omitempty"`
	TeamID       string    `json:"team_id,omitempty"`
	AgentID      string    `json:"agent_id"`
	Name         string    `json:"name"`
	Version      string    `json:"version,omitempty"`
	Platform     string    `json:"platform,omitempty"`
	LastSeen     time.Time `json:"last_seen"`
	Status       string    `json:"status"` // active, inactive, suspended
	Capabilities JSONSlice `json:"capabilities,omitempty"`
	Settings     JSONMap   `json:"settings,omitempty"`
	CreatedAt    time.Time `json:"created_at"`
}

// =============================================================================
// Phase 14E: Task Dependency Management System
// =============================================================================

type DependencyChange struct {
	ID                string                 `json:"id"`
	TaskID            string                 `json:"task_id"`
	DependencyID      string                 `json:"dependency_id,omitempty"`
	ChangeType        string                 `json:"change_type"`
	AffectedChain     []string               `json:"affected_chain"`
	RippleEffects     map[string]interface{} `json:"ripple_effects"`
	CycleRisk         bool                   `json:"cycle_risk"`
	PerformanceImpact string                 `json:"performance_impact"`
	ChangedAt         time.Time              `json:"changed_at"`
}

type APIVersion struct {
	ID              string     `json:"id"`
	Version         string     `json:"version"`
	Status          string     `json:"status"` // active, deprecated, sunset
	SunsetDate      *time.Time `json:"sunset_date,omitempty"`
	DeprecatedBy    string     `json:"deprecated_by,omitempty"`
	Changelog       string     `json:"changelog,omitempty"`
	BreakingChanges bool       `json:"breaking_changes"`
	MigrationGuide  string     `json:"migration_guide,omitempty"`
	CreatedAt       time.Time  `json:"created_at"`
	UpdatedAt       time.Time  `json:"updated_at"`
}

type APIEndpoint struct {
	ID              string        `json:"id"`
	Path            string        `json:"path"`
	Method          string        `json:"method"`
	Version         string        `json:"version"`
	Description     string        `json:"description"`
	Deprecated      bool          `json:"deprecated"`
	DeprecatedIn    string        `json:"deprecated_in,omitempty"`
	SunsetDate      *time.Time    `json:"sunset_date,omitempty"`
	ReplacementPath string        `json:"replacement_path,omitempty"`
	MigrationNotes  string        `json:"migration_notes,omitempty"`
	Tags            []string      `json:"tags,omitempty"`
	Parameters      []APIParam    `json:"parameters,omitempty"`
	Responses       []APIResponse `json:"responses,omitempty"`
	CreatedAt       time.Time     `json:"created_at"`
	UpdatedAt       time.Time     `json:"updated_at"`
}

type APIParam struct {
	Name        string `json:"name"`
	Type        string `json:"type"`
	Required    bool   `json:"required"`
	Description string `json:"description"`
	Location    string `json:"location"` // query, path, body, header
}

type APIResponse struct {
	StatusCode  int                    `json:"status_code"`
	Description string                 `json:"description"`
	Schema      map[string]interface{} `json:"schema,omitempty"`
}

type VersionMigration struct {
	ID                  string                 `json:"id"`
	FromVersion         string                 `json:"from_version"`
	ToVersion           string                 `json:"to_version"`
	EndpointPath        string                 `json:"endpoint_path"`
	Method              string                 `json:"method"`
	MigrationType       string                 `json:"migration_type"`
	TransformationRules map[string]interface{} `json:"transformation_rules,omitempty"`
	BackwardCompatible  bool                   `json:"backward_compatible"`
	CreatedAt           time.Time              `json:"created_at"`
}

type APIDeprecationWarning struct {
	Version         string    `json:"version"`
	SunsetDate      time.Time `json:"sunset_date"`
	ReplacementPath string    `json:"replacement_path,omitempty"`
	Message         string    `json:"message"`
	MigrationGuide  string    `json:"migration_guide,omitempty"`
}

type VersionCompatibilityReport struct {
	FromVersion     string             `json:"from_version"`
	ToVersion       string             `json:"to_version"`
	Compatibility   string             `json:"compatibility"` // full, partial, breaking
	BreakingChanges []string           `json:"breaking_changes,omitempty"`
	Migrations      []VersionMigration `json:"migrations,omitempty"`
	Recommendations []string           `json:"recommendations,omitempty"`
	GeneratedAt     time.Time          `json:"generated_at"`
}

// APIEndpoint struct is already defined above
// APIParam and APIResponse are already defined above

// APIDocumentation represents complete API documentation
type APIDocumentation struct {
	Title       string        `json:"title"`
	Version     string        `json:"version"`
	Description string        `json:"description"`
	BaseURL     string        `json:"base_url"`
	Endpoints   []APIEndpoint `json:"endpoints"`
	Versions    []APIVersion  `json:"versions"`
	Security    []APISecurity `json:"security"`
	GeneratedAt time.Time     `json:"generated_at"`
}

// APISecurity represents API security information
type APISecurity struct {
	Type        string `json:"type"`
	Description string `json:"description"`
	Scheme      string `json:"scheme"`
}

// RateLimit represents rate limiting configuration
type RateLimit struct {
	Requests int           `json:"requests"`
	Window   time.Duration `json:"window"`
	Burst    int           `json:"burst"`
}

// RateLimitInfo represents current rate limit status
type RateLimitInfo struct {
	Limit      int       `json:"limit"`
	Remaining  int       `json:"remaining"`
	ResetTime  time.Time `json:"reset_time"`
	WindowSize string    `json:"window_size"`
}

// CacheEntry represents a cached response
type CacheEntry struct {
	Key        string
	Response   []byte
	Headers    map[string]string
	StatusCode int
	CreatedAt  time.Time
	ExpiresAt  time.Time
	ETag       string
}

// APIStats represents API usage statistics
type APIStats struct {
	TotalRequests    int64            `json:"total_requests"`
	RequestsByMethod map[string]int64 `json:"requests_by_method"`
	RequestsByPath   map[string]int64 `json:"requests_by_path"`
	ErrorsByType     map[string]int64 `json:"errors_by_type"`
	ResponseTimeAvg  float64          `json:"response_time_avg"`
	ResponseTimeP95  float64          `json:"response_time_p95"`
	ResponseTimeP99  float64          `json:"response_time_p99"`
	TimeRange        string           `json:"time_range"`
}

// =============================================================================
// Phase 4: Cross-Repository Analysis
// =============================================================================

type Repository struct {
	ID            string     `json:"id"`
	OrgID         string     `json:"org_id"`
	Name          string     `json:"name"`
	FullName      string     `json:"full_name"`
	Description   string     `json:"description,omitempty"`
	URL           string     `json:"url,omitempty"`
	CloneURL      string     `json:"clone_url,omitempty"`
	SSHURL        string     `json:"ssh_url,omitempty"`
	DefaultBranch string     `json:"default_branch"`
	Language      string     `json:"language,omitempty"`
	SizeBytes     int64      `json:"size_bytes,omitempty"`
	StarsCount    int        `json:"stars_count"`
	ForksCount    int        `json:"forks_count"`
	WatchersCount int        `json:"watchers_count"`
	IsPrivate     bool       `json:"is_private"`
	IsArchived    bool       `json:"is_archived"`
	IsTemplate    bool       `json:"is_template"`
	IsFork        bool       `json:"is_fork"`
	ParentRepoID  string     `json:"parent_repo_id,omitempty"`
	CreatedAt     time.Time  `json:"created_at"`
	UpdatedAt     time.Time  `json:"updated_at"`
	LastSyncedAt  *time.Time `json:"last_synced_at,omitempty"`
	SyncStatus    string     `json:"sync_status"`
}

type RepositoryRelationship struct {
	ID               string                 `json:"id"`
	SourceRepoID     string                 `json:"source_repo_id"`
	TargetRepoID     string                 `json:"target_repo_id"`
	RelationshipType string                 `json:"relationship_type"`
	Strength         float64                `json:"strength"`
	Metadata         map[string]interface{} `json:"metadata,omitempty"`
	DiscoveredAt     time.Time              `json:"discovered_at"`
	LastUpdated      time.Time              `json:"last_updated"`
	ConfidenceScore  float64                `json:"confidence_score"`
}

type CrossRepoDependency struct {
	ID                string    `json:"id"`
	SourceRepoID      string    `json:"source_repo_id"`
	TargetRepoID      string    `json:"target_repo_id"`
	DependencyType    string    `json:"dependency_type"`
	PackageName       string    `json:"package_name,omitempty"`
	VersionConstraint string    `json:"version_constraint,omitempty"`
	SourceFilePath    string    `json:"source_file_path,omitempty"`
	SourceLineNumber  int       `json:"source_line_number,omitempty"`
	ConfidenceScore   float64   `json:"confidence_score"`
	LastDetected      time.Time `json:"last_detected"`
	IsActive          bool      `json:"is_active"`
}

type RepositoryNetwork struct {
	Repositories     map[string]*Repository   `json:"repositories"`
	Relationships    []RepositoryRelationship `json:"relationships"`
	Dependencies     []CrossRepoDependency    `json:"dependencies"`
	CentralityScores map[string]float64       `json:"centrality_scores,omitempty"`
	Clusters         [][]string               `json:"clusters,omitempty"`
	IsConnected      bool                     `json:"is_connected"`
	AnalysisDate     time.Time                `json:"analysis_date"`
}

type CrossRepoImpactAnalysis struct {
	ID                   string                `json:"id"`
	RepositoryID         string                `json:"repository_id"`
	ChangeDescription    string                `json:"change_description"`
	AffectedRepositories []RepositoryImpact    `json:"affected_repositories"`
	DependencyChain      []CrossRepoDependency `json:"dependency_chain"`
	RiskLevel            string                `json:"risk_level"`
	RiskFactors          []string              `json:"risk_factors"`
	MitigationStrategies []string              `json:"mitigation_strategies"`
	EstimatedImpactTime  int                   `json:"estimated_impact_time_days"`
	ConfidenceScore      float64               `json:"confidence_score"`
	AnalyzedAt           time.Time             `json:"analyzed_at"`
}

type RepositoryImpact struct {
	RepositoryID    string   `json:"repository_id"`
	RepositoryName  string   `json:"repository_name"`
	ImpactType      string   `json:"impact_type"`
	Severity        string   `json:"severity"`
	Description     string   `json:"description"`
	PropagationPath []string `json:"propagation_path,omitempty"`
	TimeImpact      int      `json:"time_impact_days"`
	Confidence      float64  `json:"confidence"`
}

type CrossRepoAnalysisJob struct {
	ID                 string                 `json:"id"`
	AnalysisName       string                 `json:"analysis_name"`
	Description        string                 `json:"description,omitempty"`
	RepositoryIDs      []string               `json:"repository_ids"`
	AnalysisType       string                 `json:"analysis_type"`
	Parameters         map[string]interface{} `json:"parameters,omitempty"`
	Status             string                 `json:"status"`
	ProgressPercentage float64                `json:"progress_percentage"`
	StartedAt          *time.Time             `json:"started_at,omitempty"`
	CompletedAt        *time.Time             `json:"completed_at,omitempty"`
	DurationMs         int                    `json:"duration_ms,omitempty"`
	CreatedBy          string                 `json:"created_by,omitempty"`
	CreatedAt          time.Time              `json:"created_at"`
	ResultSummary      map[string]interface{} `json:"result_summary,omitempty"`
	DetailedResults    map[string]interface{} `json:"detailed_results,omitempty"`
}

type OrganizationPattern struct {
	ID          string    `json:"id"`
	OrgID       string    `json:"org_id"`
	Name        string    `json:"name"`
	Description string    `json:"description,omitempty"`
	PatternType string    `json:"pattern_type"`
	PatternData JSONMap   `json:"pattern_data"`
	IsShared    bool      `json:"is_shared"`
	CreatedBy   string    `json:"created_by,omitempty"`
	CreatedAt   time.Time `json:"created_at"`
	UpdatedAt   time.Time `json:"updated_at"`
}

type PatternDistribution struct {
	ID            string     `json:"id"`
	PatternID     string     `json:"pattern_id"`
	TargetType    string     `json:"target_type"` // agent, team, project
	TargetID      string     `json:"target_id"`
	Status        string     `json:"status"` // pending, distributed, failed
	DistributedAt *time.Time `json:"distributed_at,omitempty"`
	ErrorMessage  string     `json:"error_message,omitempty"`
	CreatedAt     time.Time  `json:"created_at"`
}

type OrganizationAlert struct {
	ID          string     `json:"id"`
	OrgID       string     `json:"org_id"`
	AlertType   string     `json:"alert_type"`
	Severity    string     `json:"severity"` // low, medium, high, critical
	Title       string     `json:"title"`
	Message     string     `json:"message"`
	ContextData JSONMap    `json:"context_data,omitempty"`
	IsRead      bool       `json:"is_read"`
	ReadBy      string     `json:"read_by,omitempty"`
	ReadAt      *time.Time `json:"read_at,omitempty"`
	Resolved    bool       `json:"resolved"`
	ResolvedBy  string     `json:"resolved_by,omitempty"`
	ResolvedAt  *time.Time `json:"resolved_at,omitempty"`
	CreatedAt   time.Time  `json:"created_at"`
}

type AlertRule struct {
	ID         string    `json:"id"`
	OrgID      string    `json:"org_id"`
	RuleName   string    `json:"rule_name"`
	RuleType   string    `json:"rule_type"`
	Conditions JSONMap   `json:"conditions"`
	Actions    JSONMap   `json:"actions"`
	IsActive   bool      `json:"is_active"`
	CreatedBy  string    `json:"created_by,omitempty"`
	CreatedAt  time.Time `json:"created_at"`
	UpdatedAt  time.Time `json:"updated_at"`
}

// KnowledgeItem is defined in types.go

type ProcessingStage struct {
	Name     string `json:"name"`
	Status   string `json:"status"` // pending, processing, completed, failed
	Duration int    `json:"duration_ms,omitempty"`
	Error    string `json:"error,omitempty"`
}

type DocumentStatus struct {
	ID           string            `json:"id"`
	OriginalName string            `json:"original_name"`
	Status       string            `json:"status"`
	Progress     int               `json:"progress"`
	Stages       []ProcessingStage `json:"stages"`
	Result       *DocumentResult   `json:"result,omitempty"`
	Error        string            `json:"error,omitempty"`
	CreatedAt    time.Time         `json:"created_at"`
	ProcessedAt  *time.Time        `json:"processed_at,omitempty"`
}

type DocumentResult struct {
	Pages          int `json:"pages,omitempty"`
	TextLength     int `json:"text_length"`
	KnowledgeItems int `json:"knowledge_items"`
}

// =============================================================================
// DATABASE
// =============================================================================

var db *sql.DB

// Metrics collection (Phase G: Logging and Monitoring)
var (
	httpRequestCounter = make(map[string]int64) // endpoint -> count
	httpErrorCounter   = make(map[string]int64) // endpoint -> error count
	httpDurationSum    = make(map[string]int64) // endpoint -> total duration (ms)
	httpRequestCount   = make(map[string]int64) // endpoint -> request count for avg
	metricsMutex       sync.RWMutex
	perfMutex          sync.RWMutex
	performanceMonitor = &PerformanceMonitor{
		SystemSnapshots: make([]interface{}, 0),
		ToolMetrics:     make(map[string]*ToolPerformanceMetrics),
		WorkflowMetrics: make(map[string]*WorkflowPerformanceMetrics),
	}
	startTime = time.Now()
)

func initDB(databaseURL string) error {
	var err error
	db, err = sql.Open("postgres", databaseURL)
	if err != nil {
		return fmt.Errorf("failed to open database: %w", err)
	}

	db.SetMaxOpenConns(25)
	db.SetMaxIdleConns(5)
	db.SetConnMaxLifetime(5 * time.Minute)

	// Initial connectivity check
	if err := db.Ping(); err != nil {
		return fmt.Errorf("failed to ping database: %w", err)
	}

	// Validate pool settings
	stats := db.Stats()
	if stats.MaxOpenConnections != 25 {
		log.Printf("Warning: MaxOpenConnections mismatch: expected 25, got %d", stats.MaxOpenConnections)
	}

	// Start background health check goroutine
	go monitorDBHealth()

	return nil
}

func monitorDBHealth() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for range ticker.C {
		stats := db.Stats()

		// Log pool metrics
		log.Printf("DB Pool Stats: Open=%d/%d, Idle=%d, InUse=%d, WaitCount=%d, WaitDuration=%v",
			stats.OpenConnections, stats.MaxOpenConnections,
			stats.Idle, stats.InUse,
			stats.WaitCount, stats.WaitDuration)

		// Alert if pool is exhausted
		if stats.OpenConnections >= stats.MaxOpenConnections {
			log.Printf("WARNING: Database connection pool exhausted!")
		}

		// Alert if many connections waiting
		if stats.WaitCount > 0 {
			log.Printf("WARNING: %d connections waiting for database pool", stats.WaitCount)
		}

		// Health check ping
		ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
		if err := db.PingContext(ctx); err != nil {
			log.Printf("ERROR: Database health check failed: %v", err)
		}
		cancel()
	}
}

func runMigrations() error {
	migrations := []string{
		`CREATE TABLE IF NOT EXISTS organizations (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			name VARCHAR(255) NOT NULL,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS projects (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
			name VARCHAR(255) NOT NULL,
			api_key VARCHAR(64) UNIQUE NOT NULL,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS documents (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			name VARCHAR(255) NOT NULL,
			original_name VARCHAR(255) NOT NULL,
			size BIGINT NOT NULL,
			mime_type VARCHAR(100),
			status VARCHAR(20) DEFAULT 'queued',
			progress INT DEFAULT 0,
			file_path VARCHAR(500),
			extracted_text TEXT,
			error TEXT,
			created_at TIMESTAMP DEFAULT NOW(),
			processed_at TIMESTAMP
		)`,
		`CREATE TABLE IF NOT EXISTS knowledge_items (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
			type VARCHAR(50) NOT NULL,
			title VARCHAR(255) NOT NULL,
			content TEXT NOT NULL,
			confidence FLOAT DEFAULT 0,
			source_page INT,
			status VARCHAR(20) DEFAULT 'pending',
			approved_by VARCHAR(100),
			approved_at TIMESTAMP,
			created_at TIMESTAMP DEFAULT NOW(),
			structured_data JSONB
		)`,
		`CREATE INDEX IF NOT EXISTS idx_knowledge_structured ON knowledge_items USING GIN (structured_data)`,
		`CREATE TABLE IF NOT EXISTS document_search_index (
			document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
			term VARCHAR(100) NOT NULL,
			frequency INTEGER DEFAULT 1,
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			PRIMARY KEY (document_id, term)
		)`,
		`CREATE INDEX IF NOT EXISTS idx_document_search_term ON document_search_index (term)`,
		`CREATE INDEX IF NOT EXISTS idx_document_search_doc ON document_search_index (document_id)`,
		`CREATE TABLE IF NOT EXISTS knowledge_search_index (
			knowledge_item_id UUID REFERENCES knowledge_items(id) ON DELETE CASCADE,
			document_id UUID REFERENCES documents(id) ON DELETE CASCADE,
			term VARCHAR(100) NOT NULL,
			frequency INTEGER DEFAULT 1,
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			PRIMARY KEY (knowledge_item_id, term)
		)`,
		`CREATE INDEX IF NOT EXISTS idx_knowledge_search_term ON knowledge_search_index (term)`,
		`CREATE INDEX IF NOT EXISTS idx_knowledge_search_item ON knowledge_search_index (knowledge_item_id)`,
		`CREATE TABLE IF NOT EXISTS telemetry_events (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			event_type VARCHAR(50) NOT NULL,
			payload JSONB,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		// Migration: Add new columns for spec alignment
		`ALTER TABLE telemetry_events ADD COLUMN IF NOT EXISTS agent_id VARCHAR(64)`,
		`ALTER TABLE telemetry_events ADD COLUMN IF NOT EXISTS org_id UUID`,
		`ALTER TABLE telemetry_events ADD COLUMN IF NOT EXISTS team_id UUID`,
		`ALTER TABLE telemetry_events ADD COLUMN IF NOT EXISTS timestamp TIMESTAMP`,
		`CREATE INDEX IF NOT EXISTS idx_documents_project_id ON documents(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_documents_status ON documents(status)`,
		`CREATE INDEX IF NOT EXISTS idx_knowledge_items_document_id ON knowledge_items(document_id)`,
		`CREATE INDEX IF NOT EXISTS idx_telemetry_events_project_id ON telemetry_events(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_telemetry_agent ON telemetry_events(agent_id)`,
		`CREATE INDEX IF NOT EXISTS idx_telemetry_org ON telemetry_events(org_id)`,
		// Hook tables (Phase 9.5)
		`CREATE TABLE IF NOT EXISTS hook_executions (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			agent_id VARCHAR(64),
			org_id UUID,
			team_id UUID,
			hook_type VARCHAR(20) NOT NULL,
			result VARCHAR(20) NOT NULL,
			override_reason VARCHAR(255),
			findings_summary JSONB,
			user_actions JSONB,
			duration_ms BIGINT,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS hook_baselines (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			agent_id VARCHAR(64),
			org_id UUID,
			baseline_entry JSONB NOT NULL,
			source VARCHAR(20) NOT NULL,
			hook_type VARCHAR(20),
			reviewed BOOLEAN DEFAULT false,
			reviewed_by VARCHAR(100),
			reviewed_at TIMESTAMP,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS hook_policies (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
			policy_config JSONB NOT NULL,
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_hook_executions_agent ON hook_executions(agent_id)`,
		`CREATE INDEX IF NOT EXISTS idx_hook_executions_org ON hook_executions(org_id)`,
		`CREATE INDEX IF NOT EXISTS idx_hook_executions_created ON hook_executions(created_at)`,
		`CREATE INDEX IF NOT EXISTS idx_hook_baselines_org ON hook_baselines(org_id)`,
		`CREATE INDEX IF NOT EXISTS idx_hook_baselines_reviewed ON hook_baselines(reviewed)`,
		`CREATE INDEX IF NOT EXISTS idx_hook_policies_org ON hook_policies(org_id)`,
		// Phase 10: Test Enforcement System
		`CREATE TABLE IF NOT EXISTS test_requirements (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			knowledge_item_id UUID REFERENCES knowledge_items(id) ON DELETE CASCADE,
			rule_title VARCHAR(255) NOT NULL,
			requirement_type VARCHAR(50) NOT NULL,
			description TEXT NOT NULL,
			code_function VARCHAR(255),
			priority VARCHAR(20) DEFAULT 'medium',
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS test_coverage (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			test_requirement_id UUID REFERENCES test_requirements(id) ON DELETE CASCADE,
			knowledge_item_id UUID REFERENCES knowledge_items(id) ON DELETE CASCADE,
			coverage_percentage FLOAT DEFAULT 0.0,
			test_files TEXT[],
			last_updated TIMESTAMP DEFAULT NOW(),
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS test_validations (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			test_requirement_id UUID REFERENCES test_requirements(id) ON DELETE CASCADE,
			validation_status VARCHAR(20) NOT NULL,
			issues JSONB,
			test_code_hash VARCHAR(64),
			score FLOAT DEFAULT 0.0,
			validated_at TIMESTAMP DEFAULT NOW(),
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_test_validations_hash ON test_validations(test_code_hash)`,
		`CREATE TABLE IF NOT EXISTS mutation_results (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			test_requirement_id UUID REFERENCES test_requirements(id) ON DELETE CASCADE,
			mutation_score FLOAT DEFAULT 0.0,
			total_mutants INT DEFAULT 0,
			killed_mutants INT DEFAULT 0,
			survived_mutants INT DEFAULT 0,
			execution_time_ms INT,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS test_executions (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			execution_type VARCHAR(50) NOT NULL,
			status VARCHAR(20) DEFAULT 'running',
			result JSONB,
			execution_time_ms INT,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		// Phase 11: Doc-Sync System
		`CREATE TABLE IF NOT EXISTS doc_sync_reports (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			report_type VARCHAR(50) NOT NULL,
			discrepancies JSONB NOT NULL,
			summary JSONB,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS doc_sync_updates (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			report_id UUID REFERENCES doc_sync_reports(id) ON DELETE CASCADE,
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			file_path VARCHAR(500) NOT NULL,
			change_type VARCHAR(50) NOT NULL,
			old_value TEXT,
			new_value TEXT,
			line_number INT,
			approved_by VARCHAR(100),
			approved_at TIMESTAMP,
			applied BOOLEAN DEFAULT false,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_doc_sync_reports_project ON doc_sync_reports(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_doc_sync_reports_created ON doc_sync_reports(created_at)`,
		`CREATE INDEX IF NOT EXISTS idx_doc_sync_updates_report ON doc_sync_updates(report_id)`,
		`CREATE INDEX IF NOT EXISTS idx_doc_sync_updates_project ON doc_sync_updates(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_doc_sync_updates_applied ON doc_sync_updates(applied)`,
		// Phase 12: Gap Reports
		`CREATE TABLE IF NOT EXISTS gap_reports (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			gaps JSONB NOT NULL,
			summary JSONB,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_gap_reports_project ON gap_reports(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_gap_reports_created ON gap_reports(created_at DESC)`,
		// Phase 12: Change Requests
		`CREATE TABLE IF NOT EXISTS change_requests (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			knowledge_item_id UUID REFERENCES knowledge_items(id) ON DELETE CASCADE,
			type VARCHAR(20) NOT NULL,
			current_state JSONB,
			proposed_state JSONB,
			status VARCHAR(20) DEFAULT 'pending_approval',
			implementation_status VARCHAR(20) DEFAULT 'pending',
			implementation_notes TEXT,
			impact_analysis JSONB,
			created_at TIMESTAMP DEFAULT NOW(),
			approved_by VARCHAR(100),
			approved_at TIMESTAMP,
			rejected_by VARCHAR(100),
			rejected_at TIMESTAMP,
			rejection_reason TEXT
		)`,
		`CREATE INDEX IF NOT EXISTS idx_change_requests_project ON change_requests(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_change_requests_status ON change_requests(status)`,
		`CREATE INDEX IF NOT EXISTS idx_change_requests_knowledge_item ON change_requests(knowledge_item_id)`,
		// Change request sequence table for ID generation
		`CREATE TABLE IF NOT EXISTS change_request_sequences (
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			sequence_number SERIAL,
			PRIMARY KEY (project_id, sequence_number)
		)`,
		`CREATE INDEX IF NOT EXISTS idx_change_requests_impl_status ON change_requests(implementation_status)`,
		`CREATE INDEX IF NOT EXISTS idx_change_requests_created ON change_requests(created_at DESC)`,
		`CREATE INDEX IF NOT EXISTS idx_change_requests_project_status ON change_requests(project_id, status)`,
		`CREATE INDEX IF NOT EXISTS idx_change_requests_project_impl ON change_requests(project_id, implementation_status)`,
		`CREATE INDEX IF NOT EXISTS idx_test_requirements_knowledge_item ON test_requirements(knowledge_item_id)`,
		`CREATE INDEX IF NOT EXISTS idx_test_coverage_requirement ON test_coverage(test_requirement_id)`,
		`CREATE INDEX IF NOT EXISTS idx_test_coverage_knowledge_item ON test_coverage(knowledge_item_id)`,
		`CREATE INDEX IF NOT EXISTS idx_test_validations_requirement ON test_validations(test_requirement_id)`,
		`CREATE INDEX IF NOT EXISTS idx_mutation_results_requirement ON mutation_results(test_requirement_id)`,
		`CREATE INDEX IF NOT EXISTS idx_test_executions_project ON test_executions(project_id)`,
		// Phase 14A: Comprehensive Feature Analysis
		`CREATE TABLE IF NOT EXISTS comprehensive_validations (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			validation_id VARCHAR(50) UNIQUE NOT NULL,
			feature VARCHAR(255) NOT NULL,
			mode VARCHAR(20) NOT NULL,
			depth VARCHAR(20) NOT NULL,
			findings JSONB NOT NULL,
			summary JSONB NOT NULL,
			layer_analysis JSONB NOT NULL,
			end_to_end_flows JSONB,
			checklist JSONB NOT NULL,
			created_at TIMESTAMP DEFAULT NOW(),
			completed_at TIMESTAMP
		)`,
		`CREATE INDEX IF NOT EXISTS idx_validations_project ON comprehensive_validations(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_validations_validation_id ON comprehensive_validations(validation_id)`,
		`CREATE INDEX IF NOT EXISTS idx_validations_created ON comprehensive_validations(created_at DESC)`,
		// Phase 15: Intent & Simple Language
		`CREATE TABLE IF NOT EXISTS intent_decisions (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			original_prompt TEXT NOT NULL,
			intent_type VARCHAR(50) NOT NULL,
			clarifying_question TEXT NOT NULL,
			user_choice TEXT NOT NULL,
			resolved_prompt TEXT,
			context_data JSONB,
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE TABLE IF NOT EXISTS intent_patterns (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			pattern_type VARCHAR(50) NOT NULL,
			pattern_data JSONB NOT NULL,
			frequency INTEGER DEFAULT 1,
			last_used TIMESTAMP DEFAULT NOW(),
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_intent_decisions_project ON intent_decisions(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_intent_decisions_type ON intent_decisions(intent_type)`,
		`CREATE INDEX IF NOT EXISTS idx_intent_decisions_created ON intent_decisions(created_at)`,
		`CREATE INDEX IF NOT EXISTS idx_intent_patterns_project ON intent_patterns(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_intent_patterns_type ON intent_patterns(pattern_type)`,
		`CREATE INDEX IF NOT EXISTS idx_intent_patterns_frequency ON intent_patterns(frequency DESC)`,
		`CREATE UNIQUE INDEX IF NOT EXISTS idx_intent_patterns_unique ON intent_patterns(project_id, pattern_type, pattern_data)`,
		// Phase F: Database Optimization - Additional indexes for performance
		`CREATE INDEX IF NOT EXISTS idx_projects_api_key ON projects(api_key)`,                                       // Critical for auth lookups (already UNIQUE, but explicit index helps)
		`CREATE INDEX IF NOT EXISTS idx_knowledge_items_doc_type ON knowledge_items(document_id, type)`,              // Composite for knowledge item queries
		`CREATE INDEX IF NOT EXISTS idx_knowledge_items_status ON knowledge_items(status) WHERE status = 'approved'`, // Partial index for approved items
		`CREATE INDEX IF NOT EXISTS idx_documents_project_status ON documents(project_id, status)`,                   // Composite for document queries
		// Composite index for business context queries (JOIN optimization)
		`CREATE INDEX IF NOT EXISTS idx_documents_project_id_status ON documents(project_id, status) WHERE status = 'completed'`, // Partial index for completed documents
		`CREATE TABLE IF NOT EXISTS analysis_configurations (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			configuration JSONB NOT NULL,
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_configs_project ON analysis_configurations(project_id)`,
		`CREATE TABLE IF NOT EXISTS llm_configurations (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			provider VARCHAR(50) NOT NULL,
			api_key_encrypted BYTEA NOT NULL,
			model VARCHAR(100) NOT NULL,
			key_type VARCHAR(20) NOT NULL,
			cost_optimization JSONB,
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_llm_configs_project ON llm_configurations(project_id)`,
		`CREATE TABLE IF NOT EXISTS llm_usage (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			validation_id UUID REFERENCES comprehensive_validations(id) ON DELETE CASCADE,
			provider VARCHAR(50) NOT NULL,
			model VARCHAR(100) NOT NULL,
			tokens_used INT NOT NULL,
			estimated_cost DECIMAL(10, 4),
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_llm_usage_project ON llm_usage(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_llm_usage_validation ON llm_usage(validation_id)`,
		// Phase 14C: Config Audit Log
		`CREATE TABLE IF NOT EXISTS config_audit_log (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
			config_id UUID,
			action VARCHAR(20) NOT NULL,
			changed_by VARCHAR(100),
			old_value JSONB,
			new_value JSONB,
			ip_address VARCHAR(45),
			created_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_config_audit_project ON config_audit_log(project_id)`,
		`CREATE INDEX IF NOT EXISTS idx_config_audit_config ON config_audit_log(config_id)`,
		`CREATE INDEX IF NOT EXISTS idx_config_audit_created ON config_audit_log(created_at DESC)`,
		// Binary distribution system
		`CREATE TABLE IF NOT EXISTS binary_versions (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			version VARCHAR(50) NOT NULL,
			platform VARCHAR(20) NOT NULL,
			architecture VARCHAR(10) NOT NULL,
			os VARCHAR(10) NOT NULL,
			file_path TEXT NOT NULL,
			file_size BIGINT NOT NULL,
			checksum_sha256 VARCHAR(64) NOT NULL,
			checksum_md5 VARCHAR(32),
			signature TEXT,
			release_notes TEXT,
			is_stable BOOLEAN DEFAULT true,
			is_latest BOOLEAN DEFAULT false,
			min_go_version VARCHAR(10),
			created_at TIMESTAMP DEFAULT NOW(),
			released_at TIMESTAMP,
			created_by UUID,
			UNIQUE(version, platform)
		)`,
		`CREATE INDEX IF NOT EXISTS idx_binary_versions_platform_latest ON binary_versions(platform, is_latest) WHERE is_latest = true`,
		`CREATE INDEX IF NOT EXISTS idx_binary_versions_version ON binary_versions(version)`,
		`CREATE TABLE IF NOT EXISTS binary_downloads (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			version_id UUID REFERENCES binary_versions(id),
			project_id UUID REFERENCES projects(id),
			user_agent TEXT,
			ip_address INET,
			downloaded_at TIMESTAMP DEFAULT NOW()
		)`,
		`CREATE INDEX IF NOT EXISTS idx_binary_downloads_version ON binary_downloads(version_id)`,
		`CREATE INDEX IF NOT EXISTS idx_binary_downloads_project ON binary_downloads(project_id)`,
		`CREATE TABLE IF NOT EXISTS rules_versions (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			version VARCHAR(50) NOT NULL,
			rule_name VARCHAR(100) NOT NULL,
			rule_content TEXT NOT NULL,
			rule_type VARCHAR(20) NOT NULL,
			globs TEXT[],
			is_latest BOOLEAN DEFAULT false,
			created_at TIMESTAMP DEFAULT NOW(),
			released_at TIMESTAMP,
			UNIQUE(version, rule_name)
		)`,
		`CREATE INDEX IF NOT EXISTS idx_rules_versions_latest ON rules_versions(is_latest) WHERE is_latest = true`,
		// Phase 14E: Task Dependency & Verification System
		`CREATE TABLE IF NOT EXISTS tasks (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			project_id UUID NOT NULL REFERENCES projects(id) ON DELETE CASCADE,
			source VARCHAR(50) NOT NULL,
			title TEXT NOT NULL,
			description TEXT,
			file_path VARCHAR(500),
			line_number INTEGER,
			status VARCHAR(20) NOT NULL DEFAULT 'pending',
			priority VARCHAR(10) DEFAULT 'medium',
			assigned_to VARCHAR(100),
			estimated_effort INTEGER,
			actual_effort INTEGER,
			tags TEXT[],
			verification_confidence FLOAT DEFAULT 0.0,
			version INTEGER DEFAULT 1,
			created_at TIMESTAMP NOT NULL DEFAULT NOW(),
			updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
			completed_at TIMESTAMP,
			verified_at TIMESTAMP,
			archived_at TIMESTAMP,
			CONSTRAINT valid_status CHECK (status IN ('pending', 'in_progress', 'completed', 'blocked')),
			CONSTRAINT valid_priority CHECK (priority IN ('low', 'medium', 'high', 'critical')),
			CONSTRAINT valid_confidence CHECK (verification_confidence >= 0.0 AND verification_confidence <= 1.0)
		)`,
		`CREATE TABLE IF NOT EXISTS task_dependencies (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
			depends_on_task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
			dependency_type VARCHAR(20) NOT NULL,
			confidence FLOAT DEFAULT 0.0,
			created_at TIMESTAMP NOT NULL DEFAULT NOW(),
			CONSTRAINT valid_dependency_type CHECK (dependency_type IN ('explicit', 'implicit', 'integration', 'feature')),
			CONSTRAINT no_self_dependency CHECK (task_id != depends_on_task_id),
			CONSTRAINT unique_dependency UNIQUE (task_id, depends_on_task_id),
			CONSTRAINT valid_dep_confidence CHECK (confidence >= 0.0 AND confidence <= 1.0)
		)`,
		`CREATE TABLE IF NOT EXISTS task_verifications (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
			verification_type VARCHAR(20) NOT NULL,
			status VARCHAR(20) NOT NULL DEFAULT 'pending',
			confidence FLOAT DEFAULT 0.0,
			evidence JSONB,
			retry_count INTEGER DEFAULT 0,
			verified_at TIMESTAMP,
			created_at TIMESTAMP NOT NULL DEFAULT NOW(),
			CONSTRAINT valid_verification_type CHECK (verification_type IN ('code_existence', 'code_usage', 'test_coverage', 'integration')),
			CONSTRAINT valid_verification_status CHECK (status IN ('pending', 'verified', 'failed')),
			CONSTRAINT valid_ver_confidence CHECK (confidence >= 0.0 AND confidence <= 1.0)
		)`,
		`CREATE TABLE IF NOT EXISTS task_links (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
			link_type VARCHAR(50) NOT NULL,
			linked_id UUID NOT NULL,
			created_at TIMESTAMP NOT NULL DEFAULT NOW(),
			CONSTRAINT valid_link_type CHECK (link_type IN ('change_request', 'knowledge_item', 'comprehensive_analysis', 'test_requirement')),
			CONSTRAINT unique_task_link UNIQUE (task_id, link_type, linked_id)
		)`,
		`CREATE TABLE IF NOT EXISTS task_changes (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
			change_type VARCHAR(20) NOT NULL,
			field_name VARCHAR(50),
			old_value TEXT,
			new_value TEXT,
			changed_by VARCHAR(100),
			change_reason TEXT,
			version_before INTEGER,
			version_after INTEGER,
			created_at TIMESTAMP NOT NULL DEFAULT NOW(),
			CONSTRAINT valid_change_type CHECK (change_type IN ('create', 'update', 'delete', 'status_change', 'dependency_add', 'dependency_remove'))
		)`,
		`CREATE TABLE IF NOT EXISTS task_impact_analysis (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
			change_type VARCHAR(20) NOT NULL,
			impact_scope VARCHAR(20) NOT NULL,
			affected_tasks JSONB,
			risk_level VARCHAR(10) NOT NULL,
			risk_factors JSONB,
			mitigation_suggestions JSONB,
			estimated_impact_time INTEGER,
			confidence_score FLOAT DEFAULT 0.0,
			analyzed_at TIMESTAMP NOT NULL DEFAULT NOW(),
			CONSTRAINT valid_impact_scope CHECK (impact_scope IN ('isolated', 'local', 'project_wide', 'cross_project')),
			CONSTRAINT valid_risk_level CHECK (risk_level IN ('low', 'medium', 'high', 'critical'))
		)`,
		`CREATE TABLE IF NOT EXISTS dependency_changes (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			task_id UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
			dependency_id UUID REFERENCES task_dependencies(id) ON DELETE CASCADE,
			change_type VARCHAR(20) NOT NULL,
			affected_chain JSONB,
			ripple_effects JSONB,
			cycle_risk BOOLEAN DEFAULT FALSE,
			performance_impact VARCHAR(10),
			changed_at TIMESTAMP NOT NULL DEFAULT NOW(),
			CONSTRAINT valid_dep_change_type CHECK (change_type IN ('dependency_added', 'dependency_removed', 'dependency_updated')),
			CONSTRAINT valid_perf_impact CHECK (performance_impact IN ('none', 'minor', 'moderate', 'major', 'critical'))
		)`,
		`CREATE TABLE IF NOT EXISTS repositories (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
			name VARCHAR(255) NOT NULL,
			full_name VARCHAR(500) UNIQUE NOT NULL,
			description TEXT,
			url VARCHAR(1000),
			clone_url VARCHAR(1000),
			ssh_url VARCHAR(1000),
			default_branch VARCHAR(100) DEFAULT 'main',
			language VARCHAR(50),
			size_bytes BIGINT,
			stars_count INTEGER DEFAULT 0,
			forks_count INTEGER DEFAULT 0,
			watchers_count INTEGER DEFAULT 0,
			is_private BOOLEAN DEFAULT FALSE,
			is_archived BOOLEAN DEFAULT FALSE,
			is_template BOOLEAN DEFAULT FALSE,
			is_fork BOOLEAN DEFAULT FALSE,
			parent_repo_id UUID REFERENCES repositories(id),
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			last_synced_at TIMESTAMP,
			sync_status VARCHAR(20) DEFAULT 'pending',
			CONSTRAINT valid_sync_status CHECK (sync_status IN ('pending', 'syncing', 'completed', 'failed'))
		)`,
		`CREATE TABLE IF NOT EXISTS repository_relationships (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			source_repo_id UUID NOT NULL REFERENCES repositories(id) ON DELETE CASCADE,
			target_repo_id UUID NOT NULL REFERENCES repositories(id) ON DELETE CASCADE,
			relationship_type VARCHAR(50) NOT NULL,
			strength FLOAT DEFAULT 1.0,
			metadata JSONB,
			discovered_at TIMESTAMP DEFAULT NOW(),
			last_updated TIMESTAMP DEFAULT NOW(),
			confidence_score FLOAT DEFAULT 0.0,
			CONSTRAINT valid_relationship_type CHECK (relationship_type IN ('dependency', 'import', 'fork', 'template', 'monorepo', 'microservice', 'shared_library', 'api_contract')),
			CONSTRAINT valid_relationship_strength CHECK (strength >= 0.0 AND strength <= 1.0),
			CONSTRAINT valid_relationship_confidence CHECK (confidence_score >= 0.0 AND confidence_score <= 1.0),
			CONSTRAINT no_self_relationship CHECK (source_repo_id != target_repo_id)
		)`,
		`CREATE TABLE IF NOT EXISTS cross_repo_dependencies (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			source_repo_id UUID NOT NULL REFERENCES repositories(id) ON DELETE CASCADE,
			target_repo_id UUID NOT NULL REFERENCES repositories(id) ON DELETE CASCADE,
			dependency_type VARCHAR(50) NOT NULL,
			package_name VARCHAR(255),
			version_constraint VARCHAR(100),
			source_file_path VARCHAR(1000),
			source_line_number INTEGER,
			confidence_score FLOAT DEFAULT 0.0,
			last_detected TIMESTAMP DEFAULT NOW(),
			is_active BOOLEAN DEFAULT TRUE,
			CONSTRAINT valid_cross_dep_type CHECK (dependency_type IN ('package', 'library', 'api', 'contract', 'data', 'config')),
			CONSTRAINT valid_cross_dep_confidence CHECK (confidence_score >= 0.0 AND confidence_score <= 1.0)
		)`,
		`CREATE TABLE IF NOT EXISTS repository_analysis (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			repo_id UUID NOT NULL REFERENCES repositories(id) ON DELETE CASCADE,
			analysis_type VARCHAR(50) NOT NULL,
			analysis_scope VARCHAR(20) DEFAULT 'single',
			result_data JSONB,
			confidence_score FLOAT DEFAULT 0.0,
			analyzed_at TIMESTAMP DEFAULT NOW(),
			duration_ms INTEGER,
			status VARCHAR(20) DEFAULT 'completed',
			CONSTRAINT valid_analysis_type CHECK (analysis_type IN ('complexity', 'dependencies', 'security', 'performance', 'maintainability', 'cross_repo_impact')),
			CONSTRAINT valid_analysis_scope CHECK (analysis_scope IN ('single', 'multi_repo', 'organization')),
			CONSTRAINT valid_analysis_status CHECK (status IN ('pending', 'running', 'completed', 'failed')),
			CONSTRAINT valid_analysis_confidence CHECK (confidence_score >= 0.0 AND confidence_score <= 1.0)
		)`,
		`CREATE TABLE IF NOT EXISTS cross_repo_analysis (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			analysis_name VARCHAR(255) NOT NULL,
			description TEXT,
			repo_ids UUID[] NOT NULL,
			analysis_type VARCHAR(50) NOT NULL,
			parameters JSONB,
			result_summary JSONB,
			detailed_results JSONB,
			status VARCHAR(20) DEFAULT 'pending',
			progress_percentage FLOAT DEFAULT 0.0,
			started_at TIMESTAMP,
			completed_at TIMESTAMP,
			duration_ms INTEGER,
			created_by VARCHAR(100),
			created_at TIMESTAMP DEFAULT NOW(),
			CONSTRAINT valid_cross_analysis_type CHECK (analysis_type IN ('dependency_network', 'impact_propagation', 'security_cross_repo', 'performance_interaction', 'maintainability_network')),
			CONSTRAINT valid_cross_analysis_status CHECK (status IN ('pending', 'running', 'completed', 'failed', 'cancelled')),
			CONSTRAINT valid_cross_analysis_progress CHECK (progress_percentage >= 0.0 AND progress_percentage <= 100.0)
		)`,
		`CREATE TABLE IF NOT EXISTS api_versions (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			version VARCHAR(20) UNIQUE NOT NULL,
			status VARCHAR(20) NOT NULL DEFAULT 'active',
			sunset_date TIMESTAMP,
			deprecated_by VARCHAR(20),
			changelog TEXT,
			breaking_changes BOOLEAN DEFAULT FALSE,
			migration_guide TEXT,
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			CONSTRAINT valid_version_status CHECK (status IN ('active', 'deprecated', 'sunset')),
			CONSTRAINT valid_version_format CHECK (version ~ '^v\d+$')
		)`,
		`CREATE TABLE IF NOT EXISTS api_endpoints (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			path VARCHAR(255) NOT NULL,
			method VARCHAR(10) NOT NULL,
			version VARCHAR(20) NOT NULL,
			description TEXT,
			deprecated BOOLEAN DEFAULT FALSE,
			deprecated_in VARCHAR(20),
			sunset_date TIMESTAMP,
			replacement_path VARCHAR(255),
			migration_notes TEXT,
			tags TEXT[],
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			CONSTRAINT valid_endpoint_method CHECK (method IN ('GET', 'POST', 'PUT', 'DELETE', 'PATCH', 'OPTIONS')),
			CONSTRAINT unique_endpoint_version UNIQUE (path, method, version)
		)`,
		`CREATE TABLE IF NOT EXISTS version_migrations (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			from_version VARCHAR(20) NOT NULL,
			to_version VARCHAR(20) NOT NULL,
			endpoint_path VARCHAR(255) NOT NULL,
			method VARCHAR(10) NOT NULL,
			migration_type VARCHAR(20) NOT NULL,
			transformation_rules JSONB,
			backward_compatible BOOLEAN DEFAULT TRUE,
			created_at TIMESTAMP DEFAULT NOW(),
			CONSTRAINT valid_migration_type CHECK (migration_type IN ('path_change', 'parameter_change', 'response_change', 'breaking_change')),
			CONSTRAINT different_versions CHECK (from_version != to_version)
		)`,
		// Indexes for tasks table
		`CREATE INDEX IF NOT EXISTS idx_tasks_project_status ON tasks(project_id, status)`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_project_priority ON tasks(project_id, priority)`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_file_path ON tasks(file_path)`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status) WHERE status IN ('pending', 'in_progress')`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_priority ON tasks(priority) WHERE priority IN ('high', 'critical')`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_assigned_to ON tasks(assigned_to) WHERE assigned_to IS NOT NULL`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_created_at ON tasks(created_at DESC)`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_updated_at ON tasks(updated_at DESC)`,
		// Full-text search index for task title (using text search)
		`CREATE INDEX IF NOT EXISTS idx_tasks_title_search ON tasks USING gin(to_tsvector('english', title))`,
		`CREATE INDEX IF NOT EXISTS idx_tasks_description_search ON tasks USING gin(to_tsvector('english', description)) WHERE description IS NOT NULL`,
		// Indexes for task_dependencies table
		`CREATE INDEX IF NOT EXISTS idx_task_dependencies_task ON task_dependencies(task_id)`,
		`CREATE INDEX IF NOT EXISTS idx_task_dependencies_depends_on ON task_dependencies(depends_on_task_id)`,
		`CREATE INDEX IF NOT EXISTS idx_task_dependencies_type ON task_dependencies(dependency_type)`,
		// Indexes for task_verifications table
		`CREATE INDEX IF NOT EXISTS idx_task_verifications_task ON task_verifications(task_id)`,
		`CREATE INDEX IF NOT EXISTS idx_task_verifications_status ON task_verifications(status)`,
		`CREATE INDEX IF NOT EXISTS idx_task_verifications_type ON task_verifications(verification_type)`,
		// Indexes for task_links table
		`CREATE INDEX IF NOT EXISTS idx_task_links_task ON task_links(task_id)`,
		`CREATE INDEX IF NOT EXISTS idx_task_links_linked ON task_links(link_type, linked_id)`,
		// Phase 16: Organization Features
		`CREATE TABLE IF NOT EXISTS teams (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
			name VARCHAR(100) NOT NULL,
			description TEXT,
			lead_user_id VARCHAR(100),
			settings JSONB DEFAULT '{}',
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			UNIQUE(org_id, name)
		)`,
		`CREATE TABLE IF NOT EXISTS team_members (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			team_id UUID NOT NULL REFERENCES teams(id) ON DELETE CASCADE,
			user_id VARCHAR(100) NOT NULL,
			role VARCHAR(20) NOT NULL DEFAULT 'member',
			added_by VARCHAR(100),
			added_at TIMESTAMP DEFAULT NOW(),
			UNIQUE(team_id, user_id),
			CONSTRAINT valid_team_role CHECK (role IN ('lead', 'senior', 'member'))
		)`,
		`CREATE TABLE IF NOT EXISTS registered_agents (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
			team_id UUID REFERENCES teams(id) ON DELETE SET NULL,
			agent_id VARCHAR(64) UNIQUE NOT NULL,
			name VARCHAR(100) NOT NULL,
			version VARCHAR(20),
			platform VARCHAR(50),
			last_seen TIMESTAMP DEFAULT NOW(),
			status VARCHAR(20) DEFAULT 'active',
			capabilities JSONB DEFAULT '[]',
			settings JSONB DEFAULT '{}',
			created_at TIMESTAMP DEFAULT NOW(),
			CONSTRAINT valid_agent_status CHECK (status IN ('active', 'inactive', 'suspended'))
		)`,
		`CREATE TABLE IF NOT EXISTS organization_patterns (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
			name VARCHAR(100) NOT NULL,
			description TEXT,
			pattern_type VARCHAR(50) NOT NULL,
			pattern_data JSONB NOT NULL,
			is_shared BOOLEAN DEFAULT true,
			created_by VARCHAR(100),
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			UNIQUE(org_id, name, pattern_type)
		)`,
		`CREATE TABLE IF NOT EXISTS pattern_distributions (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			pattern_id UUID NOT NULL REFERENCES organization_patterns(id) ON DELETE CASCADE,
			target_type VARCHAR(20) NOT NULL,
			target_id UUID NOT NULL,
			status VARCHAR(20) DEFAULT 'pending',
			distributed_at TIMESTAMP,
			error_message TEXT,
			created_at TIMESTAMP DEFAULT NOW(),
			CONSTRAINT valid_target_type CHECK (target_type IN ('agent', 'team', 'project'))
		)`,
		`CREATE TABLE IF NOT EXISTS organization_alerts (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
			alert_type VARCHAR(50) NOT NULL,
			severity VARCHAR(10) NOT NULL,
			title VARCHAR(200) NOT NULL,
			message TEXT NOT NULL,
			context_data JSONB DEFAULT '{}',
			is_read BOOLEAN DEFAULT false,
			read_by VARCHAR(100),
			read_at TIMESTAMP,
			resolved BOOLEAN DEFAULT false,
			resolved_by VARCHAR(100),
			resolved_at TIMESTAMP,
			created_at TIMESTAMP DEFAULT NOW(),
			CONSTRAINT valid_severity CHECK (severity IN ('low', 'medium', 'high', 'critical'))
		)`,
		`CREATE TABLE IF NOT EXISTS alert_rules (
			id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
			org_id UUID NOT NULL REFERENCES organizations(id) ON DELETE CASCADE,
			rule_name VARCHAR(100) NOT NULL,
			rule_type VARCHAR(50) NOT NULL,
			conditions JSONB NOT NULL,
			actions JSONB NOT NULL,
			is_active BOOLEAN DEFAULT true,
			created_by VARCHAR(100),
			created_at TIMESTAMP DEFAULT NOW(),
			updated_at TIMESTAMP DEFAULT NOW(),
			UNIQUE(org_id, rule_name)
		)`,
		`CREATE INDEX IF NOT EXISTS idx_teams_org ON teams(org_id)`,
		`CREATE INDEX IF NOT EXISTS idx_team_members_team ON team_members(team_id)`,
		`CREATE INDEX IF NOT EXISTS idx_team_members_user ON team_members(user_id)`,
		`CREATE INDEX IF NOT EXISTS idx_registered_agents_org ON registered_agents(org_id)`,
		`CREATE INDEX IF NOT EXISTS idx_registered_agents_team ON registered_agents(team_id)`,
		`CREATE INDEX IF NOT EXISTS idx_registered_agents_status ON registered_agents(status)`,
		`CREATE INDEX IF NOT EXISTS idx_org_patterns_org ON organization_patterns(org_id)`,
		`CREATE INDEX IF NOT EXISTS idx_org_patterns_type ON organization_patterns(pattern_type)`,
		`CREATE INDEX IF NOT EXISTS idx_pattern_distributions_pattern ON pattern_distributions(pattern_id)`,
		`CREATE INDEX IF NOT EXISTS idx_pattern_distributions_target ON pattern_distributions(target_type, target_id)`,
		`CREATE INDEX IF NOT EXISTS idx_org_alerts_org ON organization_alerts(org_id)`,
		`CREATE INDEX IF NOT EXISTS idx_org_alerts_unread ON organization_alerts(is_read) WHERE is_read = false`,
		`CREATE INDEX IF NOT EXISTS idx_alert_rules_org ON alert_rules(org_id)`,
	}

	for _, migration := range migrations {
		if _, err := db.Exec(migration); err != nil {
			return fmt.Errorf("migration failed: %w", err)
		}
	}

	return nil
}

// =============================================================================
// HANDLERS
// =============================================================================

// Health check handlers (Phase G: Logging and Monitoring)

// healthHandler returns basic health status
func healthHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusOK)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status":    "ok",
		"service":   "sentinel-hub",
		"version":   "1.0.0",
		"timestamp": time.Now().Format(time.RFC3339),
	})
}

// healthDBHandler checks database connectivity
func healthDBHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")

	ctx, cancel := context.WithTimeout(r.Context(), 5*time.Second)
	defer cancel()

	err := db.PingContext(ctx)
	if err != nil {
		w.WriteHeader(http.StatusServiceUnavailable)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"status":    "unhealthy",
			"service":   "database",
			"error":     err.Error(),
			"timestamp": time.Now().Format(time.RFC3339),
		})
		return
	}

	// Get connection pool stats
	stats := db.Stats()
	w.WriteHeader(http.StatusOK)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status":  "healthy",
		"service": "database",
		"stats": map[string]interface{}{
			"open_connections": stats.OpenConnections,
			"in_use":           stats.InUse,
			"idle":             stats.Idle,
			"wait_count":       stats.WaitCount,
		},
		"timestamp": time.Now().Format(time.RFC3339),
	})
}

// healthReadyHandler checks if the service is ready to accept traffic
func healthReadyHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")

	// Check database connectivity
	ctx, cancel := context.WithTimeout(r.Context(), 5*time.Second)
	defer cancel()

	err := db.PingContext(ctx)
	if err != nil {
		w.WriteHeader(http.StatusServiceUnavailable)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"status":    "not_ready",
			"reason":    "database_unavailable",
			"error":     err.Error(),
			"timestamp": time.Now().Format(time.RFC3339),
		})
		return
	}

	// Check storage directory
	config := GetConfig()
	if _, err := os.Stat(config.DocumentStorage); os.IsNotExist(err) {
		w.WriteHeader(http.StatusServiceUnavailable)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"status":    "not_ready",
			"reason":    "storage_unavailable",
			"error":     fmt.Sprintf("Storage directory does not exist: %s", config.DocumentStorage),
			"timestamp": time.Now().Format(time.RFC3339),
		})
		return
	}

	w.WriteHeader(http.StatusOK)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status":    "ready",
		"timestamp": time.Now().Format(time.RFC3339),
	})
}

// Upload document
func uploadDocumentHandler(config *ServerConfig) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		// Get project from context (set by auth middleware)
		project, err := getProjectFromContext(r.Context())
		if err != nil {
			LogErrorWithContext(r.Context(), err, "Failed to get project from context")
			LogErrorWithContext(r.Context(), err, "Internal server error")
			LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
			WriteErrorResponse(w, &DatabaseError{
				Operation:     "internal_operation",
				Message:       "Internal server error",
				OriginalError: fmt.Errorf("internal server error"),
			}, http.StatusInternalServerError)
			return
		}

		// Parse multipart form (max 100MB)
		if err := r.ParseMultipartForm(100 << 20); err != nil {
			LogErrorWithContext(r.Context(), err, fmt.Sprintf("Error parsing multipart form for project %s", project.ID))
			WriteErrorResponse(w, &ValidationError{
				Field:   "file",
				Message: "File too large or invalid form",
				Code:    "invalid_file",
			}, http.StatusBadRequest)
			return
		}

		file, header, err := r.FormFile("file")
		if err != nil {
			LogErrorWithContext(r.Context(), err, fmt.Sprintf("Error getting file from form for project %s", project.ID))
			WriteErrorResponse(w, &ValidationError{
				Field:   "file",
				Message: "No file provided",
				Code:    "required",
			}, http.StatusBadRequest)
			return
		}
		defer file.Close()

		// Validate content type
		contentType := header.Header.Get("Content-Type")
		if err := validateDocumentContentType(contentType); err != nil {
			LogErrorWithContext(r.Context(), err, fmt.Sprintf("Invalid content type for project %s: %s", project.ID, contentType))
			WriteErrorResponse(w, &ValidationError{
				Field:   "file",
				Message: err.Error(),
				Code:    "invalid_content_type",
			}, http.StatusBadRequest)
			return
		}

		// Generate document ID
		docID := uuid.New().String()

		// Create storage directory
		storageDir := filepath.Join(config.DocumentStorage, project.ID, docID)
		if err := os.MkdirAll(storageDir, 0755); err != nil {
			LogErrorWithContext(r.Context(), err, "Storage error")
			WriteErrorResponse(w, &ExternalServiceError{
				Service:    "storage",
				Message:    "Storage error",
				StatusCode: 0,
			}, http.StatusInternalServerError)
			return
		}

		// Save file
		filePath := filepath.Join(storageDir, header.Filename)
		dst, err := os.Create(filePath)
		if err != nil {
			LogErrorWithContext(r.Context(), err, "Storage error")
			WriteErrorResponse(w, &ExternalServiceError{
				Service:    "storage",
				Message:    "Storage error",
				StatusCode: 0,
			}, http.StatusInternalServerError)
			return
		}
		defer dst.Close()

		if _, err := io.Copy(dst, file); err != nil {
			LogErrorWithContext(r.Context(), err, "Storage error")
			WriteErrorResponse(w, &ExternalServiceError{
				Service:    "storage",
				Message:    "Storage error",
				StatusCode: 0,
			}, http.StatusInternalServerError)
			return
		}

		// Detect MIME type
		mimeType := header.Header.Get("Content-Type")
		if mimeType == "" {
			mimeType = detectMimeType(header.Filename)
		}

		// Insert into database
		doc := Document{
			ID:           docID,
			ProjectID:    project.ID,
			Name:         docID,
			OriginalName: header.Filename,
			Size:         header.Size,
			MimeType:     mimeType,
			Status:       "queued",
			Progress:     0,
			FilePath:     filePath,
			CreatedAt:    time.Now(),
		}

		_, err = db.Exec(`
			INSERT INTO documents (id, project_id, name, original_name, size, mime_type, status, progress, file_path, created_at)
			VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
		`, doc.ID, doc.ProjectID, doc.Name, doc.OriginalName, doc.Size, doc.MimeType, doc.Status, doc.Progress, doc.FilePath, doc.CreatedAt)

		if err != nil {
			WriteErrorResponse(w, &DatabaseError{
				Operation:     "database_query",
				Message:       "Database error",
				OriginalError: err,
			}, http.StatusInternalServerError)
			return
		}

		// Check if change detection is requested
		detectChanges := r.URL.Query().Get("detect_changes") == "true"

		// Change detection is automatically triggered by the processor after knowledge extraction
		// The detect_changes query parameter is deprecated but kept for backward compatibility
		if detectChanges {
			LogInfo(r.Context(), "Change detection requested for document %s (will process after extraction)", doc.ID)
		}

		// Start background document processing
		go processDocumentAsync(docID, filePath, mimeType, config)

		// Return response
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(http.StatusAccepted)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"id":                     doc.ID,
			"status":                 doc.Status,
			"estimated_time_seconds": estimateProcessingTime(doc.Size),
			"detect_changes":         detectChanges,
		})

		// Index the document for search
	}
}

// processDocumentAsync processes a document asynchronously
func processDocumentAsync(docID, filePath, mimeType string, config *ServerConfig) {
	ctx := context.Background()

	// Update status to processing
	_, err := db.Exec(`UPDATE documents SET status = 'processing', progress = 10 WHERE id = $1`, docID)
	if err != nil {
		LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to update document %s status to processing", docID))
		return
	}

	defer func() {
		if r := recover(); r != nil {
			LogErrorWithContext(ctx, fmt.Errorf("panic in document processing: %v", r), fmt.Sprintf("Document processing panic for %s", docID))
			db.Exec(`UPDATE documents SET status = 'failed', error = $1 WHERE id = $2`, fmt.Sprintf("Processing panic: %v", r), docID)
		}
	}() // Extract text based on file type
	var extractedText string

	switch {
	case strings.Contains(mimeType, "pdf"):
		extractedText, err = processPDFDocument(filePath)
	case strings.Contains(mimeType, "docx") || strings.Contains(mimeType, "document"):
		extractedText, err = processDOCXDocument(filePath)
	case strings.Contains(mimeType, "txt") || strings.Contains(mimeType, "text"):
		extractedText, err = processTextDocument(filePath)
	case strings.Contains(mimeType, "md") || strings.Contains(mimeType, "markdown"):
		extractedText, err = processMarkdownDocument(filePath)
	default:
		// Try to extract text using general method
		extractedText, err = processGenericDocument(filePath)
	}

	if err != nil {
		LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to extract text from document %s", docID))
		db.Exec(`UPDATE documents SET status = 'failed', error = $1, processed_at = $2 WHERE id = $3`,
			err.Error(), time.Now(), docID)
		return
	}

	// Update progress
	_, err = db.Exec(`UPDATE documents SET progress = 80 WHERE id = $1`, docID)
	if err != nil {
		LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to update progress for document %s", docID))
	}

	// Extract knowledge items from the text
	knowledgeItems, err := extractKnowledgeFromText(extractedText, docID)
	if err != nil {
		LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to extract knowledge from document %s", docID))
		// Continue with empty knowledge - don't fail the whole process
	}

	// Store knowledge items
	for _, item := range knowledgeItems {
		_, err = db.Exec(`
			INSERT INTO knowledge_items (id, document_id, type, title, content, confidence, structured_data, created_at)
			VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
		`, item.ID, docID, item.Type, item.Title, item.Content, item.Confidence, item.StructuredData, time.Now())
		if err != nil {
			LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to store knowledge item for document %s", docID))
		}
	}

	// Update document as completed
	processedAt := time.Now()
	_, err = db.Exec(`
		UPDATE documents
		SET status = 'completed', progress = 100, extracted_text = $1, processed_at = $2
		WHERE id = $3
	`, extractedText, processedAt, docID)

	if err != nil {
		LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to mark document %s as completed", docID))
	}

	// Index the document for search
	go indexDocumentForSearch(docID, extractedText, knowledgeItems)

	LogInfo(ctx, fmt.Sprintf("Document %s processed successfully. Extracted %d characters, %d knowledge items", docID, len(extractedText), len(knowledgeItems)))

}

// indexDocumentForSearch creates searchable index for the document
func indexDocumentForSearch(docID, extractedText string, knowledgeItems []KnowledgeItem) {
	ctx := context.Background()

	// Create or update document search index
	searchTerms := extractSearchTerms(extractedText)

	// Store search index in database
	for _, term := range searchTerms {
		_, err := db.Exec(`
			INSERT INTO document_search_index (document_id, term, frequency, created_at)
			VALUES ($1, $2, $3, $4)
			ON CONFLICT (document_id, term) DO UPDATE SET
				frequency = document_search_index.frequency + 1,
				updated_at = $4
		`, docID, term.Term, term.Frequency, time.Now())

		if err != nil {
			LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to index search term for document %s", docID))
		}
	}

	// Index knowledge items for semantic search
	for _, item := range knowledgeItems {
		knowledgeTerms := extractSearchTerms(item.Content)
		for _, term := range knowledgeTerms {
			_, err := db.Exec(`
				INSERT INTO knowledge_search_index (knowledge_item_id, document_id, term, frequency, created_at)
				VALUES ($1, $2, $3, $4, $5)
				ON CONFLICT (knowledge_item_id, term) DO UPDATE SET
					frequency = knowledge_search_index.frequency + 1,
					updated_at = $5
			`, item.ID, docID, term.Term, term.Frequency, time.Now())

			if err != nil {
				LogErrorWithContext(ctx, err, fmt.Sprintf("Failed to index knowledge term for document %s", docID))
			}
		}
	}
}

// SearchTerm represents a term and its frequency in a document
type SearchTerm struct {
	Term      string
	Frequency int
}

// extractSearchTerms tokenizes and extracts searchable terms from text
func extractSearchTerms(text string) []SearchTerm {
	termFreq := make(map[string]int)

	// Normalize text
	text = strings.ToLower(text)

	// Remove punctuation and split into words
	reg := regexp.MustCompile(`[^\w\s]+`)
	text = reg.ReplaceAllString(text, " ")

	words := strings.Fields(text)

	// Filter out stop words and count frequency
	stopWords := map[string]bool{
		"the": true, "a": true, "an": true, "and": true, "or": true, "but": true,
		"in": true, "on": true, "at": true, "to": true, "for": true, "of": true,
		"with": true, "by": true, "is": true, "are": true, "was": true, "were": true,
		"be": true, "been": true, "being": true, "have": true, "has": true, "had": true,
		"do": true, "does": true, "did": true, "will": true, "would": true, "could": true,
		"should": true, "may": true, "might": true, "must": true, "can": true,
	}

	for _, word := range words {
		word = strings.TrimSpace(word)
		if len(word) >= 3 && !stopWords[word] { // Only words >= 3 chars, skip stop words
			termFreq[word]++
		}
	}

	// Convert to SearchTerm slice
	var terms []SearchTerm
	for term, freq := range termFreq {
		terms = append(terms, SearchTerm{Term: term, Frequency: freq})
	}

	// Sort by frequency (most frequent first)
	sort.Slice(terms, func(i, j int) bool {
		return terms[i].Frequency > terms[j].Frequency
	})

	// Limit to top 100 terms per document
	if len(terms) > 100 {
		terms = terms[:100]
	}

	return terms
}

// searchDocuments performs full-text search across documents
func searchDocuments(query string, projectID string, limit int) ([]DocumentSearchResult, error) {
	// Tokenize search query
	searchTerms := extractSearchTerms(query)

	if len(searchTerms) == 0 {
		return []DocumentSearchResult{}, nil
	}

	// Build search query - find documents that contain the search terms
	var results []DocumentSearchResult

	// Simple term-based search (can be enhanced with TF-IDF, BM25, etc.)
	querySQL := `
		SELECT
			d.id,
			d.original_name,
			d.extracted_text,
			COUNT(DISTINCT si.term) as matching_terms,
			SUM(si.frequency) as total_matches,
			d.created_at
		FROM documents d
		JOIN document_search_index si ON d.id = si.document_id
		WHERE d.project_id = $1
			AND d.status = 'completed'
			AND si.term = ANY($2)
		GROUP BY d.id, d.original_name, d.extracted_text, d.created_at
		ORDER BY total_matches DESC, matching_terms DESC
		LIMIT $3
	`

	termStrings := make([]string, len(searchTerms))
	for i, term := range searchTerms {
		termStrings[i] = term.Term
	}

	rows, err := db.Query(querySQL, projectID, pq.Array(termStrings), limit)
	if err != nil {
		return nil, fmt.Errorf("search query failed: %w", err)
	}
	defer rows.Close()

	for rows.Next() {
		var result DocumentSearchResult
		err := rows.Scan(&result.DocumentID, &result.DocumentName, &result.Excerpt,
			&result.MatchingTerms, &result.TotalMatches, &result.CreatedAt)
		if err != nil {
			continue // Skip this result
		}

		// Generate excerpt with highlighted matches
		result.Excerpt = generateSearchExcerpt(result.Excerpt, searchTerms)

		results = append(results, result)
	}

	return results, nil
}

// DocumentSearchResult represents a search result
type DocumentSearchResult struct {
	DocumentID    string    `json:"document_id"`
	DocumentName  string    `json:"document_name"`
	Excerpt       string    `json:"excerpt"`
	MatchingTerms int       `json:"matching_terms"`
	TotalMatches  int       `json:"total_matches"`
	CreatedAt     time.Time `json:"created_at"`
}

// generateSearchExcerpt creates a highlighted excerpt around search matches
func generateSearchExcerpt(text string, searchTerms []SearchTerm) string {
	if len(text) <= 200 {
		return text
	}

	// Find first occurrence of any search term
	textLower := strings.ToLower(text)
	bestStart := -1
	bestTerm := ""

	for _, term := range searchTerms {
		idx := strings.Index(textLower, term.Term)
		if idx >= 0 && (bestStart == -1 || idx < bestStart) {
			bestStart = idx
			bestTerm = term.Term
		}
	}

	if bestStart == -1 {
		return text[:200] + "..."
	}

	// Extract excerpt around the match
	start := bestStart - 50
	if start < 0 {
		start = 0
	}
	end := start + 200
	if end > len(text) {
		end = len(text)
	}

	excerpt := text[start:end]
	if start > 0 {
		excerpt = "..." + excerpt
	}
	if end < len(text) {
		excerpt = excerpt + "..."
	}

	// Highlight the search term (simple replacement)
	excerpt = strings.ReplaceAll(excerpt, bestTerm, "**"+bestTerm+"**")

	return excerpt
}

// Document processing functions for different file types
func processPDFDocument(filePath string) (string, error) {
	// Try pdftotext first
	cmd := exec.Command("pdftotext", "-layout", filePath, "-")
	output, err := cmd.Output()
	if err == nil {
		return string(output), nil
	}

	// Fallback: try pdf2txt (python)
	cmd = exec.Command("python3", "-c", fmt.Sprintf(`
import sys
try:
    import pdfplumber
    with pdfplumber.open('%s') as pdf:
        text = ''
        for page in pdf.pages:
            text += page.extract_text() or ''
        print(text)
except ImportError:
    print('PDF processing not available - install pdfplumber')
    sys.exit(1)
`, filePath))

	output, err = cmd.Output()
	if err != nil {
		return "", fmt.Errorf("PDF processing failed: %w", err)
	}

	return string(output), nil
}

func processDOCXDocument(filePath string) (string, error) {
	// Try pandoc first
	cmd := exec.Command("pandoc", "-f", "docx", "-t", "plain", filePath)
	output, err := cmd.Output()
	if err == nil {
		return string(output), nil
	}

	// Fallback: try python-docx
	cmd = exec.Command("python3", "-c", fmt.Sprintf(`
import sys
try:
    from docx import Document
    doc = Document('%s')
    text = ''
    for para in doc.paragraphs:
        text += para.text + '\n'
    print(text)
except ImportError:
    # Final fallback: unzip and extract XML
    import zipfile
    import re
    try:
        with zipfile.ZipFile('%s', 'r') as zip_ref:
            with zip_ref.open('word/document.xml') as f:
                content = f.read().decode('utf-8')
                # Simple XML tag removal
                text = re.sub(r'<[^>]+>', ' ', content)
                text = re.sub(r'\s+', ' ', text)
                print(text.strip())
    except Exception as e:
        print(f'DOCX processing failed: {e}')
        sys.exit(1)
`, filePath, filePath))

	output, err = cmd.Output()
	if err != nil {
		return "", fmt.Errorf("DOCX processing failed: %w", err)
	}

	return string(output), nil
}

func processTextDocument(filePath string) (string, error) {
	content, err := os.ReadFile(filePath)
	if err != nil {
		return "", fmt.Errorf("failed to read text file: %w", err)
	}
	return string(content), nil
}

func processMarkdownDocument(filePath string) (string, error) {
	content, err := os.ReadFile(filePath)
	if err != nil {
		return "", fmt.Errorf("failed to read markdown file: %w", err)
	}

	// Simple markdown to text conversion (remove basic markdown syntax)
	text := string(content)

	// Remove headers
	text = regexp.MustCompile(`(?m)^#+\s+.*$`).ReplaceAllString(text, "")

	// Remove links
	text = regexp.MustCompile(`\[([^\]]+)\]\([^\)]+\)`).ReplaceAllString(text, "$1")

	// Remove bold/italic
	text = regexp.MustCompile(`\*\*([^\*]+)\*\*`).ReplaceAllString(text, "$1")
	text = regexp.MustCompile(`\*([^\*]+)\*`).ReplaceAllString(text, "$1")

	// Remove code blocks
	text = regexp.MustCompile("```[^`]*```").ReplaceAllString(text, "")

	return text, nil
}

func processGenericDocument(filePath string) (string, error) {
	// Try to detect file type and extract text
	content, err := os.ReadFile(filePath)
	if err != nil {
		return "", fmt.Errorf("failed to read file: %w", err)
	}

	// For unknown types, return raw content if it looks like text
	contentStr := string(content)

	// Check if it contains mostly printable characters
	printableChars := 0
	totalChars := len(contentStr)
	for _, r := range contentStr {
		if r == '\t' || r == '\n' || r == '\r' || (r >= 32 && r <= 126) {
			printableChars++
		}
	}

	if float64(printableChars)/float64(totalChars) > 0.8 {
		return contentStr, nil
	}

	return "", fmt.Errorf("unable to extract text from file type")
}

// Knowledge extraction from document text
func extractKnowledgeFromText(text, docID string) ([]KnowledgeItem, error) {
	var items []KnowledgeItem

	// Extract different types of knowledge items
	items = append(items, extractBusinessRulesFromText(text, docID)...)
	items = append(items, extractAPIDefinitions(text, docID)...)
	items = append(items, extractDataModels(text, docID)...)
	items = append(items, extractRequirements(text, docID)...)

	return items, nil
}

// extractBusinessRulesFromText extracts business rules from document text
func extractBusinessRulesFromText(text, docID string) []KnowledgeItem {
	var items []KnowledgeItem

	// Look for business rule patterns
	rulePatterns := []struct {
		pattern  *regexp.Regexp
		ruleType string
	}{
		{regexp.MustCompile(`(?i)(shall|should|must) ([^\n]+)`), "functional_requirement"},
		{regexp.MustCompile(`(?i)(performance|latency|throughput)[:\s]+([^\n]+)`), "performance_requirement"},
		{regexp.MustCompile(`(?i)(security|auth|encrypt)[:\s]+([^\n]+)`), "security_requirement"},
	}

	for _, pattern := range rulePatterns {
		matches := pattern.pattern.FindAllStringSubmatch(text, -1)
		for _, match := range matches {
			if len(match) > 1 {
				content := strings.TrimSpace(match[len(match)-1])
				if content != "" {
					items = append(items, KnowledgeItem{
						ID:         generateID(),
						Type:       pattern.ruleType,
						Title:      fmt.Sprintf("%s Rule", strings.Title(strings.ReplaceAll(pattern.ruleType, "_", " "))),
						Content:    content,
						Confidence: 0.7,
						SourcePage: 0,
						Status:     "extracted",
						StructuredData: map[string]interface{}{
							"rule_type": pattern.ruleType,
							"source":    "document_extraction",
						},
					})
				}
			}
		}
	}

	return items
}

// Knowledge item extraction functions
func extractAPIDefinitions(text, docID string) []KnowledgeItem {
	var items []KnowledgeItem

	// Look for API endpoint patterns
	apiPatterns := []struct {
		pattern *regexp.Regexp
		apiType string
	}{
		{regexp.MustCompile(`(?i)(GET|POST|PUT|DELETE)\s+(/[^\s]+)`), "api_endpoint"},
		{regexp.MustCompile(`(?i)(endpoint|api)[:\s]+([^\n]+)`), "api_definition"},
	}

	for _, pattern := range apiPatterns {
		matches := pattern.pattern.FindAllStringSubmatch(text, -1)
		for _, match := range matches {
			if len(match) > 2 {
				content := strings.TrimSpace(match[1] + " " + match[2])
				if len(match) > 2 {
					content = strings.TrimSpace(match[2])
				}
				items = append(items, KnowledgeItem{
					ID:             uuid.New().String(),
					Type:           pattern.apiType,
					Title:          "API Definition",
					Content:        content,
					Confidence:     0.7,
					Status:         "pending",
					StructuredData: map[string]interface{}{"source": "document_extraction", "pattern": match[0]},
				})
			}
		}
	}

	return items
}

func extractDataModels(text, docID string) []KnowledgeItem {
	var items []KnowledgeItem

	// Look for data model patterns
	modelPatterns := []struct {
		pattern   *regexp.Regexp
		modelType string
	}{
		{regexp.MustCompile(`(?i)(table|entity)[:\s]+([^\n]+)`), "data_table"},
		{regexp.MustCompile(`(?i)(field|column)[:\s]+([^\n]+)`), "data_field"},
		{regexp.MustCompile(`(?i)(schema|model)[:\s]+([^\n]+)`), "data_schema"},
	}

	for _, pattern := range modelPatterns {
		matches := pattern.pattern.FindAllStringSubmatch(text, -1)
		for _, match := range matches {
			if len(match) > 2 {
				items = append(items, KnowledgeItem{
					ID:             uuid.New().String(),
					Type:           pattern.modelType,
					Title:          "Data Model",
					Content:        strings.TrimSpace(match[2]),
					Confidence:     0.6,
					Status:         "pending",
					StructuredData: map[string]interface{}{"source": "document_extraction", "pattern": match[0]},
				})
			}
		}
	}

	return items
}

func extractRequirements(text, docID string) []KnowledgeItem {
	var items []KnowledgeItem

	// Look for requirement patterns
	reqPatterns := []struct {
		pattern *regexp.Regexp
		reqType string
	}{
		{regexp.MustCompile(`(?i)(shall|should|must|will) ([^\n]+)`), "functional_requirement"},
		{regexp.MustCompile(`(?i)(performance|latency|throughput)[:\s]+([^\n]+)`), "performance_requirement"},
		{regexp.MustCompile(`(?i)(security|auth|encrypt)[:\s]+([^\n]+)`), "security_requirement"},
	}

	for _, pattern := range reqPatterns {
		matches := pattern.pattern.FindAllStringSubmatch(text, -1)
		for _, match := range matches {
			if len(match) > 2 {
				content := strings.TrimSpace(match[1] + " " + match[2])
				items = append(items, KnowledgeItem{
					ID:             uuid.New().String(),
					Type:           pattern.reqType,
					Title:          "System Requirement",
					Content:        content,
					Confidence:     0.75,
					Status:         "pending",
					StructuredData: map[string]interface{}{"source": "document_extraction", "pattern": match[0]},
				})
			}
		}
	}

	return items
}

// KnowledgeItem represents extracted knowledge from documents
type ComplexityMetrics struct {
	Cyclomatic int `json:"cyclomatic"`
	Cognitive  int `json:"cognitive"`
	Nesting    int `json:"nesting"`
	Lines      int `json:"lines"`
	Functions  int `json:"functions"`
}

func calculateBasicComplexity(code, language string) ComplexityMetrics {
	lines := strings.Split(code, "\n")
	functionCount := 0
	nestingDepth := 0
	maxNesting := 0

	// Simple pattern matching for different languages
	switch language {
	case "javascript", "typescript":
		functionCount = strings.Count(code, "function") + strings.Count(code, "=>")
		// Count braces for nesting
		for _, line := range lines {
			braceCount := strings.Count(line, "{") - strings.Count(line, "}")
			nestingDepth += braceCount
			if nestingDepth > maxNesting {
				maxNesting = nestingDepth
			}
		}
	case "python":
		functionCount = strings.Count(code, "def ")
		// Count indentation for nesting (simplified)
		for _, line := range lines {
			if strings.HasPrefix(line, "    ") {
				indentLevel := len(strings.TrimLeft(line, " ")) / 4
				if indentLevel > maxNesting {
					maxNesting = indentLevel
				}
			}
		}
	case "go":
		functionCount = strings.Count(code, "func ")
		// Count braces for nesting
		for _, line := range lines {
			braceCount := strings.Count(line, "{") - strings.Count(line, "}")
			nestingDepth += braceCount
			if nestingDepth > maxNesting {
				maxNesting = nestingDepth
			}
		}
	}

	return ComplexityMetrics{
		Cyclomatic: functionCount + 1, // Simplified
		Cognitive:  functionCount * 2, // Simplified
		Nesting:    maxNesting,
		Lines:      len(lines),
		Functions:  functionCount,
	}
}

func analyzeBasicDeadCode(codebase, entryPoints []string, language string) []map[string]interface{} {
	// Placeholder implementation - would use AST to trace code reachability
	deadCode := []map[string]interface{}{}

	// Simple heuristic: files not in entry points might be dead
	for _, file := range codebase {
		isEntryPoint := false
		for _, entry := range entryPoints {
			if strings.Contains(file, entry) {
				isEntryPoint = true
				break
			}
		}

		if !isEntryPoint {
			deadCode = append(deadCode, map[string]interface{}{
				"file":       file,
				"type":       "potentially_unreachable",
				"confidence": 0.5,
				"reason":     "Not identified as entry point",
			})
		}
	}

	return deadCode
}

func analyzeBasicDependencies(files []string, language string, includeExternal bool) map[string]interface{} {
	// Placeholder implementation - would parse imports/exports using AST
	dependencies := map[string]interface{}{
		"internal": []map[string]interface{}{},
		"external": []map[string]interface{}{},
		"circular": []map[string]interface{}{},
	}

	// Simple analysis based on file extensions and content
	for _, file := range files {
		// This would be replaced with proper AST-based dependency analysis
		dependencies["internal"] = append(dependencies["internal"].([]map[string]interface{}), map[string]interface{}{
			"file":    file,
			"imports": []string{}, // Would be populated by AST analysis
			"exports": []string{}, // Would be populated by AST analysis
		})
	}

	return dependencies
}

func analyzeBasicTypeSafety(code, language string, strict bool) []map[string]interface{} {
	// Placeholder implementation - would use AST to check type safety
	issues := []map[string]interface{}{}

	// Simple pattern-based checks
	switch language {
	case "typescript":
		// Look for any usage without type annotations (simplified)
		if strings.Contains(code, "let ") && !strings.Contains(code, ": ") {
			issues = append(issues, map[string]interface{}{
				"type":     "missing_type_annotation",
				"severity": "warning",
				"message":  "Variable declared without type annotation",
				"line":     0, // Would be calculated by AST
			})
		}
	case "python":
		// Look for type hints usage
		if !strings.Contains(code, ": ") && strict {
			issues = append(issues, map[string]interface{}{
				"type":     "missing_type_hint",
				"severity": "info",
				"message":  "Function parameter without type hint",
				"line":     0,
			})
		}
	}

	return issues
}

func analyzeBasicPerformance(code, language string, context map[string]interface{}) map[string]interface{} {
	// Placeholder implementation - would use AST to identify performance issues
	performance := map[string]interface{}{
		"issues":        []map[string]interface{}{},
		"optimizations": []map[string]interface{}{},
		"score":         8.5, // Performance score out of 10
	}

	// Simple pattern-based performance checks
	switch language {
	case "javascript":
		// Check for potential performance issues
		if strings.Contains(code, "for (") && strings.Contains(code, ".length") {
			performance["issues"] = append(performance["issues"].([]map[string]interface{}), map[string]interface{}{
				"type":     "cache_array_length",
				"severity": "warning",
				"message":  "Cache array length in for loops for better performance",
			})
		}
	case "python":
		if strings.Contains(code, "for ") && strings.Contains(code, "range(") {
			performance["optimizations"] = append(performance["optimizations"].([]map[string]interface{}), map[string]interface{}{
				"type":    "list_comprehension",
				"message": "Consider using list comprehension for better performance",
			})
		}
	}

	return performance
}

// formatCodeHandler formats code according to language standards
func formatCodeHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	_, err := getProjectFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get project from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Code     string                 `json:"code"`
		Language string                 `json:"language"`
		Options  map[string]interface{} `json:"options,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Basic code formatting (placeholder - would use language-specific formatters)
	formattedCode := formatBasicCode(req.Code, req.Language, req.Options)

	result := map[string]interface{}{
		"originalCode":  req.Code,
		"formattedCode": formattedCode,
		"language":      req.Language,
		"changes":       len(formattedCode) != len(req.Code),
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// lintCodeHandler lints code for style and quality issues
func lintCodeHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	_, err := getProjectFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get project from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Code     string   `json:"code"`
		Language string   `json:"language"`
		Rules    []string `json:"rules,omitempty"`
		Strict   bool     `json:"strict,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Basic code linting (placeholder - would use language-specific linters)
	issues := lintBasicCode(req.Code, req.Language, req.Rules, req.Strict)

	result := map[string]interface{}{
		"issues":   issues,
		"language": req.Language,
		"rules":    req.Rules,
		"strict":   req.Strict,
		"total":    len(issues),
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// refactorCodeHandler suggests code refactoring improvements
func refactorCodeHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	_, err := getProjectFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get project from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Code            string                 `json:"code"`
		Language        string                 `json:"language"`
		RefactoringType string                 `json:"refactoringType"`
		Selection       map[string]interface{} `json:"selection,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Basic refactoring suggestions (placeholder - would use AST-based refactoring)
	suggestions := suggestBasicRefactoring(req.Code, req.Language, req.RefactoringType, req.Selection)

	result := map[string]interface{}{
		"suggestions":     suggestions,
		"refactoringType": req.RefactoringType,
		"language":        req.Language,
		"selection":       req.Selection,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// generateDocsHandler generates documentation for code
func generateDocsHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	_, err := getProjectFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get project from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Code            string `json:"code"`
		Language        string `json:"language"`
		Format          string `json:"format,omitempty"`
		IncludeExamples bool   `json:"includeExamples,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Basic documentation generation (placeholder - would use AST analysis)
	documentation := generateBasicDocs(req.Code, req.Language, req.Format, req.IncludeExamples)

	result := map[string]interface{}{
		"documentation":   documentation,
		"language":        req.Language,
		"format":          req.Format,
		"includeExamples": req.IncludeExamples,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// analyzeCoverageHandler analyzes test coverage
func analyzeCrossFileHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	_, err := getProjectFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get project from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Files          []string `json:"files"`
		Language       string   `json:"language"`
		EntryPoints    []string `json:"entryPoints,omitempty"`
		AnalysisDepth  string   `json:"analysisDepth,omitempty"`
		DetectCircular bool     `json:"detectCircular,omitempty"`
		FindUnused     bool     `json:"findUnused,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Set defaults
	if req.AnalysisDepth == "" {
		req.AnalysisDepth = "full"
	}
	detectCircular := req.DetectCircular
	if !req.DetectCircular && req.AnalysisDepth != "imports" {
		detectCircular = true // Default to true for deeper analysis
	}
	findUnused := req.FindUnused
	if !req.FindUnused && req.AnalysisDepth == "full" {
		findUnused = true // Default to true for full analysis
	}

	// Perform cross-file dependency analysis
	crossFileAnalysis := analyzeCrossFileDependencies(req.Files, req.Language, req.EntryPoints, req.AnalysisDepth, detectCircular, findUnused)

	result := map[string]interface{}{
		"filesAnalyzed":  len(req.Files),
		"language":       req.Language,
		"entryPoints":    req.EntryPoints,
		"analysisDepth":  req.AnalysisDepth,
		"detectCircular": detectCircular,
		"findUnused":     findUnused,
		"analysis":       crossFileAnalysis,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// Helper functions for code quality tools

func formatBasicCode(code, language string, options map[string]interface{}) string {
	// Basic code formatting - remove extra whitespace, normalize indentation
	lines := strings.Split(code, "\n")
	formatted := make([]string, 0, len(lines))

	for _, line := range lines {
		// Remove trailing whitespace
		line = strings.TrimRight(line, " \t")
		formatted = append(formatted, line)
	}

	// Remove empty lines at end
	for len(formatted) > 0 && formatted[len(formatted)-1] == "" {
		formatted = formatted[:len(formatted)-1]
	}

	return strings.Join(formatted, "\n")
}

func lintBasicCode(code, language string, rules []string, strict bool) []map[string]interface{} {
	issues := []map[string]interface{}{}
	lines := strings.Split(code, "\n")

	for i, line := range lines {
		lineNum := i + 1

		// Basic linting rules
		if strings.Contains(line, "console.log") && !contains(rules, "no-console") && (len(rules) == 0 || strict) {
			issues = append(issues, map[string]interface{}{
				"type":     "warning",
				"rule":     "no-console",
				"message":  "Unexpected console statement",
				"line":     lineNum,
				"column":   strings.Index(line, "console.log") + 1,
				"severity": "minor",
			})
		}

		if len(line) > 100 && (len(rules) == 0 || contains(rules, "max-line-length")) {
			issues = append(issues, map[string]interface{}{
				"type":     "warning",
				"rule":     "max-line-length",
				"message":  "Line too long",
				"line":     lineNum,
				"column":   101,
				"severity": "minor",
			})
		}
	}

	return issues
}

func suggestBasicRefactoring(code, language, refactoringType string, selection map[string]interface{}) []map[string]interface{} {
	suggestions := []map[string]interface{}{}

	switch refactoringType {
	case "extract_method":
		if len(code) > 200 {
			suggestions = append(suggestions, map[string]interface{}{
				"type":        "extract_method",
				"description": "Consider extracting this code into a separate method",
				"confidence":  0.7,
				"code":        "// Extracted method\nextractedFunction() {\n    // extracted code here\n}",
			})
		}
	case "rename_variable":
		if strings.Contains(code, "var ") || strings.Contains(code, "let ") {
			suggestions = append(suggestions, map[string]interface{}{
				"type":        "rename_variable",
				"description": "Variable name could be more descriptive",
				"confidence":  0.5,
				"suggestions": []string{"descriptiveName", "meaningfulVar"},
			})
		}
	}

	return suggestions
}

func generateBasicDocs(code, language, format string, includeExamples bool) map[string]interface{} {
	docs := map[string]interface{}{
		"functions": []map[string]interface{}{},
		"classes":   []map[string]interface{}{},
		"modules":   []string{},
	}

	lines := strings.Split(code, "\n")

	for i, line := range lines {
		if strings.Contains(line, "function ") || strings.Contains(line, "def ") || strings.Contains(line, "func ") {
			funcName := extractFunctionNameByLanguage(line, language)
			if funcName != "" {
				doc := map[string]interface{}{
					"name":        funcName,
					"line":        i + 1,
					"description": fmt.Sprintf("Function %s", funcName),
					"parameters":  []string{},
					"returns":     "unknown",
				}

				if includeExamples {
					doc["example"] = fmt.Sprintf("%s() // Call the %s function", funcName, funcName)
				}

				docs["functions"] = append(docs["functions"].([]map[string]interface{}), doc)
			}
		}
	}

	return docs
}

// =============================================================================
// API Versioning and Deprecation Management Functions
// =============================================================================

// getVersionInfo retrieves version information from database
func getVersionInfo(version string) (*APIVersion, error) {
	var v APIVersion
	var sunsetDate sql.NullTime

	err := db.QueryRow(`
		SELECT id, version, status, sunset_date, deprecated_by, changelog,
		       breaking_changes, migration_guide, created_at, updated_at
		FROM api_versions WHERE version = $1
	`, version).Scan(
		&v.ID, &v.Version, &v.Status, &sunsetDate, &v.DeprecatedBy, &v.Changelog,
		&v.BreakingChanges, &v.MigrationGuide, &v.CreatedAt, &v.UpdatedAt,
	)

	if err != nil {
		if err == sql.ErrNoRows {
			return nil, nil // Version not found, but not an error
		}
		return nil, err
	}

	if sunsetDate.Valid {
		v.SunsetDate = &sunsetDate.Time
	}

	return &v, nil
}

// getSupportedVersions returns all supported API versions
func getSupportedVersions() ([]APIVersion, error) {
	rows, err := db.Query(`
		SELECT id, version, status, sunset_date, deprecated_by, changelog,
		       breaking_changes, migration_guide, created_at, updated_at
		FROM api_versions
		WHERE status IN ('active', 'deprecated')
		ORDER BY version DESC
	`)

	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var versions []APIVersion
	for rows.Next() {
		var v APIVersion
		var sunsetDate sql.NullTime

		err := rows.Scan(
			&v.ID, &v.Version, &v.Status, &sunsetDate, &v.DeprecatedBy, &v.Changelog,
			&v.BreakingChanges, &v.MigrationGuide, &v.CreatedAt, &v.UpdatedAt,
		)
		if err != nil {
			continue
		}

		if sunsetDate.Valid {
			v.SunsetDate = &sunsetDate.Time
		}

		versions = append(versions, v)
	}

	return versions, nil
}

// getEndpointInfo retrieves endpoint information for deprecation warnings
func getEndpointInfo(path, method, version string) (*APIEndpoint, error) {
	var endpoint APIEndpoint
	var deprecatedIn, replacementPath, migrationNotes sql.NullString
	var sunsetDate sql.NullTime
	var tags pq.StringArray

	err := db.QueryRow(`
		SELECT id, path, method, version, description, deprecated, deprecated_in,
		       sunset_date, replacement_path, migration_notes, tags, created_at, updated_at
		FROM api_endpoints
		WHERE path = $1 AND method = $2 AND version = $3
	`, path, method, version).Scan(
		&endpoint.ID, &endpoint.Path, &endpoint.Method, &endpoint.Version,
		&endpoint.Description, &endpoint.Deprecated, &deprecatedIn, &sunsetDate,
		&replacementPath, &migrationNotes, &tags, &endpoint.CreatedAt, &endpoint.UpdatedAt,
	)

	if err != nil {
		return nil, err
	}

	endpoint.Tags = []string(tags)
	if deprecatedIn.Valid {
		endpoint.DeprecatedIn = deprecatedIn.String
	}
	if sunsetDate.Valid {
		endpoint.SunsetDate = &sunsetDate.Time
	}
	if replacementPath.Valid {
		endpoint.ReplacementPath = replacementPath.String
	}
	if migrationNotes.Valid {
		endpoint.MigrationNotes = migrationNotes.String
	}

	return &endpoint, nil
}

// API Versioning Management Handlers

// createAPIVersionHandler creates a new API version
func createAPIVersionHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Version         string `json:"version"`
		Status          string `json:"status,omitempty"`
		SunsetDate      string `json:"sunset_date,omitempty"`
		DeprecatedBy    string `json:"deprecated_by,omitempty"`
		Changelog       string `json:"changelog,omitempty"`
		BreakingChanges bool   `json:"breaking_changes,omitempty"`
		MigrationGuide  string `json:"migration_guide,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.Version == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "version",
			Message: "Version is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Validate version format
	if !strings.HasPrefix(req.Version, "v") || !regexp.MustCompile(`^v\d+$`).MatchString(req.Version) {
		WriteErrorResponse(w, &ValidationError{
			Field:   "version",
			Message: "Version must be in format v<number> (e.g., v1, v2)",
			Code:    "invalid_format",
		}, http.StatusBadRequest)
		return
	}

	// Set defaults
	if req.Status == "" {
		req.Status = "active"
	}

	// Validate status
	validStatuses := []string{"active", "deprecated", "sunset"}
	validStatus := false
	for _, status := range validStatuses {
		if req.Status == status {
			validStatus = true
			break
		}
	}
	if !validStatus {
		WriteErrorResponse(w, &ValidationError{
			Field:   "status",
			Message: fmt.Sprintf("Invalid status. Must be one of: %s", strings.Join(validStatuses, ", ")),
			Code:    "invalid_status",
		}, http.StatusBadRequest)
		return
	}

	// Parse sunset date if provided
	var sunsetDate *time.Time
	if req.SunsetDate != "" {
		parsedDate, err := time.Parse(time.RFC3339, req.SunsetDate)
		if err != nil {
			WriteErrorResponse(w, &ValidationError{
				Field:   "sunset_date",
				Message: "Invalid date format. Use RFC3339 format (e.g., 2024-12-31T23:59:59Z)",
				Code:    "invalid_date",
			}, http.StatusBadRequest)
			return
		}
		sunsetDate = &parsedDate
	}

	// Check if version already exists
	var existingID string
	db.QueryRow("SELECT id FROM api_versions WHERE version = $1", req.Version).Scan(&existingID)
	if existingID != "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "version",
			Message: "Version already exists",
			Code:    "duplicate_version",
		}, http.StatusConflict)
		return
	}

	// Create version
	versionID := uuid.New().String()
	now := time.Now()

	_, err := db.Exec(`
		INSERT INTO api_versions (
			id, version, status, sunset_date, deprecated_by, changelog,
			breaking_changes, migration_guide, created_at, updated_at
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
	`, versionID, req.Version, req.Status, sunsetDate, req.DeprecatedBy,
		req.Changelog, req.BreakingChanges, req.MigrationGuide, now, now)

	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "version_create",
			Message:       "Failed to create API version",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	version := APIVersion{
		ID:              versionID,
		Version:         req.Version,
		Status:          req.Status,
		SunsetDate:      sunsetDate,
		DeprecatedBy:    req.DeprecatedBy,
		Changelog:       req.Changelog,
		BreakingChanges: req.BreakingChanges,
		MigrationGuide:  req.MigrationGuide,
		CreatedAt:       now,
		UpdatedAt:       now,
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(version)
}

// getAPIVersionHandler retrieves a specific API version
func getAPIVersionHandler(w http.ResponseWriter, r *http.Request) {
	versionParam := chi.URLParam(r, "version")
	if versionParam == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "version",
			Message: "Version parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	version, err := getVersionInfo(versionParam)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "version_get",
			Message:       "Failed to get API version",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	if version == nil {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "api_version",
			ID:       versionParam,
			Message:  "API version not found",
		}, http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(version)
}

// listAPIVersionsHandler lists all API versions
func listAPIVersionsHandler(w http.ResponseWriter, r *http.Request) {
	versions, err := getSupportedVersions()
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "versions_list",
			Message:       "Failed to list API versions",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	result := map[string]interface{}{
		"versions": versions,
		"total":    len(versions),
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// updateAPIVersionHandler updates an API version
func updateAPIVersionHandler(w http.ResponseWriter, r *http.Request) {
	versionParam := chi.URLParam(r, "version")
	if versionParam == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "version",
			Message: "Version parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		Status          string `json:"status,omitempty"`
		SunsetDate      string `json:"sunset_date,omitempty"`
		DeprecatedBy    string `json:"deprecated_by,omitempty"`
		Changelog       string `json:"changelog,omitempty"`
		BreakingChanges *bool  `json:"breaking_changes,omitempty"`
		MigrationGuide  string `json:"migration_guide,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Build update query
	setParts := []string{}
	args := []interface{}{}
	argCount := 1

	if req.Status != "" {
		setParts = append(setParts, fmt.Sprintf("status = $%d", argCount))
		args = append(args, req.Status)
		argCount++
	}

	if req.SunsetDate != "" {
		sunsetDate, err := time.Parse(time.RFC3339, req.SunsetDate)
		if err != nil {
			WriteErrorResponse(w, &ValidationError{
				Field:   "sunset_date",
				Message: "Invalid date format",
				Code:    "invalid_date",
			}, http.StatusBadRequest)
			return
		}
		setParts = append(setParts, fmt.Sprintf("sunset_date = $%d", argCount))
		args = append(args, sunsetDate)
		argCount++
	}

	if req.DeprecatedBy != "" {
		setParts = append(setParts, fmt.Sprintf("deprecated_by = $%d", argCount))
		args = append(args, req.DeprecatedBy)
		argCount++
	}

	if req.Changelog != "" {
		setParts = append(setParts, fmt.Sprintf("changelog = $%d", argCount))
		args = append(args, req.Changelog)
		argCount++
	}

	if req.BreakingChanges != nil {
		setParts = append(setParts, fmt.Sprintf("breaking_changes = $%d", argCount))
		args = append(args, *req.BreakingChanges)
		argCount++
	}

	if req.MigrationGuide != "" {
		setParts = append(setParts, fmt.Sprintf("migration_guide = $%d", argCount))
		args = append(args, req.MigrationGuide)
		argCount++
	}

	if len(setParts) == 0 {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "No valid fields to update",
			Code:    "no_updates",
		}, http.StatusBadRequest)
		return
	}

	setParts = append(setParts, "updated_at = NOW()")

	query := fmt.Sprintf("UPDATE api_versions SET %s WHERE version = $%d",
		strings.Join(setParts, ", "), argCount)

	args = append(args, versionParam)

	result, err := db.Exec(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "version_update",
			Message:       "Failed to update API version",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	if rowsAffected, _ := result.RowsAffected(); rowsAffected == 0 {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "api_version",
			ID:       versionParam,
			Message:  "API version not found",
		}, http.StatusNotFound)
		return
	}

	// Return updated version
	version, err := getVersionInfo(versionParam)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "version_get_after_update",
			Message:       "Failed to get updated API version",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(version)
}

// createAPIEndpointHandler registers an API endpoint
func createAPIEndpointHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Path            string   `json:"path"`
		Method          string   `json:"method"`
		Version         string   `json:"version"`
		Description     string   `json:"description,omitempty"`
		Deprecated      bool     `json:"deprecated,omitempty"`
		DeprecatedIn    string   `json:"deprecated_in,omitempty"`
		SunsetDate      string   `json:"sunset_date,omitempty"`
		ReplacementPath string   `json:"replacement_path,omitempty"`
		MigrationNotes  string   `json:"migration_notes,omitempty"`
		Tags            []string `json:"tags,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.Path == "" || req.Method == "" || req.Version == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "path,method,version",
			Message: "Path, method, and version are required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Validate method
	validMethods := []string{"GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"}
	validMethod := false
	for _, method := range validMethods {
		if req.Method == method {
			validMethod = true
			break
		}
	}
	if !validMethod {
		WriteErrorResponse(w, &ValidationError{
			Field:   "method",
			Message: fmt.Sprintf("Invalid method. Must be one of: %s", strings.Join(validMethods, ", ")),
			Code:    "invalid_method",
		}, http.StatusBadRequest)
		return
	}

	// Parse sunset date if provided
	var sunsetDate *time.Time
	if req.SunsetDate != "" {
		parsedDate, err := time.Parse(time.RFC3339, req.SunsetDate)
		if err != nil {
			WriteErrorResponse(w, &ValidationError{
				Field:   "sunset_date",
				Message: "Invalid date format",
				Code:    "invalid_date",
			}, http.StatusBadRequest)
			return
		}
		sunsetDate = &parsedDate
	}

	// Check if endpoint already exists
	var existingID string
	db.QueryRow("SELECT id FROM api_endpoints WHERE path = $1 AND method = $2 AND version = $3",
		req.Path, req.Method, req.Version).Scan(&existingID)
	if existingID != "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "path,method,version",
			Message: "Endpoint already exists",
			Code:    "duplicate_endpoint",
		}, http.StatusConflict)
		return
	}

	// Create endpoint
	endpointID := uuid.New().String()
	now := time.Now()

	_, err := db.Exec(`
		INSERT INTO api_endpoints (
			id, path, method, version, description, deprecated, deprecated_in,
			sunset_date, replacement_path, migration_notes, tags, created_at, updated_at
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13)
	`, endpointID, req.Path, req.Method, req.Version, req.Description, req.Deprecated,
		req.DeprecatedIn, sunsetDate, req.ReplacementPath, req.MigrationNotes,
		pq.Array(req.Tags), now, now)

	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "endpoint_create",
			Message:       "Failed to create API endpoint",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	endpoint := APIEndpoint{
		ID:              endpointID,
		Path:            req.Path,
		Method:          req.Method,
		Version:         req.Version,
		Description:     req.Description,
		Deprecated:      req.Deprecated,
		DeprecatedIn:    req.DeprecatedIn,
		SunsetDate:      sunsetDate,
		ReplacementPath: req.ReplacementPath,
		MigrationNotes:  req.MigrationNotes,
		Tags:            req.Tags,
		CreatedAt:       now,
		UpdatedAt:       now,
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(endpoint)
}

// getVersionCompatibilityHandler generates compatibility report between versions
func getVersionCompatibilityHandler(w http.ResponseWriter, r *http.Request) {
	fromVersion := r.URL.Query().Get("from")
	toVersion := r.URL.Query().Get("to")

	if fromVersion == "" || toVersion == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "from,to",
			Message: "Both 'from' and 'to' version parameters are required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	report := generateCompatibilityReport(fromVersion, toVersion)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(report)
}

// generateCompatibilityReport creates a version compatibility report
func generateCompatibilityReport(fromVersion, toVersion string) VersionCompatibilityReport {
	report := VersionCompatibilityReport{
		FromVersion: fromVersion,
		ToVersion:   toVersion,
		GeneratedAt: time.Now(),
	}

	// Get migrations between versions
	migrations, err := getVersionMigrations(fromVersion, toVersion)
	if err == nil {
		report.Migrations = migrations
	}

	// Analyze compatibility
	compatibility := "full"
	var breakingChanges []string
	var recommendations []string

	// Check for breaking changes
	for _, migration := range migrations {
		if migration.MigrationType == "breaking_change" {
			compatibility = "breaking"
			breakingChanges = append(breakingChanges,
				fmt.Sprintf("Breaking change in %s %s", migration.Method, migration.EndpointPath))
		} else if migration.MigrationType != "path_change" && !migration.BackwardCompatible {
			if compatibility == "full" {
				compatibility = "partial"
			}
		}
	}

	if compatibility == "breaking" {
		recommendations = append(recommendations,
			"Complete migration required - breaking changes detected")
		recommendations = append(recommendations,
			"Test all integrations thoroughly before migration")
	} else if compatibility == "partial" {
		recommendations = append(recommendations,
			"Review migration guides for backward compatibility")
		recommendations = append(recommendations,
			"Test integrations with new version")
	} else {
		recommendations = append(recommendations,
			"Compatible upgrade - minimal testing required")
	}

	report.Compatibility = compatibility
	report.BreakingChanges = breakingChanges
	report.Recommendations = recommendations

	return report
}

// getVersionMigrations retrieves migrations between versions
func getVersionMigrations(fromVersion, toVersion string) ([]VersionMigration, error) {
	rows, err := db.Query(`
		SELECT id, from_version, to_version, endpoint_path, method, migration_type,
		       transformation_rules, backward_compatible, created_at
		FROM version_migrations
		WHERE (from_version = $1 AND to_version = $2) OR (from_version = $2 AND to_version = $1)
		ORDER BY created_at
	`, fromVersion, toVersion)

	if err != nil {
		return nil, err
	}
	defer rows.Close()

	var migrations []VersionMigration
	for rows.Next() {
		var migration VersionMigration
		var transformationRules []byte

		err := rows.Scan(&migration.ID, &migration.FromVersion, &migration.ToVersion,
			&migration.EndpointPath, &migration.Method, &migration.MigrationType,
			&transformationRules, &migration.BackwardCompatible, &migration.CreatedAt)
		if err != nil {
			continue
		}

		json.Unmarshal(transformationRules, &migration.TransformationRules)
		migrations = append(migrations, migration)
	}

	return migrations, nil
}

// createVersionMigrationHandler creates a version migration rule
func createVersionMigrationHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		FromVersion         string                 `json:"from_version"`
		ToVersion           string                 `json:"to_version"`
		EndpointPath        string                 `json:"endpoint_path"`
		Method              string                 `json:"method"`
		MigrationType       string                 `json:"migration_type"`
		TransformationRules map[string]interface{} `json:"transformation_rules,omitempty"`
		BackwardCompatible  bool                   `json:"backward_compatible,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.FromVersion == "" || req.ToVersion == "" || req.EndpointPath == "" || req.Method == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "from_version,to_version,endpoint_path,method",
			Message: "All migration fields are required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Validate migration type
	validTypes := []string{"path_change", "parameter_change", "response_change", "breaking_change"}
	validType := false
	for _, t := range validTypes {
		if req.MigrationType == t {
			validType = true
			break
		}
	}
	if !validType {
		WriteErrorResponse(w, &ValidationError{
			Field:   "migration_type",
			Message: fmt.Sprintf("Invalid migration type. Must be one of: %s", strings.Join(validTypes, ", ")),
			Code:    "invalid_type",
		}, http.StatusBadRequest)
		return
	}

	// Validate method
	validMethods := []string{"GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"}
	validMethod := false
	for _, method := range validMethods {
		if req.Method == method {
			validMethod = true
			break
		}
	}
	if !validMethod {
		WriteErrorResponse(w, &ValidationError{
			Field:   "method",
			Message: fmt.Sprintf("Invalid method. Must be one of: %s", strings.Join(validMethods, ", ")),
			Code:    "invalid_method",
		}, http.StatusBadRequest)
		return
	}

	// Check if migration already exists
	var existingID string
	db.QueryRow(`
		SELECT id FROM version_migrations
		WHERE from_version = $1 AND to_version = $2 AND endpoint_path = $3 AND method = $4
	`, req.FromVersion, req.ToVersion, req.EndpointPath, req.Method).Scan(&existingID)
	if existingID != "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "from_version,to_version,endpoint_path,method",
			Message: "Migration rule already exists",
			Code:    "duplicate_migration",
		}, http.StatusConflict)
		return
	}

	// Create migration
	migrationID := uuid.New().String()
	transformationRulesJSON, _ := json.Marshal(req.TransformationRules)
	now := time.Now()

	_, err := db.Exec(`
		INSERT INTO version_migrations (
			id, from_version, to_version, endpoint_path, method, migration_type,
			transformation_rules, backward_compatible, created_at
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
	`, migrationID, req.FromVersion, req.ToVersion, req.EndpointPath, req.Method,
		req.MigrationType, transformationRulesJSON, req.BackwardCompatible, now)

	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "migration_create",
			Message:       "Failed to create version migration",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	migration := VersionMigration{
		ID:                  migrationID,
		FromVersion:         req.FromVersion,
		ToVersion:           req.ToVersion,
		EndpointPath:        req.EndpointPath,
		Method:              req.Method,
		MigrationType:       req.MigrationType,
		TransformationRules: req.TransformationRules,
		BackwardCompatible:  req.BackwardCompatible,
		CreatedAt:           now,
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(migration)
}

// =============================================================================
// MCP ERROR CLASSIFICATION & MONITORING HANDLERS - PHASE 3
// =============================================================================

// getErrorDashboardHandler provides comprehensive error monitoring dashboard
func getErrorDashboardHandler(w http.ResponseWriter, r *http.Request) {
	dashboard := getErrorDashboardData()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"dashboard":    dashboard,
		"api_version":  "v1",
		"generated_at": time.Now().Format(time.RFC3339),
	})
}

// getErrorAnalysisHandler provides detailed error pattern analysis
func getErrorAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	// Parse time window from query parameters
	timeWindowStr := r.URL.Query().Get("window")
	timeWindow := 24 * time.Hour // default 24 hours

	if timeWindowStr != "" {
		if parsed, err := time.ParseDuration(timeWindowStr); err == nil {
			timeWindow = parsed
		}
	}

	analysis := analyzeErrorPatterns(timeWindow)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analysis":    analysis,
		"time_window": timeWindow.String(),
		"api_version": "v1",
	})
}

// classifyErrorHandler allows manual error classification for testing/debugging
func classifyErrorHandler(w http.ResponseWriter, r *http.Request) {
	errorCodeStr := r.URL.Query().Get("code")
	message := r.URL.Query().Get("message")

	if errorCodeStr == "" || message == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "code,message",
			Message: "Both 'code' and 'message' query parameters are required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	errorCode, err := strconv.Atoi(errorCodeStr)
	if err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "code",
			Message: "Error code must be a valid integer",
			Code:    "invalid_format",
		}, http.StatusBadRequest)
		return
	}

	classification := classifyMCPError(errorCode, message, fmt.Errorf(message))

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"classification": classification,
		"input": map[string]interface{}{
			"error_code": errorCode,
			"message":    message,
		},
		"api_version": "v1",
	})
}

// getErrorStatsHandler provides current error statistics
func getErrorStatsHandler(w http.ResponseWriter, r *http.Request) {
	stats := getErrorStatistics()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"statistics":  stats,
		"api_version": "v1",
		"timestamp":   time.Now().Format(time.RFC3339),
	})
}

// reportErrorHandler allows clients to report errors for aggregation
func reportErrorHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		ErrorCode int         `json:"error_code"`
		Message   string      `json:"message"`
		Details   interface{} `json:"details,omitempty"`
		RequestID string      `json:"request_id,omitempty"`
		ToolName  string      `json:"tool_name,omitempty"`
		UserAgent string      `json:"user_agent,omitempty"`
		ClientIP  string      `json:"client_ip,omitempty"`
		Severity  int         `json:"severity,omitempty"`
		Category  string      `json:"category,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.Message == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "message",
			Message: "Error message is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Create error classification
	classification := ErrorClassification{
		ErrorCode:   req.ErrorCode,
		Timestamp:   time.Now(),
		RequestID:   req.RequestID,
		ToolName:    req.ToolName,
		UserVisible: true,
		Retryable:   false,
	}

	// Override with provided classification if available
	if req.Severity > 0 {
		classification.Severity = ErrorSeverity(req.Severity)
	}
	if req.Category != "" {
		classification.Category = req.Category
	}

	// Classify if not fully provided
	if classification.Severity == 0 || classification.Category == "" {
		fullClassification := classifyMCPError(req.ErrorCode, req.Message, fmt.Errorf(req.Message))
		if classification.Severity == 0 {
			classification.Severity = fullClassification.Severity
		}
		if classification.Category == "" {
			classification.Category = fullClassification.Category
		}
		classification.Recovery = fullClassification.Recovery
		classification.Retryable = fullClassification.Retryable
		classification.Suggestions = fullClassification.Suggestions
		classification.RelatedErrors = fullClassification.RelatedErrors
		classification.Context = fullClassification.Context
	}

	// Track the reported error
	trackError(classification)

	// Log the reported error
	LogInfo(context.Background(), fmt.Sprintf("Error reported via API: %s (code: %d, severity: %s)", req.Message, req.ErrorCode, getSeverityLabel(classification.Severity)), "request_id", req.RequestID, "tool_name", req.ToolName, "client_ip", req.ClientIP)

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusAccepted)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status":         "reported",
		"classification": classification,
		"error_id":       fmt.Sprintf("reported_%d", time.Now().Unix()),
		"api_version":    "v1",
	})
}

// =============================================================================
// MCP WORKFLOW ORCHESTRATION HANDLERS - PHASE 4
// =============================================================================

// getWorkflowsHandler returns all registered workflows
func getWorkflowsHandler(w http.ResponseWriter, r *http.Request) {
	workflows := ListWorkflows()

	result := make([]map[string]interface{}, len(workflows))
	for i, workflow := range workflows {
		result[i] = map[string]interface{}{
			"id":          workflow.ID,
			"name":        workflow.Name,
			"description": workflow.Description,
			"version":     workflow.Version,
			"steps":       len(workflow.Steps),
			"created_at":  workflow.CreatedAt.Format(time.RFC3339),
			"updated_at":  workflow.UpdatedAt.Format(time.RFC3339),
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"workflows":   result,
		"count":       len(result),
		"api_version": "v1",
	})
}

// getWorkflowHandler returns a specific workflow
func getWorkflowHandler(w http.ResponseWriter, r *http.Request) {
	workflowID := chi.URLParam(r, "id")
	if workflowID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Workflow ID parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	workflow, err := GetWorkflow(workflowID)
	if err != nil {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "workflow",
			ID:       workflowID,
			Message:  "Workflow not found",
		}, http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"workflow":    workflow,
		"api_version": "v1",
	})
}

// createWorkflowHandler creates a new workflow
func createWorkflowHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Workflow WorkflowDefinition `json:"workflow"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	err := RegisterWorkflow(req.Workflow)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "workflow_create",
			Message:       "Failed to create workflow",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"workflow":    req.Workflow,
		"status":      "created",
		"api_version": "v1",
	})
}

// deleteWorkflowHandler deletes a workflow
func deleteWorkflowHandler(w http.ResponseWriter, r *http.Request) {
	workflowID := chi.URLParam(r, "id")
	if workflowID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Workflow ID parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// For now, just return not implemented since we don't have delete functionality
	WriteErrorResponse(w, &NotImplementedError{
		Resource: "workflow_deletion",
		Message:  "Workflow deletion is not yet implemented",
	}, http.StatusNotImplemented)
}

// executeWorkflowHandler executes a workflow
func executeWorkflowHandler(w http.ResponseWriter, r *http.Request) {
	workflowID := chi.URLParam(r, "id")
	if workflowID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Workflow ID parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		InputData map[string]interface{} `json:"input_data,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		// Body might be empty, which is OK
		if err != io.EOF {
			WriteErrorResponse(w, &ValidationError{
				Field:   "body",
				Message: "Invalid JSON",
				Code:    "invalid_json",
			}, http.StatusBadRequest)
			return
		}
	}

	if req.InputData == nil {
		req.InputData = make(map[string]interface{})
	}

	requestID := fmt.Sprintf("api_%d", time.Now().UnixNano())
	execution, err := ExecuteWorkflow(workflowID, req.InputData, requestID)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "workflow_execute",
			Message:       "Failed to execute workflow",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusAccepted)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"execution":   execution,
		"status":      "started",
		"api_version": "v1",
	})
}

// getWorkflowExecutionsHandler returns workflow executions
func getWorkflowExecutionsHandler(w http.ResponseWriter, r *http.Request) {
	workflowID := r.URL.Query().Get("workflow_id")
	statusStr := r.URL.Query().Get("status")
	limitStr := r.URL.Query().Get("limit")

	var limit int
	if limitStr != "" {
		if parsed, err := strconv.Atoi(limitStr); err == nil && parsed > 0 {
			limit = parsed
		}
	}

	var status models.WorkflowStatus
	if statusStr != "" {
		// Convert string to WorkflowStatus enum
		switch statusStr {
		case "pending":
			status = models.WorkflowStatusPending
		case "running":
			status = models.WorkflowStatusRunning
		case "completed":
			status = models.WorkflowStatusCompleted
		case "failed":
			status = models.WorkflowStatusFailed
		case "cancelled":
			status = models.WorkflowStatusCancelled
		default:
			status = models.WorkflowStatusPending
		}
	}

	executions := ListWorkflowExecutions(workflowID, status, limit)

	result := make([]map[string]interface{}, len(executions))
	for i, execution := range executions {
		result[i] = map[string]interface{}{
			"execution_id": execution.ID,
			"workflow_id":  execution.WorkflowID,
			"status":       execution.Status,
			"progress":     execution.GetProgress(),
			"started_at":   execution.StartedAt.Format(time.RFC3339),
		}

		if execution.CompletedAt != nil {
			result[i]["completed_at"] = execution.CompletedAt.Format(time.RFC3339)
			result[i]["duration_ms"] = execution.Duration.Milliseconds()
		}

		if execution.Error != nil {
			result[i]["error"] = map[string]interface{}{
				"code":    execution.Error.Code,
				"message": execution.Error.Message,
			}
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"executions": result,
		"count":      len(result),
		"filters": map[string]interface{}{
			"workflow_id": workflowID,
			"status":      statusStr,
			"limit":       limit,
		},
		"api_version": "v1",
	})
}

// getWorkflowExecutionHandler returns a specific workflow execution
func getWorkflowExecutionHandler(w http.ResponseWriter, r *http.Request) {
	executionID := chi.URLParam(r, "id")
	if executionID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Execution ID parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	execution, err := GetWorkflowExecution(executionID)
	if err != nil {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "workflow_execution",
			ID:       executionID,
			Message:  "Workflow execution not found",
		}, http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"execution":   execution,
		"api_version": "v1",
	})
}

// cancelWorkflowExecutionHandler cancels a workflow execution
func cancelWorkflowExecutionHandler(w http.ResponseWriter, r *http.Request) {
	executionID := chi.URLParam(r, "id")
	if executionID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Execution ID parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	err := CancelWorkflowExecution(executionID)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "workflow_cancel",
			Message:       "Failed to cancel workflow execution",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"execution_id": executionID,
		"status":       "cancelled",
		"cancelled_at": time.Now().Format(time.RFC3339),
		"api_version":  "v1",
	})
}

// =============================================================================
// MCP PERFORMANCE MONITORING & OPTIMIZATION HANDLERS - PHASE 4.2
// =============================================================================

// getPerformanceDashboardHandler returns the comprehensive performance dashboard
func getPerformanceDashboardHandler(w http.ResponseWriter, r *http.Request) {
	dashboard := getPerformanceDashboard()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"dashboard":   dashboard,
		"api_version": "v1",
	})
}

// getToolPerformanceMetricsHandler returns tool performance metrics
func getToolPerformanceMetricsHandler(w http.ResponseWriter, r *http.Request) {
	toolName := chi.URLParam(r, "tool")
	sortBy := r.URL.Query().Get("sort_by")
	if sortBy == "" {
		sortBy = "performance_score"
	}

	limitStr := r.URL.Query().Get("limit")
	limit := 20
	if limitStr != "" {
		if parsed, err := strconv.Atoi(limitStr); err == nil && parsed > 0 {
			limit = parsed
		}
	}

	// Simulate the MCP tool call
	args := map[string]interface{}{
		"tool_name": toolName,
		"sort_by":   sortBy,
		"limit":     float64(limit),
	}

	response := handleToolPerformanceMetrics("api_request", args)

	if response.Error != nil {
		WriteErrorResponse(w, &InternalServerError{
			Message:       "Failed to get tool performance metrics",
			OriginalError: fmt.Errorf(response.Error.Message),
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response.Result)
}

// getWorkflowPerformanceMetricsHandler returns workflow performance metrics
func getWorkflowPerformanceMetricsHandler(w http.ResponseWriter, r *http.Request) {
	workflowID := chi.URLParam(r, "workflow")
	sortBy := r.URL.Query().Get("sort_by")
	if sortBy == "" {
		sortBy = "success_rate"
	}

	limitStr := r.URL.Query().Get("limit")
	limit := 20
	if limitStr != "" {
		if parsed, err := strconv.Atoi(limitStr); err == nil && parsed > 0 {
			limit = parsed
		}
	}

	args := map[string]interface{}{
		"workflow_id": workflowID,
		"sort_by":     sortBy,
		"limit":       float64(limit),
	}

	response := handleWorkflowPerformanceMetrics("api_request", args)

	if response.Error != nil {
		WriteErrorResponse(w, &InternalServerError{
			Message:       "Failed to get workflow performance metrics",
			OriginalError: fmt.Errorf(response.Error.Message),
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response.Result)
}

// getPerformanceRecommendationsHandler returns optimization recommendations
func getPerformanceRecommendationsHandler(w http.ResponseWriter, r *http.Request) {
	priorityFilter := r.URL.Query().Get("priority")
	typeFilter := r.URL.Query().Get("type")

	limitStr := r.URL.Query().Get("limit")
	limit := 10
	if limitStr != "" {
		if parsed, err := strconv.Atoi(limitStr); err == nil && parsed > 0 {
			limit = parsed
		}
	}

	args := map[string]interface{}{
		"priority_filter": priorityFilter,
		"type_filter":     typeFilter,
		"limit":           float64(limit),
	}

	response := handlePerformanceOptimizationRecommendations(args)

	if response.Error != nil {
		WriteErrorResponse(w, &InternalServerError{
			Message:       "Failed to get performance recommendations",
			OriginalError: fmt.Errorf(response.Error.Message),
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response.Result)
}

// getPerformanceBottlenecksHandler analyzes and returns performance bottlenecks
func getPerformanceBottlenecksHandler(w http.ResponseWriter, r *http.Request) {
	analysisType := r.URL.Query().Get("type")
	if analysisType == "" {
		analysisType = "comprehensive"
	}

	severityFilter := r.URL.Query().Get("severity")

	includeRecStr := r.URL.Query().Get("include_recommendations")
	includeRecommendations := true
	if includeRecStr == "false" {
		includeRecommendations = false
	}

	args := map[string]interface{}{
		"analysis_type":           analysisType,
		"severity_filter":         severityFilter,
		"include_recommendations": includeRecommendations,
	}

	response := handleAnalyzePerformanceBottlenecks(args)

	if response.Error != nil {
		WriteErrorResponse(w, &InternalServerError{
			Message:       "Failed to analyze performance bottlenecks",
			OriginalError: fmt.Errorf(response.Error.Message),
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response.Result)
}

// getSystemPerformanceSnapshotHandler returns current system performance snapshot
func getSystemPerformanceSnapshotHandler(w http.ResponseWriter, r *http.Request) {
	perfMutex.RLock()
	defer perfMutex.RUnlock()

	if len(performanceMonitor.SystemSnapshots) == 0 {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "system_performance_snapshot",
			Message:  "No system performance snapshots available",
		}, http.StatusNotFound)
		return
	}

	// Return the most recent snapshot
	latestSnapshot := performanceMonitor.SystemSnapshots[len(performanceMonitor.SystemSnapshots)-1]

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"snapshot":    latestSnapshot,
		"api_version": "v1",
	})
}

// getPerformanceAnalyticsHandler returns advanced performance analytics
func getPerformanceAnalyticsHandler(w http.ResponseWriter, r *http.Request) {
	timeRange := r.URL.Query().Get("time_range")
	if timeRange == "" {
		timeRange = "24h"
	}

	perfMutex.RLock()
	defer perfMutex.RUnlock()

	analytics := map[string]interface{}{
		"timestamp":  time.Now().Format(time.RFC3339),
		"time_range": timeRange,
		"analytics": map[string]interface{}{
			"tool_performance_trends":     analyzeToolPerformanceTrends(timeRange),
			"workflow_performance_trends": analyzeWorkflowPerformanceTrends(timeRange),
			"system_health_trends":        analyzeSystemHealthTrends(timeRange),
			"bottleneck_analysis":         analyzeBottleneckTrends(timeRange),
			"optimization_impact":         analyzeOptimizationImpact(timeRange),
		},
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analytics":   analytics,
		"api_version": "v1",
	})
}

// analyzeToolPerformanceTrends analyzes tool performance trends over time
func analyzeToolPerformanceTrends(timeRange string) map[string]interface{} {
	// Simplified trend analysis - in production, this would analyze historical data
	trends := map[string]interface{}{
		"improving_tools": []string{},
		"declining_tools": []string{},
		"stable_tools":    []string{},
		"trend_period":    timeRange,
	}

	// Analyze each tool's recent performance
	for toolName, metrics := range performanceMonitor.ToolMetrics {
		if metrics.TotalExecutions < 10 {
			continue // Not enough data
		}

		// Simple analysis based on current metrics
		if metrics.PerformanceScore > 80 {
			trends["improving_tools"] = append(trends["improving_tools"].([]string), toolName)
		} else if metrics.PerformanceScore < 60 {
			trends["declining_tools"] = append(trends["declining_tools"].([]string), toolName)
		} else {
			trends["stable_tools"] = append(trends["stable_tools"].([]string), toolName)
		}
	}

	return trends
}

// analyzeWorkflowPerformanceTrends analyzes workflow performance trends
func analyzeWorkflowPerformanceTrends(timeRange string) map[string]interface{} {
	trends := map[string]interface{}{
		"improving_workflows": []string{},
		"declining_workflows": []string{},
		"stable_workflows":    []string{},
		"trend_period":        timeRange,
	}

	for workflowID, metrics := range performanceMonitor.WorkflowMetrics {
		if metrics.TotalExecutions < 5 {
			continue // Not enough data
		}

		if metrics.SuccessRate > 0.9 {
			trends["improving_workflows"] = append(trends["improving_workflows"].([]string), workflowID)
		} else if metrics.SuccessRate < 0.7 {
			trends["declining_workflows"] = append(trends["declining_workflows"].([]string), workflowID)
		} else {
			trends["stable_workflows"] = append(trends["stable_workflows"].([]string), workflowID)
		}
	}

	return trends
}

// analyzeSystemHealthTrends analyzes overall system health trends
func analyzeSystemHealthTrends(timeRange string) map[string]interface{} {
	trends := map[string]interface{}{
		"current_health_score": calculateOverallSystemHealth(),
		"health_trend":         "stable", // Would analyze historical snapshots
		"trend_period":         timeRange,
		"health_indicators": map[string]interface{}{
			"tool_performance_avg": calculateAverageToolPerformance(),
			"workflow_success_avg": calculateAverageWorkflowSuccess(),
			"bottleneck_count":     countTotalBottlenecks(),
		},
	}

	return trends
}

// analyzeBottleneckTrends analyzes bottleneck trends over time
func analyzeBottleneckTrends(timeRange string) map[string]interface{} {
	trends := map[string]interface{}{
		"new_bottlenecks":        []string{},
		"resolved_bottlenecks":   []string{},
		"persistent_bottlenecks": []string{},
		"trend_period":           timeRange,
	}

	// Simplified - would analyze historical bottleneck data
	totalBottlenecks := countTotalBottlenecks()
	trends["current_bottleneck_count"] = totalBottlenecks

	if totalBottlenecks > 10 {
		trends["bottleneck_trend"] = "increasing"
	} else if totalBottlenecks < 5 {
		trends["bottleneck_trend"] = "decreasing"
	} else {
		trends["bottleneck_trend"] = "stable"
	}

	return trends
}

// analyzeOptimizationImpact analyzes the impact of implemented optimizations
func analyzeOptimizationImpact(timeRange string) map[string]interface{} {
	impact := map[string]interface{}{
		"implemented_optimizations":  len(performanceMonitor.Optimizations),
		"estimated_performance_gain": 0.0, // Would calculate based on optimization history
		"optimization_success_rate":  0.0, // Would track optimization effectiveness
		"trend_period":               timeRange,
	}

	return impact
}

// calculateAverageToolPerformance calculates average tool performance score
func calculateAverageToolPerformance() float64 {
	if len(performanceMonitor.ToolMetrics) == 0 {
		return 100.0
	}

	totalScore := 0.0
	for _, metrics := range performanceMonitor.ToolMetrics {
		totalScore += metrics.PerformanceScore
	}

	return totalScore / float64(len(performanceMonitor.ToolMetrics))
}

// calculateAverageWorkflowSuccess calculates average workflow success rate
func calculateAverageWorkflowSuccess() float64 {
	if len(performanceMonitor.WorkflowMetrics) == 0 {
		return 1.0
	}

	totalSuccess := 0.0
	for _, metrics := range performanceMonitor.WorkflowMetrics {
		totalSuccess += metrics.SuccessRate
	}

	return totalSuccess / float64(len(performanceMonitor.WorkflowMetrics))
}

// =============================================================================
// MCP GRACEFUL DEGRADATION HANDLERS - PHASE 3.2
// =============================================================================

// getDegradationStatusHandler returns current system degradation status
func getDegradationStatusHandler(w http.ResponseWriter, r *http.Request) {
	status := getCurrentDegradationState()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"degradation_status": status,
		"circuit_breakers":   getCircuitBreakerSummary(),
		"system_health":      getSystemHealthScore(),
		"api_version":        "v1",
	})
}

// getCircuitBreakersHandler returns all circuit breaker statuses
func getCircuitBreakersHandler(w http.ResponseWriter, r *http.Request) {
	summary := getCircuitBreakerSummary()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"circuit_breakers": summary,
		"total_count":      len(summary),
		"api_version":      "v1",
	})
}

// getCircuitBreakerHandler returns specific circuit breaker status
func getCircuitBreakerHandler(w http.ResponseWriter, r *http.Request) {
	toolName := chi.URLParam(r, "tool")
	if toolName == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "tool",
			Message: "Tool name parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	cb := getCircuitBreaker(toolName)
	if cb == nil {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "circuit_breaker",
			ID:       toolName,
			Message:  "Circuit breaker not found for tool",
		}, http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"circuit_breaker": cb,
		"api_version":     "v1",
	})
}

// resetCircuitBreakerHandler resets a circuit breaker to closed state
func resetCircuitBreakerHandler(w http.ResponseWriter, r *http.Request) {
	toolName := chi.URLParam(r, "tool")
	if toolName == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "tool",
			Message: "Tool name parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	cb := getCircuitBreaker(toolName)
	if cb == nil {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "circuit_breaker",
			ID:       toolName,
			Message:  "Circuit breaker not found for tool",
		}, http.StatusNotFound)
		return
	}

	// Reset circuit breaker
	cb.mutex.Lock()
	cb.State = CircuitClosed
	cb.FailureCount = 0
	cb.SuccessCount = 0
	cb.LastFailure = time.Time{}
	cb.mutex.Unlock()

	// Log circuit breaker reset
	log.Printf("Circuit breaker manually reset: tool=%s, reset_by=api_call", toolName)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"status":          "reset",
		"circuit_breaker": cb,
		"message":         fmt.Sprintf("Circuit breaker for %s has been reset to closed state", toolName),
		"api_version":     "v1",
	})
}

// getFallbackStrategiesHandler returns all fallback strategies
func getFallbackStrategiesHandler(w http.ResponseWriter, r *http.Request) {
	strategies := getAllFallbackStrategies()

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"fallback_strategies": strategies,
		"total_count":         len(strategies),
		"api_version":         "v1",
	})
}

// getFallbackStrategyHandler returns specific fallback strategy
func getFallbackStrategyHandler(w http.ResponseWriter, r *http.Request) {
	toolName := chi.URLParam(r, "tool")
	if toolName == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "tool",
			Message: "Tool name parameter is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	strategy := getFallbackStrategy(toolName)
	if strategy == nil {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "fallback_strategy",
			ID:       toolName,
			Message:  "Fallback strategy not found for tool",
		}, http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"fallback_strategy": strategy,
		"api_version":       "v1",
	})
}

// initializeDefaultAPIVersion creates the default v1 API version if it doesn't exist
func initializeDefaultAPIVersion() {
	versionID := uuid.New().String()
	now := time.Now()

	_, err := db.Exec(`
		INSERT INTO api_versions (id, version, status, changelog, created_at, updated_at)
		VALUES ($1, $2, $3, $4, $5, $6)
		ON CONFLICT (version) DO NOTHING
	`, versionID, "v1", "active",
		"Initial stable API version with full feature set including tasks, repositories, documents, and analysis",
		now, now)

	if err != nil {
		log.Printf("Warning: Failed to initialize default API version: %v", err)
	}
}

func analyzeBasicCoverage(codebase, testFiles []string, language string, coverageData map[string]interface{}) map[string]interface{} {
	coverage := map[string]interface{}{
		"overall":         0.0,
		"coveredLines":    0,
		"totalLines":      0,
		"uncoveredLines":  []int{},
		"missingTests":    []string{},
		"recommendations": []string{},
	}

	// Basic coverage estimation
	totalLines := 0
	for range codebase {
		// Estimate lines of code (simplified)
		totalLines += 50 // placeholder
	}

	coveredLines := int(float64(totalLines) * 0.65) // Assume 65% coverage
	coverage["overall"] = 65.0
	coverage["coveredLines"] = coveredLines
	coverage["totalLines"] = totalLines

	// Generate recommendations
	coverage["recommendations"] = []string{
		"Add unit tests for uncovered functions",
		"Consider integration tests for complex workflows",
		"Add test coverage for error conditions",
	}

	return coverage
}

func extractFunctionNameByLanguage(line, language string) string {
	// Simple function name extraction
	switch language {
	case "javascript", "typescript":
		if idx := strings.Index(line, "function "); idx >= 0 {
			afterFunc := line[idx+9:]
			if spaceIdx := strings.Index(afterFunc, "("); spaceIdx > 0 {
				return strings.TrimSpace(afterFunc[:spaceIdx])
			}
		}
	case "python":
		if idx := strings.Index(line, "def "); idx >= 0 {
			afterDef := line[idx+4:]
			if colonIdx := strings.Index(afterDef, "("); colonIdx > 0 {
				return strings.TrimSpace(afterDef[:colonIdx])
			}
		}
	case "go":
		if idx := strings.Index(line, "func "); idx >= 0 {
			afterFunc := line[idx+5:]
			if spaceIdx := strings.Index(afterFunc, "("); spaceIdx > 0 {
				return strings.TrimSpace(afterFunc[:spaceIdx])
			}
		}
	}
	return ""
}

// contains function is defined in ast_analyzer.go

// Update knowledge item status
func updateKnowledgeStatusHandler(w http.ResponseWriter, r *http.Request) {
	itemID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Status     string `json:"status"`      // approved, rejected
		ApprovedBy string `json:"approved_by"` // optional
	}
	if err = json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.Status != "approved" && req.Status != "rejected" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "status",
			Message: "Status must be 'approved' or 'rejected'",
			Code:    "invalid_enum",
		}, http.StatusBadRequest)
		return
	}

	// Verify knowledge item belongs to project
	var docID string
	err = db.QueryRow(`
		SELECT ki.document_id 
		FROM knowledge_items ki
		INNER JOIN documents d ON ki.document_id = d.id
		WHERE ki.id = $1 AND d.project_id = $2
	`, itemID, project.ID).Scan(&docID)

	if err == sql.ErrNoRows {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "knowledge_item",
			ID:       itemID,
			Message:  "Knowledge item not found",
		}, http.StatusNotFound)
		return
	}
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// Update status
	now := time.Now()
	var approvedBy sql.NullString
	var approvedAt sql.NullTime

	if req.Status == "approved" {
		approvedBy = sql.NullString{String: req.ApprovedBy, Valid: req.ApprovedBy != ""}
		approvedAt = sql.NullTime{Time: now, Valid: true}
	}

	_, err = db.Exec(`
		UPDATE knowledge_items 
		SET status = $1, approved_by = $2, approved_at = $3
		WHERE id = $4
	`, req.Status, approvedBy, approvedAt, itemID)

	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"id":     itemID,
		"status": req.Status,
	})
}

// List all knowledge items for project
func listProjectKnowledgeHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get status filter from query param
	statusFilter := r.URL.Query().Get("status") // pending, approved, rejected

	query := `
		SELECT ki.id, ki.document_id, ki.type, ki.title, ki.content, ki.confidence, 
		       ki.source_page, ki.status, ki.approved_by, ki.approved_at, ki.created_at,
		       d.original_name
		FROM knowledge_items ki
		INNER JOIN documents d ON ki.document_id = d.id
		WHERE d.project_id = $1
	`
	args := []interface{}{project.ID}

	if statusFilter != "" {
		query += " AND ki.status = $2"
		args = append(args, statusFilter)
	}

	query += " ORDER BY ki.created_at DESC LIMIT 500"

	rows, err := db.Query(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	items := []map[string]interface{}{}
	ctx := r.Context()
	for rows.Next() {
		var item KnowledgeItem
		var sourcePage sql.NullInt32
		var approvedBy sql.NullString
		var approvedAt sql.NullTime
		var docName string

		err := rows.Scan(&item.ID, &item.DocumentID, &item.Type, &item.Title, &item.Content,
			&item.Confidence, &sourcePage, &item.Status, &approvedBy, &approvedAt,
			&item.CreatedAt, &docName)
		if err != nil {
			LogWarn(ctx, "Failed to scan knowledge item row (skipping): %v. ProjectID: %s", err, project.ID)
			continue
		}

		result := map[string]interface{}{
			"id":          item.ID,
			"document_id": item.DocumentID,
			"document":    docName,
			"type":        item.Type,
			"title":       item.Title,
			"content":     item.Content,
			"confidence":  item.Confidence,
			"status":      item.Status,
			"created_at":  item.CreatedAt,
		}

		if sourcePage.Valid {
			result["source_page"] = sourcePage.Int32
		}
		if approvedBy.Valid {
			result["approved_by"] = approvedBy.String
		}
		if approvedAt.Valid {
			result["approved_at"] = approvedAt.Time
		}

		items = append(items, result)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"knowledge_items": items,
		"total":           len(items),
	})
}

// getBusinessContextHandler handles GET /api/v1/knowledge/business
// Returns business rules, entities, and journeys filtered by type
func getBusinessContextHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get type filter from query param (rule, entity, journey)
	typeFilter := r.URL.Query().Get("type")

	query := `
		SELECT ki.id, ki.document_id, ki.type, ki.title, ki.content, ki.confidence, 
		       ki.source_page, ki.status, ki.created_at, d.original_name
		FROM knowledge_items ki
		INNER JOIN documents d ON ki.document_id = d.id
		WHERE d.project_id = $1 AND ki.status = 'approved'
	`
	args := []interface{}{project.ID}

	// Filter by business-related types: rule, entity, journey
	if typeFilter != "" {
		query += " AND ki.type = $2"
		args = append(args, typeFilter)
	} else {
		// Default: return all business-related types
		query += " AND ki.type IN ('rule', 'entity', 'journey')"
	}

	query += " ORDER BY ki.created_at DESC LIMIT 200"

	rows, err := queryWithTimeout(ctx, query, args...)
	if err != nil {
		LogErrorWithContext(ctx, err, "Failed to query business context")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	items := []map[string]interface{}{}
	for rows.Next() {
		var item struct {
			ID           string
			DocumentID   string
			Type         string
			Title        string
			Content      string
			Confidence   float64
			SourcePage   sql.NullInt32
			Status       string
			CreatedAt    time.Time
			DocumentName string
		}

		err := rows.Scan(&item.ID, &item.DocumentID, &item.Type, &item.Title, &item.Content,
			&item.Confidence, &item.SourcePage, &item.Status, &item.CreatedAt, &item.DocumentName)
		if err != nil {
			LogWarn(ctx, "Failed to scan business context item: %v", err)
			continue
		}

		result := map[string]interface{}{
			"id":          item.ID,
			"document_id": item.DocumentID,
			"document":    item.DocumentName,
			"item_type":   item.Type,
			"type":        item.Type, // Also include as 'type' for compatibility
			"title":       item.Title,
			"content":     item.Content,
			"confidence":  item.Confidence,
			"status":      item.Status,
			"created_at":  item.CreatedAt.Format(time.RFC3339),
		}

		if item.SourcePage.Valid {
			result["source_page"] = item.SourcePage.Int32
		}

		items = append(items, result)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"items":   items,
		"total":   len(items),
	})
}

// getSecurityContextHandler handles GET /api/v1/security/context
// Returns security rules, compliance status, and security score
func getSecurityContextHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogError(ctx, "Failed to get project from context: %v", err)
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get all security rules
	rules := []map[string]interface{}{}
	for ruleID, rule := range security.SecurityRules {
		ruleData := map[string]interface{}{
			"rule_id":     ruleID,
			"name":        rule.Name,
			"type":        rule.Type,
			"severity":    rule.Severity,
			"status":      "active", // All rules are active by default
		}
		rules = append(rules, ruleData)
	}

	// Calculate compliance status (simplified - in production, would query recent analyses)
	compliance := map[string]interface{}{
		"total_rules":     len(security.SecurityRules),
		"rules_checked":   len(security.SecurityRules),
		"rules_compliant": len(security.SecurityRules), // Default to compliant
		"last_check":      time.Now().Format(time.RFC3339),
	}

	// Default security score (in production, would calculate from recent analyses)
	securityScore := 85.0
	securityGrade := "B"

	// Try to get recent security score from database if available
	// Note: This would require a security_analysis table which may not exist yet
	// For now, return default values

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":        true,
		"security_score": securityScore,
		"security_grade": securityGrade,
		"rules":          rules,
		"compliance":     compliance,
		"project_id":     project.ID,
	})
}

// validateCodeHandler handles POST /api/v1/validate/code
// Validates code using AST analysis
func validateCodeHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	var req struct {
		Code     string `json:"code"`
		FilePath string `json:"file_path"`
		Language string `json:"language"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.Code == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "code",
			Message: "code is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Infer language from file extension if not provided
	if req.Language == "" && req.FilePath != "" {
		ext := filepath.Ext(req.FilePath)
		langMap := map[string]string{
			".js": "javascript", ".ts": "typescript", ".jsx": "javascript",
			".py": "python", ".go": "go", ".java": "java",
			".cs": "csharp", ".php": "php", ".rb": "ruby",
		}
		if lang, ok := langMap[ext]; ok {
			req.Language = lang
		}
	}

	// Perform AST analysis
	violations := []map[string]interface{}{}

	if req.Language != "" {
		// Call AST analysis using existing analyzeAST function
		findings, stats, err := analyzeAST(req.Code, req.Language, []string{"duplicates", "unused", "unreachable"})
		if err != nil {
			LogErrorWithContext(ctx, err, "AST analysis failed")
			// Continue with empty violations rather than failing the request
		} else {
			// Convert AST findings to violations format
			for _, finding := range findings {
				violation := map[string]interface{}{
					"type":       finding.Type,
					"severity":   finding.Severity,
					"line":       finding.Line,
					"column":     finding.Column,
					"end_line":   finding.EndLine,
					"end_column": finding.EndColumn,
					"message":    finding.Message,
					"code":       finding.Code,
					"suggestion": finding.Suggestion,
				}
				violations = append(violations, violation)
			}
		}
		_ = stats // Stats available for future use (logging, metrics)
	} else {
		// Language not provided and couldn't be inferred
		violations = append(violations, map[string]interface{}{
			"type":     "language_required",
			"severity": "warning",
			"message":  "Language not specified and could not be inferred from file path",
		})
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":    true,
		"violations": violations,
		"valid":      len(violations) == 0,
	})
}

// validateBusinessHandler handles POST /api/v1/validate/business
// Validates code against business rules
func validateBusinessHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Feature string `json:"feature"`
		Code    string `json:"code"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.Feature == "" || req.Code == "" {
		var missingField string
		if req.Feature == "" {
			missingField = "feature"
		} else {
			missingField = "code"
		}
		WriteErrorResponse(w, &ValidationError{
			Field:   missingField,
			Message: fmt.Sprintf("%s is required", missingField),
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Get business rules for the project
	query := `
		SELECT ki.id, ki.type, ki.title, ki.content
		FROM knowledge_items ki
		INNER JOIN documents d ON ki.document_id = d.id
		WHERE d.project_id = $1 AND ki.type IN ('rule', 'entity', 'journey') AND ki.status = 'approved'
		ORDER BY ki.created_at DESC
		LIMIT 100
	`
	rows, err := queryWithTimeout(ctx, query, project.ID)
	if err != nil {
		LogErrorWithContext(ctx, err, "Failed to query business rules")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	violations := []map[string]interface{}{}
	businessRules := []map[string]interface{}{}

	for rows.Next() {
		var rule struct {
			ID      string
			Type    string
			Title   string
			Content string
		}
		if err := rows.Scan(&rule.ID, &rule.Type, &rule.Title, &rule.Content); err != nil {
			continue
		}
		businessRules = append(businessRules, map[string]interface{}{
			"id":      rule.ID,
			"type":    rule.Type,
			"title":   rule.Title,
			"content": rule.Content,
		})
	}

	// Simple validation: check if code mentions business entities/rules
	// In production, would use LLM or more sophisticated analysis
	codeLower := strings.ToLower(req.Code)
	for _, rule := range businessRules {
		if title, ok := rule["title"].(string); ok {
			titleLower := strings.ToLower(title)
			// Check if rule title is mentioned in code (simplified check)
			if !strings.Contains(codeLower, titleLower) && rule["type"] == "rule" {
				// This is a simplified check - in production would be more sophisticated
			}
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":       true,
		"violations":    violations,
		"valid":         len(violations) == 0,
		"rules_checked": len(businessRules),
	})
}

// applyFixHandler handles POST /api/v1/fixes/apply
// Applies fixes to code issues
func applyFixHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	var req struct {
		FilePath string `json:"file_path"`
		FixType  string `json:"fix_type"`
		Content  string `json:"content"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.FilePath == "" || req.FixType == "" || req.Content == "" {
		var missingField string
		if req.FilePath == "" {
			missingField = "file_path"
		} else if req.FixType == "" {
			missingField = "fix_type"
		} else {
			missingField = "content"
		}
		WriteErrorResponse(w, &ValidationError{
			Field:   missingField,
			Message: fmt.Sprintf("%s is required", missingField),
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Apply fixes based on fix type
	var fixedCode string
	var changes []map[string]interface{}
	var err error

	// Infer language from file path if not provided
	language := inferLanguageFromPath(req.FilePath)

	switch req.FixType {
	case "security":
		fixedCode, changes, err = ApplySecurityFixes(ctx, req.Content, language)
	case "style":
		fixedCode, changes, err = ApplyStyleFixes(ctx, req.Content, language)
	case "performance":
		fixedCode, changes, err = ApplyPerformanceFixes(ctx, req.Content, language)
	default:
		WriteErrorResponse(w, &ValidationError{
			Field:   "fix_type",
			Message: fmt.Sprintf("Unknown fix type: %s. Supported types: security, style, performance", req.FixType),
			Code:    "invalid_enum",
		}, http.StatusBadRequest)
		return
	}

	if err != nil {
		LogErrorWithContext(ctx, err, "Fix application failed")
		WriteErrorResponse(w, &ExternalServiceError{
			Service:    "fix_applier",
			Message:    fmt.Sprintf("Failed to apply fixes: %v", err),
			StatusCode: 0,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":    true,
		"fixed_code": fixedCode,
		"changes":    changes,
	})
}

// Sync knowledge items (bidirectional)
func syncKnowledgeHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Items []struct {
			ID     string `json:"id"`
			Status string `json:"status"`
		} `json:"items"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	updated := 0
	for _, item := range req.Items {
		// Verify item belongs to project
		var exists bool
		db.QueryRow(`
			SELECT EXISTS(
				SELECT 1 FROM knowledge_items ki
				INNER JOIN documents d ON ki.document_id = d.id
				WHERE ki.id = $1 AND d.project_id = $2
			)
		`, item.ID, project.ID).Scan(&exists)

		if !exists {
			continue
		}

		// Update status
		var approvedAt sql.NullTime
		if item.Status == "approved" {
			approvedAt = sql.NullTime{Time: time.Now(), Valid: true}
		}

		_, err := db.Exec(`
			UPDATE knowledge_items 
			SET status = $1, approved_at = $2
			WHERE id = $3
		`, item.Status, approvedAt, item.ID)

		if err == nil {
			updated++
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"updated": updated,
		"total":   len(req.Items),
	})
}

// Gap Analysis Handler (Phase 12)
func gapAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req GapAnalysisRequest

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Use project ID from context if not provided
	if req.ProjectID == "" {
		req.ProjectID = project.ID
	}

	// Default codebase path if not provided
	if req.CodebasePath == "" {
		req.CodebasePath = "."
	}

	// Default options
	if req.Options == nil {
		req.Options = make(map[string]interface{})
	}

	ctx, cancel := context.WithTimeout(r.Context(), getAnalysisTimeout())
	defer cancel()

	report, err := analyzeGaps(ctx, req.ProjectID, req.CodebasePath, req.Options)
	if err != nil {
		LogErrorWithContext(ctx, err, fmt.Sprintf("Gap analysis failed (project: %s, path: %s)", req.ProjectID, req.CodebasePath))
		http.Error(w, fmt.Sprintf("Gap analysis failed: %v", err), http.StatusInternalServerError)
		return
	}

	// Store report in database
	reportID, err := storeGapReport(ctx, report)
	if err != nil {
		// Log error but don't fail the request (report is still returned)
		LogWarn(ctx, "Failed to store gap report (project: %s): %v", req.ProjectID, err)
		// reportID will be empty string, which is acceptable
	}

	w.Header().Set("Content-Type", "application/json")
	response := GapAnalysisResponse{
		Success:  true,
		Report:   report,
		ReportID: reportID, // Set from storage
	}

	json.NewEncoder(w).Encode(response)
}

// =============================================================================
// PHASE 12: CHANGE REQUEST HANDLERS
// =============================================================================

// List change requests handler
func listChangeRequestsHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	statusFilter := r.URL.Query().Get("status")
	limit := 50 // Default
	offset := 0

	if l := r.URL.Query().Get("limit"); l != "" {
		fmt.Sscanf(l, "%d", &limit)
	}
	if o := r.URL.Query().Get("offset"); o != "" {
		fmt.Sscanf(o, "%d", &offset)
	}

	ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second)
	defer cancel()

	requests, total, err := listChangeRequests(ctx, project.ID, statusFilter, limit, offset)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to list change requests: %v", err), http.StatusInternalServerError)
		return
	}

	// Calculate pagination metadata
	hasNext := offset+limit < total
	hasPrevious := offset > 0

	response := ListChangeRequestsResponse{
		ChangeRequests: requests,
		Total:          total,
		Limit:          limit,
		Offset:         offset,
		HasNext:        hasNext,
		HasPrevious:    hasPrevious,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

// Get change request handler
func getChangeRequestHandler(w http.ResponseWriter, r *http.Request) {
	changeRequestID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second)
	defer cancel()

	cr, err := getChangeRequest(ctx, changeRequestID)
	if err != nil {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	// Verify change request belongs to project
	if cr.ProjectID != project.ID {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(cr)
}

// Approve change request handler
func approveChangeRequestHandler(w http.ResponseWriter, r *http.Request) {
	changeRequestID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req ApproveChangeRequestRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.ApprovedBy == "" {
		req.ApprovedBy = "system" // Default
	}

	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	// Verify change request belongs to project
	cr, err := getChangeRequest(ctx, changeRequestID)
	if err != nil {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}
	if cr.ProjectID != project.ID {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	err = approveChangeRequest(ctx, changeRequestID, req.ApprovedBy)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to approve: %v", err), http.StatusBadRequest)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Change request approved",
	})
}

// Reject change request handler
func rejectChangeRequestHandler(w http.ResponseWriter, r *http.Request) {
	changeRequestID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req RejectChangeRequestRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.RejectedBy == "" {
		req.RejectedBy = "system" // Default
	}
	if req.Reason == "" {
		req.Reason = "No reason provided"
	}

	ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second)
	defer cancel()

	// Verify change request belongs to project
	cr, err := getChangeRequest(ctx, changeRequestID)
	if err != nil {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}
	if cr.ProjectID != project.ID {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	err = rejectChangeRequest(ctx, changeRequestID, req.RejectedBy, req.Reason)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to reject: %v", err), http.StatusBadRequest)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Change request rejected",
	})
}

// Analyze impact handler
func analyzeImpactHandler(w http.ResponseWriter, r *http.Request) {
	changeRequestID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req ImpactAnalysisRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		// Use default if not provided
		req.CodebasePath = "."
	}

	if req.CodebasePath == "" {
		req.CodebasePath = "."
	}

	ctx, cancel := context.WithTimeout(r.Context(), getAnalysisTimeout())
	defer cancel()

	// Verify change request belongs to project
	cr, err := getChangeRequest(ctx, changeRequestID)
	if err != nil {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}
	if cr.ProjectID != project.ID {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	impact, err := analyzeImpact(ctx, changeRequestID, project.ID, req.CodebasePath)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to analyze impact: %v", err), http.StatusInternalServerError)
		return
	}

	// Store impact analysis in database
	if err := storeImpactAnalysis(ctx, changeRequestID, impact); err != nil {
		// Log error but don't fail the request (impact is still returned)
		LogWarn(ctx, "Failed to store impact analysis: %v", err)
	}

	response := ImpactAnalysisResponse{
		Success: true,
		Impact:  impact,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

// Start implementation handler
func startImplementationHandler(w http.ResponseWriter, r *http.Request) {
	changeRequestID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req StartImplementationRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	ctx, cancel := context.WithTimeout(r.Context(), getQueryTimeout())
	defer cancel()

	// Verify change request belongs to project
	cr, err := getChangeRequest(ctx, changeRequestID)
	if err != nil {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}
	if cr.ProjectID != project.ID {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	err = updateImplementationStatus(ctx, changeRequestID, ImplStatusInProgress, req.Notes)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to start implementation: %v", err), http.StatusBadRequest)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Implementation started",
	})
}

// Complete implementation handler
func completeImplementationHandler(w http.ResponseWriter, r *http.Request) {
	changeRequestID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req CompleteImplementationRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	ctx, cancel := context.WithTimeout(r.Context(), getQueryTimeout())
	defer cancel()

	// Verify change request belongs to project
	cr, err := getChangeRequest(ctx, changeRequestID)
	if err != nil {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}
	if cr.ProjectID != project.ID {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	err = updateImplementationStatus(ctx, changeRequestID, ImplStatusCompleted, req.Notes)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to complete implementation: %v", err), http.StatusBadRequest)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Implementation completed",
	})
}

// Update implementation handler
func updateImplementationHandler(w http.ResponseWriter, r *http.Request) {
	changeRequestID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req UpdateImplementationRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.Status == "" {
		http.Error(w, "Status is required", http.StatusBadRequest)
		return
	}

	ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second)
	defer cancel()

	// Verify change request belongs to project
	cr, err := getChangeRequest(ctx, changeRequestID)
	if err != nil {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}
	if cr.ProjectID != project.ID {
		http.Error(w, "Change request not found", http.StatusNotFound)
		return
	}

	err = updateImplementationStatus(ctx, changeRequestID, req.Status, req.Notes)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to update implementation: %v", err), http.StatusBadRequest)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Implementation status updated",
	})
}

// Get change requests dashboard handler
func getChangeRequestsDashboardHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	ctx, cancel := context.WithTimeout(r.Context(), 10*time.Second)
	defer cancel()

	// Get counts by status
	statusQuery := `
		SELECT status, COUNT(*) 
		FROM change_requests 
		WHERE project_id = $1 
		GROUP BY status
	`
	rows, err := db.QueryContext(ctx, statusQuery, project.ID)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	byStatus := make(map[string]int)
	for rows.Next() {
		var status string
		var count int
		if err := rows.Scan(&status, &count); err == nil {
			byStatus[status] = count
		}
	}

	// Get counts by implementation status
	implQuery := `
		SELECT implementation_status, COUNT(*) 
		FROM change_requests 
		WHERE project_id = $1 
		GROUP BY implementation_status
	`
	rows2, err := db.QueryContext(ctx, implQuery, project.ID)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows2.Close()

	byImplStatus := make(map[string]int)
	for rows2.Next() {
		var status string
		var count int
		if err := rows2.Scan(&status, &count); err == nil {
			byImplStatus[status] = count
		}
	}

	// Get total
	var total int
	db.QueryRowContext(ctx, "SELECT COUNT(*) FROM change_requests WHERE project_id = $1", project.ID).Scan(&total)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"total":                    total,
		"by_status":                byStatus,
		"by_implementation_status": byImplStatus,
	})
}

// =============================================================================
// TELEMETRY
// =============================================================================

type TelemetryEvent struct {
	Event     string                 `json:"event"`
	EventType string                 `json:"event_type"` // Keep for backward compat
	AgentID   string                 `json:"agentId"`
	OrgID     string                 `json:"orgId"`
	TeamID    string                 `json:"teamId,omitempty"`
	Timestamp string                 `json:"timestamp"`
	Payload   map[string]interface{} `json:"payload"`
	Metrics   map[string]interface{} `json:"metrics"`
}

// Telemetry ingestion handler
func telemetryIngestionHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var events []TelemetryEvent
	if err := json.NewDecoder(r.Body).Decode(&events); err != nil {
		log.Printf("Error decoding telemetry events for project %s: %v", project.ID, err)
		http.Error(w, "Invalid request body: "+err.Error(), http.StatusBadRequest)
		return
	}

	if len(events) == 0 {
		log.Printf("Telemetry ingestion request with no events for project %s", project.ID)
		http.Error(w, "No events provided", http.StatusBadRequest)
		return
	}

	// Validate and sanitize events
	inserted := 0
	for _, event := range events {
		// Validate event type
		validTypes := map[string]bool{
			"audit_complete":   true,
			"fix_applied":      true,
			"pattern_learned":  true,
			"doc_ingested":     true,
			"knowledge_synced": true,
		}

		if !validTypes[event.EventType] {
			log.Printf("Invalid event type: %s", event.EventType)
			continue
		}

		// Sanitize payload (ensure no code content)
		sanitizedPayload := sanitizeTelemetryPayload(event.Payload)

		// Insert into database
		payloadJSON, err := json.Marshal(sanitizedPayload)
		if err != nil {
			log.Printf("Failed to marshal payload: %v", err)
			continue
		}

		// Extract additional fields from event
		agentID := event.AgentID
		orgID := event.OrgID
		teamID := event.TeamID
		eventTimestamp := event.Timestamp

		// Parse timestamp if provided
		var timestamp sql.NullTime
		if eventTimestamp != "" {
			if t, err := time.Parse(time.RFC3339, eventTimestamp); err == nil {
				timestamp = sql.NullTime{Time: t, Valid: true}
			}
		}

		// Convert orgID and teamID to UUID if provided
		var orgIDUUID sql.NullString
		var teamIDUUID sql.NullString
		if orgID != "" {
			if _, err := uuid.Parse(orgID); err == nil {
				orgIDUUID = sql.NullString{String: orgID, Valid: true}
			}
		}
		if teamID != "" {
			if _, err := uuid.Parse(teamID); err == nil {
				teamIDUUID = sql.NullString{String: teamID, Valid: true}
			}
		}

		_, err = db.Exec(`
			INSERT INTO telemetry_events (project_id, event_type, payload, agent_id, org_id, team_id, timestamp)
			VALUES ($1, $2, $3, $4, $5, $6, $7)
		`, project.ID, event.EventType, string(payloadJSON),
			sql.NullString{String: agentID, Valid: agentID != ""},
			orgIDUUID, teamIDUUID, timestamp)

		if err != nil {
			log.Printf("Failed to insert telemetry event: %v", err)
			continue
		}

		inserted++
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"received": len(events),
		"inserted": inserted,
	})
}

// Sanitize telemetry payload to ensure no sensitive data
func sanitizeTelemetryPayload(payload map[string]interface{}) map[string]interface{} {
	sanitized := make(map[string]interface{})

	// Allowed fields (no code content)
	allowedFields := map[string]bool{
		"finding_count":      true,
		"critical_count":     true,
		"warning_count":      true,
		"info_count":         true,
		"compliance_percent": true,
		"fix_count":          true,
		"fix_type":           true,
		"pattern_confidence": true,
		"pattern_type":       true,
		"doc_count":          true,
		"knowledge_count":    true,
		"file_count":         true,
		"timestamp":          true,
		"duration_ms":        true,
	}

	for key, value := range payload {
		if allowedFields[key] {
			sanitized[key] = value
		}
	}

	return sanitized
}

// Get metrics handler
func getMetricsHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get query parameters
	startDate := r.URL.Query().Get("start_date")
	endDate := r.URL.Query().Get("end_date")
	eventType := r.URL.Query().Get("event_type")

	// Build query
	query := `
		SELECT event_type, payload, created_at
		FROM telemetry_events
		WHERE project_id = $1
	`
	args := []interface{}{project.ID}
	argIndex := 2

	if startDate != "" {
		query += fmt.Sprintf(" AND created_at >= $%d", argIndex)
		args = append(args, startDate)
		argIndex++
	}

	if endDate != "" {
		query += fmt.Sprintf(" AND created_at <= $%d", argIndex)
		args = append(args, endDate)
		argIndex++
	}

	if eventType != "" {
		query += fmt.Sprintf(" AND event_type = $%d", argIndex)
		args = append(args, eventType)
		argIndex++
	}

	query += " ORDER BY created_at DESC LIMIT 1000"

	rows, err := db.Query(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	events := []map[string]interface{}{}
	for rows.Next() {
		var eventType string
		var payloadJSON string
		var createdAt time.Time

		if err := rows.Scan(&eventType, &payloadJSON, &createdAt); err != nil {
			continue
		}

		var payload map[string]interface{}
		if err := json.Unmarshal([]byte(payloadJSON), &payload); err != nil {
			payload = make(map[string]interface{})
		}

		events = append(events, map[string]interface{}{
			"event_type": eventType,
			"payload":    payload,
			"created_at": createdAt,
		})
	}

	// Calculate aggregated metrics
	metrics := calculateMetrics(events)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"events":  events,
		"metrics": metrics,
		"total":   len(events),
	})
}

// prometheusMetricsHandler exposes metrics in Prometheus format (Phase G: Logging and Monitoring)
func prometheusMetricsHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "text/plain; version=0.0.4")

	metricsMutex.RLock()
	defer metricsMutex.RUnlock()

	// HTTP request counter
	for endpoint, count := range httpRequestCounter {
		fmt.Fprintf(w, "sentinel_http_requests_total{endpoint=\"%s\"} %d\n", endpoint, count)
	}

	// HTTP error counter
	for endpoint, count := range httpErrorCounter {
		fmt.Fprintf(w, "sentinel_http_errors_total{endpoint=\"%s\"} %d\n", endpoint, count)
	}

	// HTTP request duration (average)
	for endpoint := range httpDurationSum {
		if requestCount := httpRequestCount[endpoint]; requestCount > 0 {
			avgDuration := float64(httpDurationSum[endpoint]) / float64(requestCount)
			fmt.Fprintf(w, "sentinel_http_request_duration_ms{endpoint=\"%s\"} %.2f\n", endpoint, avgDuration)
		}
	}

	// Database connection pool stats
	stats := db.Stats()
	fmt.Fprintf(w, "sentinel_db_open_connections %d\n", stats.OpenConnections)
	fmt.Fprintf(w, "sentinel_db_in_use %d\n", stats.InUse)
	fmt.Fprintf(w, "sentinel_db_idle %d\n", stats.Idle)
	fmt.Fprintf(w, "sentinel_db_wait_count %d\n", stats.WaitCount)
	fmt.Fprintf(w, "sentinel_db_wait_duration_ms %d\n", stats.WaitDuration.Milliseconds())

	// Uptime
	uptime := time.Since(startTime).Seconds()
	fmt.Fprintf(w, "sentinel_uptime_seconds %.2f\n", uptime)
}

// recordHTTPMetric records HTTP request metrics (Phase G: Logging and Monitoring)
func recordHTTPMetric(endpoint string, statusCode int, duration time.Duration) {
	metricsMutex.Lock()
	defer metricsMutex.Unlock()

	httpRequestCounter[endpoint]++
	if statusCode >= 400 {
		httpErrorCounter[endpoint]++
	}
	httpDurationSum[endpoint] += duration.Milliseconds()
	httpRequestCount[endpoint]++
}

// Calculate aggregated metrics from events
func calculateMetrics(events []map[string]interface{}) map[string]interface{} {
	auditCount := 0
	fixCount := 0
	patternCount := 0
	docCount := 0
	totalFindings := 0
	totalCritical := 0
	totalWarnings := 0
	complianceSum := 0.0
	complianceCount := 0

	for _, event := range events {
		eventType, _ := event["event_type"].(string)
		payload, _ := event["payload"].(map[string]interface{})

		switch eventType {
		case "audit_complete":
			auditCount++
			if compliance, ok := payload["compliance_percent"].(float64); ok {
				complianceSum += compliance
				complianceCount++
			}
			if count, ok := payload["finding_count"].(float64); ok {
				totalFindings += int(count)
			}
			if count, ok := payload["critical_count"].(float64); ok {
				totalCritical += int(count)
			}
			if count, ok := payload["warning_count"].(float64); ok {
				totalWarnings += int(count)
			}

		case "fix_applied":
			fixCount++

		case "pattern_learned":
			patternCount++

		case "doc_ingested":
			docCount++
		}
	}

	avgCompliance := 0.0
	if complianceCount > 0 {
		avgCompliance = complianceSum / float64(complianceCount)
	}

	return map[string]interface{}{
		"total_events":   len(events),
		"audit_count":    auditCount,
		"fix_count":      fixCount,
		"pattern_count":  patternCount,
		"doc_count":      docCount,
		"avg_compliance": avgCompliance,
		"total_findings": totalFindings,
		"total_critical": totalCritical,
		"total_warnings": totalWarnings,
	}
}

// Get recent telemetry events handler
func getRecentTelemetryHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get query parameters
	limitStr := r.URL.Query().Get("limit")
	offsetStr := r.URL.Query().Get("offset")
	eventType := r.URL.Query().Get("event_type")

	limit := 100
	if limitStr != "" {
		if l, err := strconv.Atoi(limitStr); err == nil && l > 0 && l <= 1000 {
			limit = l
		}
	}

	offset := 0
	if offsetStr != "" {
		if o, err := strconv.Atoi(offsetStr); err == nil && o >= 0 {
			offset = o
		}
	}

	// Build query
	query := `
		SELECT event_type, payload, created_at
		FROM telemetry_events
		WHERE project_id = $1
	`
	args := []interface{}{project.ID}
	argIndex := 2

	if eventType != "" {
		query += fmt.Sprintf(" AND event_type = $%d", argIndex)
		args = append(args, eventType)
		argIndex++
	}

	query += fmt.Sprintf(" ORDER BY created_at DESC LIMIT $%d OFFSET $%d", argIndex, argIndex+1)
	args = append(args, limit, offset)

	rows, err := db.Query(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	events := []map[string]interface{}{}
	for rows.Next() {
		var eventType string
		var payloadJSON string
		var createdAt time.Time

		if err := rows.Scan(&eventType, &payloadJSON, &createdAt); err != nil {
			continue
		}

		var payload map[string]interface{}
		if err := json.Unmarshal([]byte(payloadJSON), &payload); err != nil {
			payload = make(map[string]interface{})
		}

		events = append(events, map[string]interface{}{
			"event_type": eventType,
			"payload":    payload,
			"created_at": createdAt,
		})
	}

	// Get total count for pagination
	var total int
	countQuery := `
		SELECT COUNT(*) FROM telemetry_events WHERE project_id = $1
	`
	countArgs := []interface{}{project.ID}
	if eventType != "" {
		countQuery += " AND event_type = $2"
		countArgs = append(countArgs, eventType)
	}
	db.QueryRow(countQuery, countArgs...).Scan(&total)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"events": events,
		"total":  total,
		"limit":  limit,
		"offset": offset,
	})
}

// Get metrics trends handler
func getMetricsTrendsHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get query parameters
	startDate := r.URL.Query().Get("start_date")
	endDate := r.URL.Query().Get("end_date")
	period := r.URL.Query().Get("period") // daily, weekly, monthly

	if period == "" {
		period = "daily"
	}

	// Default to last 30 days if not specified
	if startDate == "" {
		startDate = time.Now().AddDate(0, 0, -30).Format("2006-01-02")
	}
	if endDate == "" {
		endDate = time.Now().Format("2006-01-02")
	}

	// Build query based on period
	var dateFormat string
	switch period {
	case "weekly":
		dateFormat = "DATE_TRUNC('week', created_at)"
	case "monthly":
		dateFormat = "DATE_TRUNC('month', created_at)"
	default:
		dateFormat = "DATE(created_at)"
	}

	query := fmt.Sprintf(`
		SELECT 
			%s as period,
			event_type,
			COUNT(*) as event_count,
			AVG((payload->>'finding_count')::float) as avg_findings,
			AVG((payload->>'compliance_percent')::float) as avg_compliance
		FROM telemetry_events
		WHERE project_id = $1
			AND created_at >= $2
			AND created_at <= $3
		GROUP BY period, event_type
		ORDER BY period DESC, event_type
	`, dateFormat)

	rows, err := db.Query(query, project.ID, startDate, endDate)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	trends := []map[string]interface{}{}
	for rows.Next() {
		var period time.Time
		var eventType string
		var eventCount int
		var avgFindings sql.NullFloat64
		var avgCompliance sql.NullFloat64

		if err := rows.Scan(&period, &eventType, &eventCount, &avgFindings, &avgCompliance); err != nil {
			continue
		}

		trend := map[string]interface{}{
			"period":     period.Format("2006-01-02"),
			"event_type": eventType,
			"count":      eventCount,
		}

		if avgFindings.Valid {
			trend["avg_findings"] = avgFindings.Float64
		}
		if avgCompliance.Valid {
			trend["avg_compliance"] = avgCompliance.Float64
		}

		trends = append(trends, trend)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"trends":     trends,
		"period":     period,
		"start_date": startDate,
		"end_date":   endDate,
	})
}

// Get team metrics handler
func getTeamMetricsHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}
	teamID := chi.URLParam(r, "teamId")

	if teamID == "" {
		http.Error(w, "Team ID required", http.StatusBadRequest)
		return
	}

	// Get query parameters
	startDate := r.URL.Query().Get("start_date")
	endDate := r.URL.Query().Get("end_date")

	// Build query - note: team_id column needs to be added in Phase C
	query := `
		SELECT event_type, payload, created_at
		FROM telemetry_events
		WHERE project_id = $1
	`
	args := []interface{}{project.ID}
	argIndex := 2

	// TODO: Add team_id filter once column is added
	// if teamID != "" {
	// 	query += fmt.Sprintf(" AND team_id = $%d", argIndex)
	// 	args = append(args, teamID)
	// 	argIndex++
	// }

	if startDate != "" {
		query += fmt.Sprintf(" AND created_at >= $%d", argIndex)
		args = append(args, startDate)
		argIndex++
	}

	if endDate != "" {
		query += fmt.Sprintf(" AND created_at <= $%d", argIndex)
		args = append(args, endDate)
		argIndex++
	}

	query += " ORDER BY created_at DESC LIMIT 1000"

	rows, err := db.Query(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	events := []map[string]interface{}{}
	for rows.Next() {
		var eventType string
		var payloadJSON string
		var createdAt time.Time

		if err := rows.Scan(&eventType, &payloadJSON, &createdAt); err != nil {
			continue
		}

		var payload map[string]interface{}
		if err := json.Unmarshal([]byte(payloadJSON), &payload); err != nil {
			payload = make(map[string]interface{})
		}

		events = append(events, map[string]interface{}{
			"event_type": eventType,
			"payload":    payload,
			"created_at": createdAt,
		})
	}

	// Calculate team-specific metrics
	metrics := calculateMetrics(events)
	metrics["team_id"] = teamID

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"team_id": teamID,
		"events":  events,
		"metrics": metrics,
		"total":   len(events),
	})
}

// =============================================================================
// MIDDLEWARE
// =============================================================================

func apiKeyAuthMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		authHeader := r.Header.Get("Authorization")
		if authHeader == "" {
			http.Error(w, "Missing Authorization header", http.StatusUnauthorized)
			return
		}

		// Extract API key from "Bearer <key>" format
		parts := strings.Split(authHeader, " ")
		if len(parts) != 2 || parts[0] != "Bearer" {
			http.Error(w, "Invalid Authorization header format", http.StatusUnauthorized)
			return
		}
		apiKey := parts[1]

		// Enhanced API key validation
		if err := ValidateAPIKey(apiKey); err != nil {
			WriteErrorResponse(w, &ValidationError{
				Field:   "authorization",
				Message: err.Error(),
				Code:    "invalid_api_key",
			}, http.StatusUnauthorized)
			return
		}

		// Check per-API-key rate limit (before database lookup for efficiency)
		endpoint := r.URL.Path
		if !checkAPIKeyRateLimit(apiKey, endpoint) {
			w.Header().Set("Content-Type", "application/json")
			w.Header().Set("Retry-After", "1")
			w.WriteHeader(http.StatusTooManyRequests)
			w.Write([]byte(`{"error": "Rate limit exceeded for this API key. Please try again later."}`))
			return
		}

		// Look up project by API key
		var project Project
		err := db.QueryRow(`
			SELECT id, org_id, name, api_key, created_at
			FROM projects WHERE api_key = $1
		`, apiKey).Scan(&project.ID, &project.OrgID, &project.Name, &project.APIKey, &project.CreatedAt)

		if err == sql.ErrNoRows {
			WriteErrorResponse(w, &ValidationError{
				Field:   "authorization",
				Message: "Invalid API key",
				Code:    "unauthorized",
			}, http.StatusUnauthorized)
			return
		}
		if err != nil {
			http.Error(w, "Authentication error", http.StatusInternalServerError)
			return
		}

		// Add project to context
		ctx := context.WithValue(r.Context(), projectKey, &project)
		next.ServeHTTP(w, r.WithContext(ctx))
	})
}

// adminAuthMiddleware validates admin API key for admin endpoints
func adminAuthMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		config := GetConfig()

		// Check if admin key is configured
		if config.AdminAPIKey == "" {
			LogError(r.Context(), "Admin API key not configured")
			WriteErrorResponse(w, &InternalError{
				Message: "Admin authentication not configured",
				Code:    "configuration_error",
			}, http.StatusInternalServerError)
			return
		}

		// Try X-Admin-API-Key header first
		adminKey := r.Header.Get("X-Admin-API-Key")

		// Fallback to Authorization header
		if adminKey == "" {
			authHeader := r.Header.Get("Authorization")
			if authHeader != "" {
				parts := strings.Split(authHeader, " ")
				if len(parts) == 2 && parts[0] == "Bearer" {
					adminKey = parts[1]
				}
			}
		}

		// Validate admin key
		if adminKey == "" {
			WriteErrorResponse(w, &ValidationError{
				Field:   "authorization",
				Message: "Missing admin API key. Provide X-Admin-API-Key header or Authorization: Bearer <admin-key>",
				Code:    "unauthorized",
			}, http.StatusUnauthorized)
			return
		}

		// Use constant-time comparison to prevent timing attacks
		if !constantTimeEqual(adminKey, config.AdminAPIKey) {
			WriteErrorResponse(w, &ValidationError{
				Field:   "authorization",
				Message: "Invalid admin API key",
				Code:    "unauthorized",
			}, http.StatusUnauthorized)
			return
		}

		// Admin authenticated, proceed
		next.ServeHTTP(w, r)
	})
}

// constantTimeEqual performs constant-time string comparison to prevent timing attacks
func constantTimeEqual(a, b string) bool {
	if len(a) != len(b) {
		return false
	}
	result := 0
	for i := 0; i < len(a); i++ {
		result |= int(a[i]) ^ int(b[i])
	}
	return result == 0
}

// sanitizeString sanitizes a string by trimming, limiting length, and removing control characters
func sanitizeString(s string, maxLen int) string {
	s = strings.TrimSpace(s)
	if len(s) > maxLen {
		s = s[:maxLen]
	}
	// Remove control characters except newline, carriage return, and tab
	s = strings.Map(func(r rune) rune {
		if r < 32 && r != '\n' && r != '\r' && r != '\t' {
			return -1
		}
		return r
	}, s)
	return s
}

// validateVersionFormat validates that version follows semver format
func validateVersionFormat(version string) error {
	versionPattern := regexp.MustCompile(`^v?\d+\.\d+\.\d+(-[a-zA-Z0-9]+)?$`)
	if !versionPattern.MatchString(version) {
		return fmt.Errorf("version must be in semver format (e.g., 1.2.3 or v1.2.3)")
	}
	return nil
}

// validatePlatform validates that platform is in the allowed list
func validatePlatform(platform string) error {
	allowedPlatforms := []string{"linux-amd64", "linux-arm64", "darwin-amd64", "darwin-arm64", "windows-amd64"}
	for _, p := range allowedPlatforms {
		if platform == p {
			return nil
		}
	}
	return fmt.Errorf("platform must be one of: %v", allowedPlatforms)
}

// =============================================================================
// HELPERS
// =============================================================================

func detectMimeType(filename string) string {
	ext := strings.ToLower(filepath.Ext(filename))
	switch ext {
	case ".pdf":
		return "application/pdf"
	case ".docx":
		return "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
	case ".doc":
		return "application/msword"
	case ".xlsx":
		return "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
	case ".xls":
		return "application/vnd.ms-excel"
	case ".txt":
		return "text/plain"
	case ".md", ".markdown":
		return "text/markdown"
	case ".eml":
		return "message/rfc822"
	case ".png":
		return "image/png"
	case ".jpg", ".jpeg":
		return "image/jpeg"
	default:
		return "application/octet-stream"
	}
}

func estimateProcessingTime(size int64) int {
	// Rough estimate: 1 second per 100KB + 10 seconds base
	return 10 + int(size/(100*1024))
}

func stageStatus(docStatus string, progress int, threshold int) string {
	if docStatus == "failed" {
		return "failed"
	}
	if docStatus == "completed" {
		return "completed"
	}
	if progress >= threshold {
		return "completed"
	}
	if progress > threshold-50 {
		return "processing"
	}
	return "pending"
}

func generateAPIKey() string {
	bytes := make([]byte, 32)
	rand.Read(bytes)
	return "sk_live_" + hex.EncodeToString(bytes)
}

// =============================================================================
// ADMIN HANDLERS (for setup)
// =============================================================================

func createOrganizationHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		Name string `json:"name"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	orgID := uuid.New().String()
	_, err := db.Exec("INSERT INTO organizations (id, name) VALUES ($1, $2)", orgID, req.Name)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]string{
		"id":   orgID,
		"name": req.Name,
	})
}

func createProjectHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		OrgID string `json:"org_id"`
		Name  string `json:"name"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	projectID := uuid.New().String()
	apiKey := generateAPIKey()

	_, err := db.Exec("INSERT INTO projects (id, org_id, name, api_key) VALUES ($1, $2, $3, $4)",
		projectID, req.OrgID, req.Name, apiKey)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]string{
		"id":      projectID,
		"name":    req.Name,
		"api_key": apiKey,
	})
}

// =============================================================================
// Phase 16: Organization Features - Team Management
// =============================================================================

// createTeamHandler creates a new team
func createTeamHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		OrgID       string `json:"org_id"`
		Name        string `json:"name"`
		Description string `json:"description,omitempty"`
		LeadUserID  string `json:"lead_user_id,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.OrgID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "org_id",
			Message: "Organization ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	if req.Name == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "name",
			Message: "Team name is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	if len(req.Name) > 100 {
		WriteErrorResponse(w, &ValidationError{
			Field:   "name",
			Message: "Team name must be 100 characters or less",
			Code:    "field_too_long",
		}, http.StatusBadRequest)
		return
	}

	teamID := uuid.New().String()
	now := time.Now()

	_, err := db.Exec(`
		INSERT INTO teams (id, org_id, name, description, lead_user_id, created_at, updated_at)
		VALUES ($1, $2, $3, $4, $5, $6, $7)`,
		teamID, req.OrgID, req.Name, req.Description, req.LeadUserID, now, now)

	if err != nil {
		LogError(r.Context(), "Failed to create team: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "create_team",
			Message:   "Failed to create team",
		}, http.StatusInternalServerError)
		return
	}

	team := Team{
		ID:          teamID,
		OrgID:       req.OrgID,
		Name:        req.Name,
		Description: req.Description,
		LeadUserID:  req.LeadUserID,
		CreatedAt:   now,
		UpdatedAt:   now,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(team)
}

// getTeamsHandler lists teams for an organization
func getTeamsHandler(w http.ResponseWriter, r *http.Request) {
	orgID := r.URL.Query().Get("org_id")
	if orgID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "org_id",
			Message: "Organization ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	rows, err := db.Query(`
		SELECT id, org_id, name, description, lead_user_id, settings, created_at, updated_at
		FROM teams
		WHERE org_id = $1
		ORDER BY created_at DESC`, orgID)

	if err != nil {
		LogError(r.Context(), "Failed to query teams: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "list_teams",
			Message:   "Failed to retrieve teams",
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var teams []Team
	for rows.Next() {
		var team Team
		var settingsBytes []byte
		err := rows.Scan(&team.ID, &team.OrgID, &team.Name, &team.Description,
			&team.LeadUserID, &settingsBytes, &team.CreatedAt, &team.UpdatedAt)
		if err != nil {
			LogError(r.Context(), "Failed to scan team: %v", err)
			continue
		}

		if len(settingsBytes) > 0 {
			json.Unmarshal(settingsBytes, &team.Settings)
		}

		teams = append(teams, team)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"teams": teams,
		"count": len(teams),
	})
}

// addTeamMemberHandler adds a user to a team
func addTeamMemberHandler(w http.ResponseWriter, r *http.Request) {
	teamID := chi.URLParam(r, "teamId")
	if teamID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "teamId",
			Message: "Team ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		UserID string `json:"user_id"`
		Role   string `json:"role,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.UserID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "user_id",
			Message: "User ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	// Default role if not specified
	if req.Role == "" {
		req.Role = "member"
	}

	// Validate role
	if req.Role != "lead" && req.Role != "senior" && req.Role != "member" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "role",
			Message: "Role must be one of: lead, senior, member",
			Code:    "invalid_value",
		}, http.StatusBadRequest)
		return
	}

	memberID := uuid.New().String()
	now := time.Now()

	_, err := db.Exec(`
		INSERT INTO team_members (id, team_id, user_id, role, added_at)
		VALUES ($1, $2, $3, $4, $5)`,
		memberID, teamID, req.UserID, req.Role, now)

	if err != nil {
		LogError(r.Context(), "Failed to add team member: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "add_team_member",
			Message:   "Failed to add team member",
		}, http.StatusInternalServerError)
		return
	}

	member := TeamMember{
		ID:      memberID,
		TeamID:  teamID,
		UserID:  req.UserID,
		Role:    req.Role,
		AddedAt: now,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(member)
}

// getTeamMembersHandler lists members of a team
func getTeamMembersHandler(w http.ResponseWriter, r *http.Request) {
	teamID := chi.URLParam(r, "teamId")
	if teamID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "teamId",
			Message: "Team ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	rows, err := db.Query(`
		SELECT id, team_id, user_id, role, added_by, added_at
		FROM team_members
		WHERE team_id = $1
		ORDER BY added_at DESC`, teamID)

	if err != nil {
		LogError(r.Context(), "Failed to query team members: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "list_team_members",
			Message:   "Failed to retrieve team members",
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var members []TeamMember
	for rows.Next() {
		var member TeamMember
		err := rows.Scan(&member.ID, &member.TeamID, &member.UserID,
			&member.Role, &member.AddedBy, &member.AddedAt)
		if err != nil {
			LogError(r.Context(), "Failed to scan team member: %v", err)
			continue
		}
		members = append(members, member)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"members": members,
		"count":   len(members),
	})
}

// registerAgentHandler registers a new agent
func registerAgentHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		OrgID        string   `json:"org_id,omitempty"`
		TeamID       string   `json:"team_id,omitempty"`
		AgentID      string   `json:"agent_id"`
		Name         string   `json:"name"`
		Version      string   `json:"version,omitempty"`
		Platform     string   `json:"platform,omitempty"`
		Capabilities []string `json:"capabilities,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.AgentID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "agent_id",
			Message: "Agent ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	if req.Name == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "name",
			Message: "Agent name is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	agentID := uuid.New().String()
	now := time.Now()

	capabilitiesBytes, _ := json.Marshal(req.Capabilities)
	settingsBytes, _ := json.Marshal(map[string]interface{}{})

	_, err := db.Exec(`
		INSERT INTO registered_agents (id, org_id, team_id, agent_id, name, version, platform,
			last_seen, status, capabilities, settings, created_at)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)`,
		agentID, req.OrgID, req.TeamID, req.AgentID, req.Name, req.Version,
		req.Platform, now, "active", capabilitiesBytes, settingsBytes, now)

	if err != nil {
		LogError(r.Context(), "Failed to register agent: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "register_agent",
			Message:   "Failed to register agent",
		}, http.StatusInternalServerError)
		return
	}

	// Convert []string to JSONSlice ([]interface{})
	capabilities := make(JSONSlice, len(req.Capabilities))
	for i, cap := range req.Capabilities {
		capabilities[i] = cap
	}

	agent := RegisteredAgent{
		ID:           agentID,
		OrgID:        req.OrgID,
		TeamID:       req.TeamID,
		AgentID:      req.AgentID,
		Name:         req.Name,
		Version:      req.Version,
		Platform:     req.Platform,
		LastSeen:     now,
		Status:       "active",
		Capabilities: capabilities,
		CreatedAt:    now,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(agent)
}

// getRegisteredAgentsHandler lists registered agents
func getRegisteredAgentsHandler(w http.ResponseWriter, r *http.Request) {
	orgID := r.URL.Query().Get("org_id")
	teamID := r.URL.Query().Get("team_id")

	var query string
	var args []interface{}

	if teamID != "" {
		query = `SELECT id, org_id, team_id, agent_id, name, version, platform, last_seen, status, capabilities, settings, created_at
			FROM registered_agents WHERE team_id = $1 ORDER BY last_seen DESC`
		args = []interface{}{teamID}
	} else if orgID != "" {
		query = `SELECT id, org_id, team_id, agent_id, name, version, platform, last_seen, status, capabilities, settings, created_at
			FROM registered_agents WHERE org_id = $1 ORDER BY last_seen DESC`
		args = []interface{}{orgID}
	} else {
		WriteErrorResponse(w, &ValidationError{
			Field:   "query",
			Message: "Either org_id or team_id query parameter is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	rows, err := db.Query(query, args...)
	if err != nil {
		LogError(r.Context(), "Failed to query registered agents: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "list_agents",
			Message:   "Failed to retrieve agents",
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var agents []RegisteredAgent
	for rows.Next() {
		var agent RegisteredAgent
		var capabilitiesBytes, settingsBytes []byte
		err := rows.Scan(&agent.ID, &agent.OrgID, &agent.TeamID, &agent.AgentID, &agent.Name,
			&agent.Version, &agent.Platform, &agent.LastSeen, &agent.Status,
			&capabilitiesBytes, &settingsBytes, &agent.CreatedAt)
		if err != nil {
			LogError(r.Context(), "Failed to scan agent: %v", err)
			continue
		}

		if len(capabilitiesBytes) > 0 {
			json.Unmarshal(capabilitiesBytes, &agent.Capabilities)
		}
		if len(settingsBytes) > 0 {
			json.Unmarshal(settingsBytes, &agent.Settings)
		}

		agents = append(agents, agent)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"agents": agents,
		"count":  len(agents),
	})
}

// createOrgPatternHandler creates a new organization pattern
func createOrgPatternHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		OrgID       string  `json:"org_id"`
		Name        string  `json:"name"`
		Description string  `json:"description,omitempty"`
		PatternType string  `json:"pattern_type"`
		PatternData JSONMap `json:"pattern_data"`
		IsShared    *bool   `json:"is_shared,omitempty"`
		CreatedBy   string  `json:"created_by,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.OrgID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "org_id",
			Message: "Organization ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	if req.Name == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "name",
			Message: "Pattern name is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	if req.PatternType == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "pattern_type",
			Message: "Pattern type is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	isShared := true
	if req.IsShared != nil {
		isShared = *req.IsShared
	}

	patternID := uuid.New().String()
	now := time.Now()
	patternDataBytes, _ := json.Marshal(req.PatternData)

	_, err := db.Exec(`
		INSERT INTO organization_patterns (id, org_id, name, description, pattern_type,
			pattern_data, is_shared, created_by, created_at, updated_at)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)`,
		patternID, req.OrgID, req.Name, req.Description, req.PatternType,
		patternDataBytes, isShared, req.CreatedBy, now, now)

	if err != nil {
		LogError(r.Context(), "Failed to create organization pattern: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "create_org_pattern",
			Message:   "Failed to create organization pattern",
		}, http.StatusInternalServerError)
		return
	}

	pattern := OrganizationPattern{
		ID:          patternID,
		OrgID:       req.OrgID,
		Name:        req.Name,
		Description: req.Description,
		PatternType: req.PatternType,
		PatternData: req.PatternData,
		IsShared:    isShared,
		CreatedBy:   req.CreatedBy,
		CreatedAt:   now,
		UpdatedAt:   now,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(pattern)
}

// getOrgPatternsHandler lists organization patterns
func getOrgPatternsHandler(w http.ResponseWriter, r *http.Request) {
	orgID := r.URL.Query().Get("org_id")
	patternType := r.URL.Query().Get("pattern_type")

	if orgID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "org_id",
			Message: "Organization ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	var query string
	var args []interface{}

	if patternType != "" {
		query = `SELECT id, org_id, name, description, pattern_type, pattern_data,
			is_shared, created_by, created_at, updated_at
			FROM organization_patterns
			WHERE org_id = $1 AND pattern_type = $2
			ORDER BY created_at DESC`
		args = []interface{}{orgID, patternType}
	} else {
		query = `SELECT id, org_id, name, description, pattern_type, pattern_data,
			is_shared, created_by, created_at, updated_at
			FROM organization_patterns
			WHERE org_id = $1
			ORDER BY created_at DESC`
		args = []interface{}{orgID}
	}

	rows, err := db.Query(query, args...)
	if err != nil {
		LogError(r.Context(), "Failed to query organization patterns: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "list_org_patterns",
			Message:   "Failed to retrieve organization patterns",
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var patterns []OrganizationPattern
	for rows.Next() {
		var pattern OrganizationPattern
		var patternDataBytes []byte
		err := rows.Scan(&pattern.ID, &pattern.OrgID, &pattern.Name, &pattern.Description,
			&pattern.PatternType, &patternDataBytes, &pattern.IsShared, &pattern.CreatedBy,
			&pattern.CreatedAt, &pattern.UpdatedAt)
		if err != nil {
			LogError(r.Context(), "Failed to scan organization pattern: %v", err)
			continue
		}

		if len(patternDataBytes) > 0 {
			json.Unmarshal(patternDataBytes, &pattern.PatternData)
		}

		patterns = append(patterns, pattern)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"patterns": patterns,
		"count":    len(patterns),
	})
}

// distributePatternHandler distributes a pattern to targets
func distributePatternHandler(w http.ResponseWriter, r *http.Request) {
	patternID := chi.URLParam(r, "patternId")
	if patternID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "patternId",
			Message: "Pattern ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		TargetType string `json:"target_type"` // "agent", "team", "project"
		TargetID   string `json:"target_id"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.TargetType == "" || req.TargetID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "target",
			Message: "Both target_type and target_id are required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	if req.TargetType != "agent" && req.TargetType != "team" && req.TargetType != "project" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "target_type",
			Message: "Target type must be one of: agent, team, project",
			Code:    "invalid_value",
		}, http.StatusBadRequest)
		return
	}

	distributionID := uuid.New().String()
	now := time.Now()

	_, err := db.Exec(`
		INSERT INTO pattern_distributions (id, pattern_id, target_type, target_id, status, created_at)
		VALUES ($1, $2, $3, $4, $5, $6)`,
		distributionID, patternID, req.TargetType, req.TargetID, "distributed", now)

	if err != nil {
		LogError(r.Context(), "Failed to distribute pattern: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "distribute_pattern",
			Message:   "Failed to distribute pattern",
		}, http.StatusInternalServerError)
		return
	}

	distribution := PatternDistribution{
		ID:            distributionID,
		PatternID:     patternID,
		TargetType:    req.TargetType,
		TargetID:      req.TargetID,
		Status:        "distributed",
		DistributedAt: &now,
		CreatedAt:     now,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(distribution)
}

// getPatternDistributionsHandler lists pattern distributions
func getPatternDistributionsHandler(w http.ResponseWriter, r *http.Request) {
	patternID := r.URL.Query().Get("pattern_id")
	targetType := r.URL.Query().Get("target_type")
	targetID := r.URL.Query().Get("target_id")

	var query string
	var args []interface{}

	if patternID != "" {
		query = `SELECT id, pattern_id, target_type, target_id, status, distributed_at, error_message, created_at
			FROM pattern_distributions WHERE pattern_id = $1 ORDER BY created_at DESC`
		args = []interface{}{patternID}
	} else if targetType != "" && targetID != "" {
		query = `SELECT id, pattern_id, target_type, target_id, status, distributed_at, error_message, created_at
			FROM pattern_distributions WHERE target_type = $1 AND target_id = $2 ORDER BY created_at DESC`
		args = []interface{}{targetType, targetID}
	} else {
		WriteErrorResponse(w, &ValidationError{
			Field:   "query",
			Message: "Either pattern_id or both target_type and target_id are required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	rows, err := db.Query(query, args...)
	if err != nil {
		LogError(r.Context(), "Failed to query pattern distributions: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "list_pattern_distributions",
			Message:   "Failed to retrieve pattern distributions",
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var distributions []PatternDistribution
	for rows.Next() {
		var distribution PatternDistribution
		err := rows.Scan(&distribution.ID, &distribution.PatternID, &distribution.TargetType,
			&distribution.TargetID, &distribution.Status, &distribution.DistributedAt,
			&distribution.ErrorMessage, &distribution.CreatedAt)
		if err != nil {
			LogError(r.Context(), "Failed to scan pattern distribution: %v", err)
			continue
		}
		distributions = append(distributions, distribution)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"distributions": distributions,
		"count":         len(distributions),
	})
}

// createAlertHandler creates a new organization alert
func createAlertHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		OrgID       string  `json:"org_id"`
		AlertType   string  `json:"alert_type"`
		Severity    string  `json:"severity"`
		Title       string  `json:"title"`
		Message     string  `json:"message"`
		ContextData JSONMap `json:"context_data,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.OrgID == "" || req.AlertType == "" || req.Severity == "" ||
		req.Title == "" || req.Message == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "required_fields",
			Message: "org_id, alert_type, severity, title, and message are required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	if req.Severity != "low" && req.Severity != "medium" &&
		req.Severity != "high" && req.Severity != "critical" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "severity",
			Message: "Severity must be one of: low, medium, high, critical",
			Code:    "invalid_value",
		}, http.StatusBadRequest)
		return
	}

	alertID := uuid.New().String()
	now := time.Now()
	contextDataBytes, _ := json.Marshal(req.ContextData)

	_, err := db.Exec(`
		INSERT INTO organization_alerts (id, org_id, alert_type, severity, title, message,
			context_data, created_at)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8)`,
		alertID, req.OrgID, req.AlertType, req.Severity, req.Title, req.Message,
		contextDataBytes, now)

	if err != nil {
		LogError(r.Context(), "Failed to create alert: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "create_alert",
			Message:   "Failed to create alert",
		}, http.StatusInternalServerError)
		return
	}

	alert := OrganizationAlert{
		ID:          alertID,
		OrgID:       req.OrgID,
		AlertType:   req.AlertType,
		Severity:    req.Severity,
		Title:       req.Title,
		Message:     req.Message,
		ContextData: req.ContextData,
		IsRead:      false,
		Resolved:    false,
		CreatedAt:   now,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(alert)
}

// getAlertsHandler retrieves organization alerts
func getAlertsHandler(w http.ResponseWriter, r *http.Request) {
	orgID := r.URL.Query().Get("org_id")
	if orgID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "org_id",
			Message: "Organization ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	includeResolved := r.URL.Query().Get("include_resolved") == "true"
	unreadOnly := r.URL.Query().Get("unread_only") == "true"

	var query string
	var args []interface{}

	if unreadOnly {
		query = `SELECT id, org_id, alert_type, severity, title, message, context_data,
			is_read, read_by, read_at, resolved, resolved_by, resolved_at, created_at
			FROM organization_alerts
			WHERE org_id = $1 AND is_read = false
			ORDER BY created_at DESC`
		args = []interface{}{orgID}
	} else if includeResolved {
		query = `SELECT id, org_id, alert_type, severity, title, message, context_data,
			is_read, read_by, read_at, resolved, resolved_by, resolved_at, created_at
			FROM organization_alerts
			WHERE org_id = $1
			ORDER BY created_at DESC`
		args = []interface{}{orgID}
	} else {
		query = `SELECT id, org_id, alert_type, severity, title, message, context_data,
			is_read, read_by, read_at, resolved, resolved_by, resolved_at, created_at
			FROM organization_alerts
			WHERE org_id = $1 AND resolved = false
			ORDER BY created_at DESC`
		args = []interface{}{orgID}
	}

	rows, err := db.Query(query, args...)
	if err != nil {
		LogError(r.Context(), "Failed to query alerts: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "list_alerts",
			Message:   "Failed to retrieve alerts",
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var alerts []OrganizationAlert
	for rows.Next() {
		var alert OrganizationAlert
		var contextDataBytes []byte
		err := rows.Scan(&alert.ID, &alert.OrgID, &alert.AlertType, &alert.Severity,
			&alert.Title, &alert.Message, &contextDataBytes, &alert.IsRead, &alert.ReadBy,
			&alert.ReadAt, &alert.Resolved, &alert.ResolvedBy, &alert.ResolvedAt, &alert.CreatedAt)
		if err != nil {
			LogError(r.Context(), "Failed to scan alert: %v", err)
			continue
		}

		if len(contextDataBytes) > 0 {
			json.Unmarshal(contextDataBytes, &alert.ContextData)
		}

		alerts = append(alerts, alert)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"alerts": alerts,
		"count":  len(alerts),
	})
}

// markAlertReadHandler marks an alert as read
func markAlertReadHandler(w http.ResponseWriter, r *http.Request) {
	alertID := chi.URLParam(r, "alertId")
	if alertID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "alertId",
			Message: "Alert ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		ReadBy string `json:"read_by,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		// Allow empty body for simple read marking
		req.ReadBy = "system"
	}

	now := time.Now()

	result, err := db.Exec(`
		UPDATE organization_alerts
		SET is_read = true, read_by = $1, read_at = $2
		WHERE id = $3 AND is_read = false`,
		req.ReadBy, now, alertID)

	if err != nil {
		LogError(r.Context(), "Failed to mark alert as read: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "mark_alert_read",
			Message:   "Failed to mark alert as read",
		}, http.StatusInternalServerError)
		return
	}

	rowsAffected, _ := result.RowsAffected()
	if rowsAffected == 0 {
		WriteErrorResponse(w, &ValidationError{
			Field:   "alert",
			Message: "Alert not found or already read",
			Code:    "not_found",
		}, http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Alert marked as read",
	})
}

// resolveAlertHandler marks an alert as resolved
func resolveAlertHandler(w http.ResponseWriter, r *http.Request) {
	alertID := chi.URLParam(r, "alertId")
	if alertID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "alertId",
			Message: "Alert ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		ResolvedBy string `json:"resolved_by,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		req.ResolvedBy = "system"
	}

	now := time.Now()

	result, err := db.Exec(`
		UPDATE organization_alerts
		SET resolved = true, resolved_by = $1, resolved_at = $2
		WHERE id = $3 AND resolved = false`,
		req.ResolvedBy, now, alertID)

	if err != nil {
		LogError(r.Context(), "Failed to resolve alert: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "resolve_alert",
			Message:   "Failed to resolve alert",
		}, http.StatusInternalServerError)
		return
	}

	rowsAffected, _ := result.RowsAffected()
	if rowsAffected == 0 {
		WriteErrorResponse(w, &ValidationError{
			Field:   "alert",
			Message: "Alert not found or already resolved",
			Code:    "not_found",
		}, http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Alert resolved",
	})
}

// createAlertRuleHandler creates a new alert rule
func createAlertRuleHandler(w http.ResponseWriter, r *http.Request) {
	var req struct {
		OrgID      string  `json:"org_id"`
		RuleName   string  `json:"rule_name"`
		RuleType   string  `json:"rule_type"`
		Conditions JSONMap `json:"conditions"`
		Actions    JSONMap `json:"actions"`
		CreatedBy  string  `json:"created_by,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.OrgID == "" || req.RuleName == "" || req.RuleType == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "required_fields",
			Message: "org_id, rule_name, and rule_type are required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	ruleID := uuid.New().String()
	now := time.Now()
	conditionsBytes, _ := json.Marshal(req.Conditions)
	actionsBytes, _ := json.Marshal(req.Actions)

	_, err := db.Exec(`
		INSERT INTO alert_rules (id, org_id, rule_name, rule_type, conditions, actions,
			is_active, created_by, created_at, updated_at)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)`,
		ruleID, req.OrgID, req.RuleName, req.RuleType, conditionsBytes, actionsBytes,
		true, req.CreatedBy, now, now)

	if err != nil {
		LogError(r.Context(), "Failed to create alert rule: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "create_alert_rule",
			Message:   "Failed to create alert rule",
		}, http.StatusInternalServerError)
		return
	}

	rule := AlertRule{
		ID:         ruleID,
		OrgID:      req.OrgID,
		RuleName:   req.RuleName,
		RuleType:   req.RuleType,
		Conditions: req.Conditions,
		Actions:    req.Actions,
		IsActive:   true,
		CreatedBy:  req.CreatedBy,
		CreatedAt:  now,
		UpdatedAt:  now,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(rule)
}

// getAlertRulesHandler lists alert rules for an organization
func getAlertRulesHandler(w http.ResponseWriter, r *http.Request) {
	orgID := r.URL.Query().Get("org_id")
	if orgID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "org_id",
			Message: "Organization ID is required",
			Code:    "required_field",
		}, http.StatusBadRequest)
		return
	}

	rows, err := db.Query(`
		SELECT id, org_id, rule_name, rule_type, conditions, actions, is_active,
			created_by, created_at, updated_at
		FROM alert_rules
		WHERE org_id = $1
		ORDER BY created_at DESC`, orgID)

	if err != nil {
		LogError(r.Context(), "Failed to query alert rules: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation: "list_alert_rules",
			Message:   "Failed to retrieve alert rules",
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var rules []AlertRule
	for rows.Next() {
		var rule AlertRule
		var conditionsBytes, actionsBytes []byte
		err := rows.Scan(&rule.ID, &rule.OrgID, &rule.RuleName, &rule.RuleType,
			&conditionsBytes, &actionsBytes, &rule.IsActive, &rule.CreatedBy,
			&rule.CreatedAt, &rule.UpdatedAt)
		if err != nil {
			LogError(r.Context(), "Failed to scan alert rule: %v", err)
			continue
		}

		if len(conditionsBytes) > 0 {
			json.Unmarshal(conditionsBytes, &rule.Conditions)
		}
		if len(actionsBytes) > 0 {
			json.Unmarshal(actionsBytes, &rule.Actions)
		}

		rules = append(rules, rule)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"rules": rules,
		"count": len(rules),
	})
}

// =============================================================================
// AST ANALYSIS (Phase 6) - Stub Implementation
// =============================================================================

// AST Analysis Request
type ASTAnalysisRequest struct {
	Code      string   `json:"code"`
	Language  string   `json:"language"`
	Filename  string   `json:"filename"`
	ProjectID string   `json:"projectId"`
	Analyses  []string `json:"analyses"` // duplicates, unused, unreachable, security
}

// AST Finding
type ASTFinding struct {
	Type       string `json:"type"` // duplicate_function, unused_variable, etc.
	Severity   string `json:"severity"`
	Line       int    `json:"line"`
	Column     int    `json:"column"`
	EndLine    int    `json:"endLine"`
	EndColumn  int    `json:"endColumn"`
	Message    string `json:"message"`
	Code       string `json:"code"`
	Suggestion string `json:"suggestion"`
}

// AST Analysis Response
type ASTAnalysisResponse struct {
	Success  bool         `json:"success"`
	Findings []ASTFinding `json:"findings"`
	Stats    struct {
		ParseTime    int64 `json:"parseTimeMs"`
		AnalysisTime int64 `json:"analysisTimeMs"`
		NodesVisited int   `json:"nodesVisited"`
	} `json:"stats"`
}

// AST Analysis Handler - Phase 6 Implementation
// Status: ✅ IMPLEMENTED - Full Tree-sitter AST analysis
func astAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	var req ASTAnalysisRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Printf("Error decoding AST analysis request: %v", err)
		http.Error(w, "Invalid request: "+err.Error(), http.StatusBadRequest)
		return
	}

	// Validate request
	if req.Code == "" {
		log.Printf("AST analysis request missing code")
		http.Error(w, "Missing required field: code", http.StatusBadRequest)
		return
	}

	// Initialize parsers if not already done
	if len(parsers) == 0 {
		initParsers()
	}

	// Perform AST analysis
	findings, stats, err := analyzeAST(req.Code, req.Language, req.Analyses)
	if err != nil {
		log.Printf("AST analysis error for file %s (language: %s): %v", req.Filename, req.Language, err)
		http.Error(w, fmt.Sprintf("Analysis error: %v", err), http.StatusInternalServerError)
		return
	}

	response := ASTAnalysisResponse{
		Success:  true,
		Findings: findings,
		Stats: struct {
			ParseTime    int64 `json:"parseTimeMs"`
			AnalysisTime int64 `json:"analysisTimeMs"`
			NodesVisited int   `json:"nodesVisited"`
		}{
			ParseTime:    stats.ParseTime,
			AnalysisTime: stats.AnalysisTime,
			NodesVisited: stats.NodesVisited,
		},
	}

	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(response); err != nil {
		log.Printf("Error encoding AST analysis response: %v", err)
		http.Error(w, "Error encoding response", http.StatusInternalServerError)
		return
	}
}

// Vibe Analysis Handler - ✅ IMPLEMENTED (Phase 7)
// Status: ✅ COMPLETE - Full AST-based vibe pattern detection
// Implements: Duplicate functions, unused variables, unreachable code, orphaned code
func vibeAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	var req ASTAnalysisRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Printf("Error decoding vibe analysis request: %v", err)
		http.Error(w, "Invalid request: "+err.Error(), http.StatusBadRequest)
		return
	}

	// Validate request
	if req.Code == "" {
		log.Printf("Vibe analysis request missing code")
		http.Error(w, "Missing required field: code", http.StatusBadRequest)
		return
	}

	// Initialize parsers if not already done
	if len(parsers) == 0 {
		initParsers()
	}

	// Perform vibe-specific AST analysis (duplicates, unused, unreachable)
	analyses := []string{"duplicates", "unused", "unreachable"}
	if len(req.Analyses) > 0 {
		analyses = req.Analyses
	}

	findings, stats, err := analyzeAST(req.Code, req.Language, analyses)
	if err != nil {
		log.Printf("Vibe analysis failed: %v", err)
		http.Error(w, fmt.Sprintf("Analysis error: %v", err), http.StatusInternalServerError)
		return
	}

	response := ASTAnalysisResponse{
		Success:  true,
		Findings: findings,
		Stats: struct {
			ParseTime    int64 `json:"parseTimeMs"`
			AnalysisTime int64 `json:"analysisTimeMs"`
			NodesVisited int   `json:"nodesVisited"`
		}{
			ParseTime:    stats.ParseTime,
			AnalysisTime: stats.AnalysisTime,
			NodesVisited: stats.NodesVisited,
		},
	}

	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(response); err != nil {
		log.Printf("Error encoding vibe analysis response: %v", err)
		http.Error(w, "Error encoding response", http.StatusInternalServerError)
		return
	}
}

// Cross-File Analysis Handler - Phase 6F Implementation
// Status: ✅ COMPLETE - Functional cross-file analysis
func crossFileAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	type CrossFileRequest struct {
		Files []struct {
			Path     string `json:"path"`
			Code     string `json:"code"`
			Language string `json:"language,omitempty"`
		} `json:"files"`
		Language  string `json:"language"` // Default language if not specified per file
		ProjectID string `json:"projectId"`
	}

	var req CrossFileRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Printf("Error decoding cross-file analysis request: %v", err)
		http.Error(w, "Invalid request: "+err.Error(), http.StatusBadRequest)
		return
	}

	if len(req.Files) == 0 {
		log.Printf("Cross-file analysis request missing files")
		http.Error(w, "No files provided", http.StatusBadRequest)
		return
	}

	// Initialize parsers if not already done
	if len(parsers) == 0 {
		initParsers()
	}

	// Prepare files for analysis
	analysisFiles := make([]struct {
		Path     string
		Code     string
		Language string
	}, len(req.Files))

	for i, file := range req.Files {
		analysisFiles[i].Path = file.Path
		analysisFiles[i].Code = file.Code
		// Use file-specific language or fallback to request language
		if file.Language != "" {
			analysisFiles[i].Language = file.Language
		} else {
			analysisFiles[i].Language = req.Language
		}
	}

	// Perform cross-file analysis
	findings, stats, err := analyzeCrossFile(analysisFiles)
	if err != nil {
		log.Printf("Cross-file analysis failed: %v", err)
		http.Error(w, fmt.Sprintf("Analysis error: %v", err), http.StatusInternalServerError)
		return
	}

	response := ASTAnalysisResponse{
		Success:  true,
		Findings: findings,
		Stats: struct {
			ParseTime    int64 `json:"parseTimeMs"`
			AnalysisTime int64 `json:"analysisTimeMs"`
			NodesVisited int   `json:"nodesVisited"`
		}{
			ParseTime:    stats.ParseTime,
			AnalysisTime: stats.AnalysisTime,
			NodesVisited: stats.NodesVisited,
		},
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

// =============================================================================
// SECURITY ANALYSIS (Phase 8) ✅ IMPLEMENTED - Full security rule checking with AST analysis
// =============================================================================

// Security Analysis Request
type SecurityAnalysisRequest struct {
	Code             string          `json:"code"`
	Language         string          `json:"language"`
	Filename         string          `json:"filename"`
	ProjectID        string          `json:"projectId"`
	Rules            []string        `json:"rules,omitempty"`            // Specific rules to check (SEC-001, etc.)
	ExpectedFindings map[string]bool `json:"expectedFindings,omitempty"` // Ground truth for detection rate validation
}

// Security Finding
type SecurityFinding struct {
	RuleID      string `json:"ruleId"`
	RuleName    string `json:"ruleName"`
	Severity    string `json:"severity"`
	Line        int    `json:"line"`
	Code        string `json:"code"`
	Issue       string `json:"issue"`
	Remediation string `json:"remediation"`
	AutoFixable bool   `json:"autoFixable"`
}

// Security Analysis Response
type SecurityAnalysisResponse struct {
	Score    int               `json:"score"` // 0-100
	Grade    string            `json:"grade"` // A, B, C, D, F
	Findings []SecurityFinding `json:"findings"`
	Summary  struct {
		TotalRules int `json:"totalRules"`
		Passed     int `json:"passed"`
		Failed     int `json:"failed"`
		Critical   int `json:"critical"`
		High       int `json:"high"`
		Medium     int `json:"medium"`
		Low        int `json:"low"`
	} `json:"summary"`
	Metrics *security.DetectionMetrics `json:"metrics,omitempty"` // Optional: only for validation runs
}

// Security Rules Definitions (SEC-001 to SEC-008)
var securityRules = map[string]struct {
	Name     string
	Severity string
	Type     string
}{
	"SEC-001": {"Resource Ownership", "critical", "authorization"},
	"SEC-002": {"SQL Injection", "critical", "injection"},
	"SEC-003": {"Auth Middleware", "critical", "authentication"},
	"SEC-004": {"Rate Limiting", "high", "transport"},
	"SEC-005": {"Password Hashing", "critical", "cryptography"},
	"SEC-006": {"Input Validation", "high", "validation"},
	"SEC-007": {"Secure Headers", "medium", "transport"},
	"SEC-008": {"CORS Config", "high", "transport"},
}

// Security Analysis Handler - ✅ IMPLEMENTED (Phase 8)
// Status: ✅ COMPLETE - Full security rule checking with AST analysis
func securityAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	var req SecurityAnalysisRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid request", http.StatusBadRequest)
		return
	}

	// Perform security analysis
	findings, err := analyzeSecurity(req.Code, req.Language, req.Filename, req.Rules)
	if err != nil {
		http.Error(w, fmt.Sprintf("Security analysis failed: %v", err), http.StatusInternalServerError)
		return
	}

	// Calculate security score and summary
	score, grade := calculateSecurityScore(findings)
	summary := calculateSecuritySummary(findings)

	// Convert security.SecurityFinding to local SecurityFinding
	localFindings := make([]SecurityFinding, len(findings))
	for i, f := range findings {
		localFindings[i] = SecurityFinding{
			RuleID:      f.RuleID,
			RuleName:    f.RuleName,
			Severity:    f.Severity,
			Line:        f.Line,
			Code:       f.Code,
			Issue:      f.Issue,
			Remediation: f.Remediation,
			AutoFixable: f.AutoFixable,
		}
	}

	response := SecurityAnalysisResponse{
		Score:    score,
		Grade:    grade,
		Findings: localFindings,
		Summary:  summary,
	}

	// Calculate detection rate metrics if ground truth is provided
	if len(req.ExpectedFindings) > 0 {
		metrics := calculateDetectionRate(findings, req.ExpectedFindings)
		response.Metrics = &metrics
		detectionRate := float64(metrics.TruePositives+metrics.FalsePositives) / float64(len(req.ExpectedFindings)) * 100
		log.Printf("Detection rate metrics calculated: %.2f%% detection rate, %.2f%% precision, %.2f%% recall",
			detectionRate, metrics.Precision, metrics.Recall)
	}

	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(response); err != nil {
		log.Printf("Error encoding security analysis response: %v", err)
		http.Error(w, "Error encoding response", http.StatusInternalServerError)
		return
	}
}

// docSyncHandler handles doc-sync analysis requests (Phase 11)
func docSyncHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req DocSyncRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Set project ID from context
	req.ProjectID = project.ID

	// Default report type if not specified
	if req.ReportType == "" {
		req.ReportType = "status_tracking"
	}

	// Get codebase path (default to current directory, can be configured)
	codebasePath := "."
	if path, ok := req.Options["codebase_path"].(string); ok && path != "" {
		codebasePath = path
	}

	// Perform analysis
	ctx := r.Context()
	response, err := analyzeDocSync(ctx, req, codebasePath)
	if err != nil {
		LogErrorWithContext(ctx, err, "Doc-sync analysis error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "doc_sync_analysis",
			Message:       fmt.Sprintf("Analysis failed: %v", err),
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
}

// businessRulesComparisonHandler handles business rules comparison requests (Phase 11B)
func businessRulesComparisonHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		KnowledgeItemIDs []string `json:"knowledgeItemIds,omitempty"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	codebasePath := "."
	ctx := r.Context()
	discrepancies, err := compareBusinessRules(ctx, project.ID, codebasePath)
	if err != nil {
		log.Printf("Business rules comparison error: %v", err)
		http.Error(w, fmt.Sprintf("Comparison failed: %v", err), http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":       true,
		"discrepancies": discrepancies,
		"count":         len(discrepancies),
	})
}

// reviewQueueHandler returns pending review items (Phase 11B)
func reviewQueueHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}
	status := r.URL.Query().Get("status")
	if status == "" {
		status = "pending"
	}

	// Query doc_sync_updates table for pending reviews
	query := `
		SELECT id, file_path, change_type, old_value, new_value, line_number, created_at
		FROM doc_sync_updates
		WHERE project_id = $1 AND applied = false
	`
	if status != "" {
		query += " AND change_type = $2"
	}
	query += " ORDER BY created_at DESC"

	var rows *sql.Rows
	if status != "" {
		rows, err = db.Query(query, project.ID, status)
	} else {
		rows, err = db.Query(query, project.ID)
	}

	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to query review queue: %v", err), http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	var reviews []map[string]interface{}
	for rows.Next() {
		var id, filePath, changeType, oldValue, newValue string
		var lineNumber sql.NullInt64
		var createdAt time.Time

		err := rows.Scan(&id, &filePath, &changeType, &oldValue, &newValue, &lineNumber, &createdAt)
		if err != nil {
			continue
		}

		review := map[string]interface{}{
			"id":          id,
			"file_path":   filePath,
			"change_type": changeType,
			"old_value":   oldValue,
			"new_value":   newValue,
			"created_at":  createdAt,
		}
		if lineNumber.Valid {
			review["line_number"] = lineNumber.Int64
		}
		reviews = append(reviews, review)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"reviews": reviews,
		"count":   len(reviews),
	})
}

// reviewHandler handles approval/rejection of review items (Phase 11B)
func reviewHandler(w http.ResponseWriter, r *http.Request) {
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}
	reviewID := chi.URLParam(r, "id")

	var req struct {
		Action  string `json:"action"` // "approve" or "reject"
		Comment string `json:"comment,omitempty"`
	}
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.Action != "approve" && req.Action != "reject" {
		http.Error(w, "Action must be 'approve' or 'reject'", http.StatusBadRequest)
		return
	}

	// Update review status
	query := `
		UPDATE doc_sync_updates
		SET applied = $1, approved_by = $2, approved_at = $3
		WHERE id = $4 AND project_id = $5
	`
	approved := req.Action == "approve"
	_, err = db.Exec(query, approved, "user", time.Now(), reviewID, project.ID)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to update review: %v", err), http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": fmt.Sprintf("Review %s", req.Action),
	})
}

// architectureAnalysisHandler handles architecture analysis requests (Phase 9)
func architectureAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	var req ArchitectureAnalysisRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		log.Printf("Error decoding architecture analysis request: %v", err)
		http.Error(w, "Invalid request: "+err.Error(), http.StatusBadRequest)
		return
	}

	// Perform architecture analysis
	response := analyzeArchitecture(req.Files)

	w.Header().Set("Content-Type", "application/json")
	if err := json.NewEncoder(w).Encode(response); err != nil {
		log.Printf("Error encoding architecture analysis response: %v", err)
		http.Error(w, "Error encoding response", http.StatusInternalServerError)
		return
	}
}

// =============================================================================
// BINARY DISTRIBUTION HANDLERS
// =============================================================================

// getBinaryVersionHandler returns latest version info for platform
func getBinaryVersionHandler(w http.ResponseWriter, r *http.Request) {
	platform := r.URL.Query().Get("platform")
	if platform == "" {
		platform = detectPlatformFromUserAgent(r.Header.Get("User-Agent"))
	}

	query := `
		SELECT version, platform, file_size, checksum_sha256, checksum_md5,
		       signature, release_notes, released_at, min_go_version
		FROM binary_versions
		WHERE platform = $1 AND is_stable = true AND is_latest = true
		ORDER BY released_at DESC
		LIMIT 1
	`

	var version struct {
		Version        string
		Platform       string
		FileSize       int64
		ChecksumSHA256 string
		ChecksumMD5    sql.NullString
		Signature      sql.NullString
		ReleaseNotes   sql.NullString
		ReleasedAt     sql.NullTime
		MinGoVersion   sql.NullString
	}

	err := db.QueryRow(query, platform).Scan(
		&version.Version, &version.Platform, &version.FileSize,
		&version.ChecksumSHA256, &version.ChecksumMD5, &version.Signature,
		&version.ReleaseNotes, &version.ReleasedAt, &version.MinGoVersion,
	)

	if err == sql.ErrNoRows {
		WriteErrorResponse(w, &NotFoundError{
			Message: fmt.Sprintf("No binary found for platform: %s", platform),
		}, http.StatusNotFound)
		return
	}

	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "query_binary_version",
			Message:       "Failed to query binary version",
			Code:          "database_error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	response := map[string]interface{}{
		"version":         version.Version,
		"platform":        version.Platform,
		"file_size":       version.FileSize,
		"checksum_sha256": version.ChecksumSHA256,
	}

	if version.ChecksumMD5.Valid {
		response["checksum_md5"] = version.ChecksumMD5.String
	}
	if version.Signature.Valid {
		response["signature"] = version.Signature.String
	}
	if version.ReleaseNotes.Valid {
		response["release_notes"] = version.ReleaseNotes.String
	}
	if version.ReleasedAt.Valid {
		response["released_at"] = version.ReleasedAt.Time.Format(time.RFC3339)
	}
	if version.MinGoVersion.Valid {
		response["min_go_version"] = version.MinGoVersion.String
	}

	WriteJSONResponse(w, response, http.StatusOK)
}

// detectPlatformFromUserAgent detects platform from User-Agent header
func detectPlatformFromUserAgent(ua string) string {
	ua = strings.ToLower(ua)
	if strings.Contains(ua, "darwin") || strings.Contains(ua, "mac") {
		if strings.Contains(ua, "arm64") || strings.Contains(ua, "aarch64") {
			return "darwin-arm64"
		}
		return "darwin-amd64"
	}
	if strings.Contains(ua, "linux") {
		if strings.Contains(ua, "arm64") || strings.Contains(ua, "aarch64") {
			return "linux-arm64"
		}
		return "linux-amd64"
	}
	if strings.Contains(ua, "windows") {
		return "windows-amd64"
	}
	return "linux-amd64" // Default fallback
}

// downloadBinaryHandler streams binary file to client
func downloadBinaryHandler(w http.ResponseWriter, r *http.Request) {
	version := r.URL.Query().Get("version")
	platform := r.URL.Query().Get("platform")

	if version == "" || platform == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "version,platform",
			Message: "version and platform query parameters are required",
			Code:    "missing_params",
		}, http.StatusBadRequest)
		return
	}

	query := `
		SELECT file_path, file_size, checksum_sha256
		FROM binary_versions
		WHERE version = $1 AND platform = $2
	`

	var filePath string
	var fileSize int64
	var checksum string

	err := db.QueryRow(query, version, platform).Scan(&filePath, &fileSize, &checksum)
	if err == sql.ErrNoRows {
		WriteErrorResponse(w, &NotFoundError{
			Message: fmt.Sprintf("Binary not found: %s for %s", version, platform),
		}, http.StatusNotFound)
		return
	}
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "query_binary",
			Message:       "Failed to query binary",
			Code:          "database_error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	file, err := os.Open(filePath)
	if err != nil {
		WriteErrorResponse(w, &InternalError{
			Message: "Failed to open binary file",
			Code:    "file_error",
		}, http.StatusInternalServerError)
		return
	}
	defer file.Close()

	filename := fmt.Sprintf("sentinel-%s-%s", version, platform)
	if platform == "windows-amd64" {
		filename += ".exe"
	}

	w.Header().Set("Content-Type", "application/octet-stream")
	w.Header().Set("Content-Disposition", fmt.Sprintf("attachment; filename=%s", filename))
	w.Header().Set("Content-Length", strconv.FormatInt(fileSize, 10))
	w.Header().Set("X-Checksum-SHA256", checksum)
	w.Header().Set("X-Version", version)
	w.Header().Set("X-Platform", platform)

	io.Copy(w, file)

	// Track download asynchronously
	go trackBinaryDownload(version, platform, r)
}

// trackBinaryDownload tracks binary download for analytics
func trackBinaryDownload(version, platform string, r *http.Request) {
	project := r.Context().Value(projectKey)
	if project == nil {
		return
	}

	var versionID uuid.UUID
	err := db.QueryRow("SELECT id FROM binary_versions WHERE version = $1 AND platform = $2",
		version, platform).Scan(&versionID)
	if err != nil {
		return
	}

	projectObj := project.(*Project)
	ipAddr := r.RemoteAddr
	if forwarded := r.Header.Get("X-Forwarded-For"); forwarded != "" {
		parts := strings.Split(forwarded, ",")
		if len(parts) > 0 {
			ipAddr = strings.TrimSpace(parts[0])
		}
	}

	db.Exec(`
		INSERT INTO binary_downloads (version_id, project_id, user_agent, ip_address)
		VALUES ($1, $2, $3, $4)
	`, versionID, projectObj.ID, r.UserAgent(), ipAddr)
}

// uploadBinaryHandler allows admins to upload new binary versions
func uploadBinaryHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()

	err := r.ParseMultipartForm(100 << 20) // 100MB max
	if err != nil {
		WriteErrorResponse(w, &ValidationError{
			Message: "Failed to parse form",
			Code:    "parse_error",
		}, http.StatusBadRequest)
		return
	}

	version := r.FormValue("version")
	platform := r.FormValue("platform")
	releaseNotes := r.FormValue("release_notes")
	isStable := r.FormValue("is_stable") == "true"
	isLatest := r.FormValue("is_latest") == "true"

	if version == "" || platform == "" {
		WriteErrorResponse(w, &ValidationError{
			Message: "version and platform are required",
			Code:    "missing_fields",
		}, http.StatusBadRequest)
		return
	}

	// Validate version format (semver)
	if err := validateVersionFormat(version); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "version",
			Message: err.Error(),
			Code:    "invalid_format",
		}, http.StatusBadRequest)
		return
	}

	// Validate platform against allowed list
	if err := validatePlatform(platform); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "platform",
			Message: err.Error(),
			Code:    "invalid_platform",
		}, http.StatusBadRequest)
		return
	}

	// Sanitize release notes
	releaseNotes = sanitizeString(releaseNotes, 10000) // Max 10KB

	file, header, err := r.FormFile("binary")
	if err != nil {
		WriteErrorResponse(w, &ValidationError{
			Message: "binary file is required",
			Code:    "missing_file",
		}, http.StatusBadRequest)
		return
	}

	// Validate content type
	contentType := header.Header.Get("Content-Type")
	if err := validateBinaryContentType(contentType); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "binary",
			Message: err.Error(),
			Code:    "invalid_content_type",
		}, http.StatusBadRequest)
		return
	}
	defer file.Close()

	// Calculate checksum and validate file size
	hash := sha256.New()
	size, err := io.Copy(hash, file)
	if err != nil {
		WriteErrorResponse(w, &InternalError{
			Message: "Failed to read file",
			Code:    "file_error",
		}, http.StatusInternalServerError)
		return
	}

	// Validate file size (100MB limit as documented in API)
	maxFileSize := int64(100 * 1024 * 1024) // 100MB
	if size > maxFileSize {
		WriteErrorResponse(w, &ValidationError{
			Field:   "binary",
			Message: fmt.Sprintf("File size %d bytes exceeds maximum allowed size of %d bytes (100MB)", size, maxFileSize),
			Code:    "file_too_large",
		}, http.StatusBadRequest)
		return
	}

	checksum := hex.EncodeToString(hash.Sum(nil))

	// Save file
	config := GetConfig()
	storagePath := filepath.Join(config.BinaryStorage, version, platform)

	// Create directory with error handling
	if err := os.MkdirAll(filepath.Dir(storagePath), 0755); err != nil {
		WriteErrorResponse(w, &InternalError{
			Message: "Failed to create storage directory",
			Code:    "storage_error",
		}, http.StatusInternalServerError)
		return
	}

	destFile, err := os.Create(storagePath)
	if err != nil {
		WriteErrorResponse(w, &InternalError{
			Message: "Failed to save file",
			Code:    "storage_error",
		}, http.StatusInternalServerError)
		return
	}
	defer destFile.Close()

	// Reset file pointer with error handling
	if _, err := file.Seek(0, 0); err != nil {
		destFile.Close()
		os.Remove(storagePath)
		WriteErrorResponse(w, &InternalError{
			Message: "Failed to reset file pointer",
			Code:    "file_error",
		}, http.StatusInternalServerError)
		return
	}

	// Copy file with error handling and verification
	written, err := io.Copy(destFile, file)
	if err != nil {
		destFile.Close()
		os.Remove(storagePath)
		WriteErrorResponse(w, &InternalError{
			Message: "Failed to save file",
			Code:    "file_error",
		}, http.StatusInternalServerError)
		return
	}
	if written != size {
		destFile.Close()
		os.Remove(storagePath)
		WriteErrorResponse(w, &InternalError{
			Message: "File size mismatch",
			Code:    "file_error",
		}, http.StatusInternalServerError)
		return
	}

	// Parse platform to extract OS and architecture
	parts := strings.Split(platform, "-")
	osName := parts[0]
	arch := "amd64"
	if len(parts) > 1 {
		arch = parts[1]
	}

	// Insert database record
	query := `
		INSERT INTO binary_versions 
		(version, platform, architecture, os, file_path, file_size, 
		 checksum_sha256, release_notes, is_stable, is_latest, released_at)
		VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, NOW())
		ON CONFLICT (version, platform) DO UPDATE SET
			file_path = EXCLUDED.file_path,
			file_size = EXCLUDED.file_size,
			checksum_sha256 = EXCLUDED.checksum_sha256,
			release_notes = EXCLUDED.release_notes,
			is_stable = EXCLUDED.is_stable,
			is_latest = EXCLUDED.is_latest,
			released_at = EXCLUDED.released_at
	`

	_, err = db.Exec(query, version, platform, arch, osName, storagePath, size,
		checksum, releaseNotes, isStable, isLatest)
	if err != nil {
		LogErrorWithContext(ctx, err, "Failed to save binary version metadata", map[string]interface{}{
			"version":   version,
			"platform":  platform,
			"file_size": size,
			"checksum":  checksum,
		})
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "save_binary_version",
			Message:       fmt.Sprintf("Failed to save version metadata for version=%s platform=%s", version, platform),
			Code:          "database_error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// If marked as latest, unmark others
	if isLatest {
		_, err := db.Exec(`
			UPDATE binary_versions 
			SET is_latest = false 
			WHERE platform = $1 AND version != $2
		`, platform, version)
		if err != nil {
			LogError(ctx, "Failed to unmark other versions as latest: %v (version=%s platform=%s)", err, version, platform)
			// Don't fail the request, but log the error for investigation
			// The new version is still saved, but other versions may still be marked as latest
		}
	}

	WriteJSONResponse(w, map[string]interface{}{
		"success":  true,
		"version":  version,
		"platform": platform,
		"checksum": checksum,
	}, http.StatusCreated)
}

// listBinaryVersionsHandler lists available versions for a platform
func listBinaryVersionsHandler(w http.ResponseWriter, r *http.Request) {
	platform := r.URL.Query().Get("platform")
	stableOnly := r.URL.Query().Get("stable") == "true"

	query := `
		SELECT version, platform, file_size, checksum_sha256, 
		       release_notes, released_at, is_stable, is_latest
		FROM binary_versions
		WHERE ($1 = '' OR platform = $1)
		AND ($2 = false OR is_stable = true)
		ORDER BY released_at DESC
		LIMIT 50
	`

	rows, err := db.Query(query, platform, stableOnly)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "query_versions",
			Message:       "Failed to query versions",
			Code:          "database_error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	versions := []map[string]interface{}{}
	ctx := r.Context()
	for rows.Next() {
		var v struct {
			Version        string
			Platform       string
			FileSize       int64
			ChecksumSHA256 string
			ReleaseNotes   sql.NullString
			ReleasedAt     sql.NullTime
			IsStable       bool
			IsLatest       bool
		}
		err := rows.Scan(&v.Version, &v.Platform, &v.FileSize, &v.ChecksumSHA256,
			&v.ReleaseNotes, &v.ReleasedAt, &v.IsStable, &v.IsLatest)
		if err != nil {
			LogWarn(ctx, "Failed to scan binary version row (skipping): %v", err)
			continue
		}

		versionMap := map[string]interface{}{
			"version":         v.Version,
			"platform":        v.Platform,
			"file_size":       v.FileSize,
			"checksum_sha256": v.ChecksumSHA256,
			"is_stable":       v.IsStable,
			"is_latest":       v.IsLatest,
		}
		if v.ReleaseNotes.Valid {
			versionMap["release_notes"] = v.ReleaseNotes.String
		}
		if v.ReleasedAt.Valid {
			versionMap["released_at"] = v.ReleasedAt.Time.Format(time.RFC3339)
		}
		versions = append(versions, versionMap)
	}

	WriteJSONResponse(w, map[string]interface{}{
		"versions": versions,
		"count":    len(versions),
	}, http.StatusOK)
}

// getLatestRulesHandler returns latest rules for project
func getLatestRulesHandler(w http.ResponseWriter, r *http.Request) {
	query := `
		SELECT rule_name, rule_content, rule_type, globs
		FROM rules_versions
		WHERE is_latest = true
		ORDER BY rule_type, rule_name
	`

	rows, err := db.Query(query)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "query_rules",
			Message:       "Failed to query rules",
			Code:          "database_error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	rules := []map[string]interface{}{}
	for rows.Next() {
		var ruleName, ruleContent, ruleType string
		var globs []string
		err := rows.Scan(&ruleName, &ruleContent, &ruleType, pq.Array(&globs))
		if err != nil {
			continue
		}

		rules = append(rules, map[string]interface{}{
			"name":    ruleName,
			"content": ruleContent,
			"type":    ruleType,
			"globs":   globs,
		})
	}

	WriteJSONResponse(w, map[string]interface{}{
		"rules": rules,
		"count": len(rules),
	}, http.StatusOK)
}

// =============================================================================
// MISSING FUNCTIONS - PLACEHOLDER IMPLEMENTATIONS
// =============================================================================

// validateProvider validates LLM provider
func validateProvider(provider string) error {
	validProviders := []string{"openai", "anthropic", "azure", "ollama"}
	for _, p := range validProviders {
		if p == provider {
			return nil
		}
	}
	return fmt.Errorf("unsupported provider: %s", provider)
}

// validateModel validates LLM model for the given provider
func validateModel(provider, model string) error {
	// Basic validation - in a real system this would check model availability
	if model == "" {
		return fmt.Errorf("model cannot be empty")
	}
	// Add provider-specific model validation here
	return nil
}

// validateAPIKeyFormat validates API key format for a provider
func validateAPIKeyFormat(provider, apiKey string) error {
	if len(apiKey) < 10 {
		return fmt.Errorf("API key too short")
	}
	// Add provider-specific validation here
	return nil
}

// validateCostOptimization validates cost optimization settings
func validateCostOptimization(config interface{}) error {
	// Basic validation - in a real system this would validate cost settings
	return nil
}

// getLLMConfigByID retrieves LLM configuration by ID and project
func getLLMConfigByID(ctx context.Context, configID, projectID string) (*LLMConfig, error) {
	// Placeholder implementation
	return &LLMConfig{ID: configID}, nil
}

// testLLMConnection tests LLM connection
func testLLMConnection(ctx context.Context, config *LLMConfig) error {
	// Placeholder implementation - in a real system this would test the connection
	return nil
}

// inferLanguageFromPath infers programming language from file path
func inferLanguageFromPath(filePath string) string {
	if strings.Contains(filePath, ".go") {
		return "go"
	} else if strings.Contains(filePath, ".js") || strings.Contains(filePath, ".ts") {
		return "javascript"
	} else if strings.Contains(filePath, ".py") {
		return "python"
	} else if strings.Contains(filePath, ".java") {
		return "java"
	}
	return "unknown"
}

// ApplySecurityFixes applies security-related fixes to code
func ApplySecurityFixes(ctx context.Context, code, language string) (string, []map[string]interface{}, error) {
	// Placeholder implementation - in a real system this would apply security fixes
	changes := []map[string]interface{}{{"type": "security", "description": "Applied security fixes"}}
	return code, changes, nil
}

// ApplyStyleFixes applies style-related fixes to code
func ApplyStyleFixes(ctx context.Context, code, language string) (string, []map[string]interface{}, error) {
	// Placeholder implementation - in a real system this would apply style fixes
	changes := []map[string]interface{}{{"type": "style", "description": "Applied style fixes"}}
	return code, changes, nil
}

// ApplyPerformanceFixes applies performance-related fixes to code
func ApplyPerformanceFixes(ctx context.Context, code, language string) (string, []map[string]interface{}, error) {
	// Placeholder implementation - in a real system this would apply performance fixes
	changes := []map[string]interface{}{{"type": "performance", "description": "Applied performance fixes"}}
	return code, changes, nil
}

// =============================================================================
// MAIN
// =============================================================================

func main_original() {
	config := GetConfig()
	validateProductionConfig(config)

	// Initialize database
	log.Printf("Connecting to database: %s", config.DatabaseURL)
	if err := initDB(config.DatabaseURL); err != nil {
		log.Fatalf("Database connection failed: %v", err)
	}
	defer db.Close()

	// Set global database for extracted packages
	// TODO: Fix database.SetGlobalDB call
	// database.SetGlobalDB(db)

	// Run migrations
	log.Println("Running migrations...")
	if err := runMigrations(); err != nil {
		log.Fatalf("Migrations failed: %v", err)
	}

	// Initialize default API version
	log.Println("Initializing API versions...")
	initializeDefaultAPIVersion()

	// Create storage directories
	if err := os.MkdirAll(config.DocumentStorage, 0755); err != nil {
		log.Fatalf("Failed to create storage directory: %v", err)
	}
	if err := os.MkdirAll(config.BinaryStorage, 0755); err != nil {
		log.Fatalf("Failed to create binary storage directory: %v", err)
	}
	if err := os.MkdirAll(config.RulesStorage, 0755); err != nil {
		log.Fatalf("Failed to create rules storage directory: %v", err)
	}

	// Configure endpoint-specific rate limits
	setEndpointRateLimiter("/api/v1/documents/ingest", 10, 20)      // 10 req/s, burst 20 for document uploads
	setEndpointRateLimiter("/api/v1/admin/binary/upload", 2, 5)     // 2 req/s, burst 5 for binary uploads (resource intensive)
	setEndpointRateLimiter("/api/v1/telemetry", 50, 100)            // 50 req/s, burst 100 for telemetry
	setEndpointRateLimiter("/api/v1/analyze/ast", 5, 10)            // 5 req/s, burst 10 for AST analysis (CPU intensive)
	setEndpointRateLimiter("/api/v1/analyze/vibe", 5, 10)           // 5 req/s, burst 10 for vibe analysis
	setEndpointRateLimiter("/api/v1/knowledge/gap-analysis", 5, 10) // 5 req/s, burst 10 for gap analysis
	setEndpointRateLimiter("/api/v1/tasks/verify", 5, 10)           // 5 req/s, burst 10 for task verification (file operations)
	setEndpointRateLimiter("/api/v1/tasks/list", 10, 20)            // 10 req/s, burst 20 for task listing
	setEndpointRateLimiter("/api/v1/tasks/dependencies", 5, 10)     // 5 req/s, burst 10 for dependency detection (file operations)
	setEndpointRateLimiter("/api/v1/change-requests", 20, 40)       // 20 req/s, burst 40 for change requests

	// Setup router
	r := chi.NewRouter()

	// Add panic recovery middleware to all routes
	r.Use(func(next http.Handler) http.Handler {
		return http.HandlerFunc(func(w http.ResponseWriter, req *http.Request) {
			defer RecoverFromPanic(w, req)
			next.ServeHTTP(w, req)
		})
	})

	// Security headers middleware
	r.Use(securityHeadersMiddleware)

	// Request ID middleware (must be first)
	r.Use(requestIDMiddleware)

	// Security middleware (early in chain)
	r.Use(securityHeadersMiddleware)
	r.Use(requestSizeLimitMiddleware(DefaultMaxRequestSize))
	// r.Use(csrfProtectionMiddleware) // Temporarily disabled

	// Rate limiting middleware
	r.Use(rateLimitByEndpointMiddleware())

	// Middleware
	r.Use(middleware.Logger)
	r.Use(middleware.Recoverer)
	r.Use(middleware.RealIP)
	r.Use(apiVersionMiddleware)
	r.Use(rateLimitMiddleware)
	r.Use(cacheMiddleware)
	r.Use(cors.Handler(cors.Options{
		AllowedOrigins:   []string{config.CORSOrigin},
		AllowedMethods:   []string{"GET", "POST", "PUT", "DELETE", "OPTIONS"},
		AllowedHeaders:   []string{"Accept", "Authorization", "Content-Type", "X-CSRF-Token", "X-API-Key"},
		ExposedHeaders:   []string{"Link", "Retry-After"},
		AllowCredentials: true,
		MaxAge:           300,
	}))

	// Serve dashboard static files
	r.Get("/dashboard/*", func(w http.ResponseWriter, r *http.Request) {
		http.StripPrefix("/dashboard/", http.FileServer(http.Dir("./dashboard/"))).ServeHTTP(w, r)
	})
	r.Get("/", func(w http.ResponseWriter, r *http.Request) {
		http.Redirect(w, r, "/dashboard/", http.StatusFound)
	})

	// Public routes - Health checks (Phase G: Logging and Monitoring)
	r.Get("/health", healthHandler)
	r.Get("/health/db", healthDBHandler)
	r.Get("/health/ready", healthReadyHandler)

	// Admin routes (protected with admin authentication)
	r.Route("/api/v1/admin", func(r chi.Router) {
		r.Use(adminAuthMiddleware)
		r.Post("/organizations", createOrganizationHandler)
		r.Post("/projects", createProjectHandler)
		r.Post("/teams", createTeamHandler)
		r.Get("/teams", getTeamsHandler)
		r.Post("/teams/{teamId}/members", addTeamMemberHandler)
		r.Get("/teams/{teamId}/members", getTeamMembersHandler)
		r.Post("/agents/register", registerAgentHandler)
		r.Get("/agents", getRegisteredAgentsHandler)
		r.Post("/patterns", createOrgPatternHandler)
		r.Get("/patterns", getOrgPatternsHandler)
		r.Post("/patterns/{patternId}/distribute", distributePatternHandler)
		r.Get("/pattern-distributions", getPatternDistributionsHandler)
		r.Post("/alerts", createAlertHandler)
		r.Get("/alerts", getAlertsHandler)
		r.Post("/alerts/{alertId}/read", markAlertReadHandler)
		r.Post("/alerts/{alertId}/resolve", resolveAlertHandler)
		r.Post("/alert-rules", createAlertRuleHandler)
		r.Get("/alert-rules", getAlertRulesHandler)
		r.Post("/binary/upload", uploadBinaryHandler)
	})

	// Protected routes (require API key)
	r.Route("/api/v1", func(r chi.Router) {
		r.Use(apiKeyAuthMiddleware)

		// Document endpoints
		r.Post("/documents/ingest", uploadDocumentHandler(config))
		r.Get("/documents/{id}/status", getDocumentStatusHandler)
		r.Get("/documents/{id}/extracted", getExtractedTextHandler)
		r.Get("/documents/{id}/knowledge", getKnowledgeItemsHandler)
		r.Post("/documents/{id}/detect-changes", detectChangesHandler)
		r.Get("/documents", listDocumentsHandler)
		r.Get("/documents/search", searchDocumentsHandler)

		// Knowledge management endpoints
		r.Put("/knowledge/{id}/status", updateKnowledgeStatusHandler)
		r.Get("/projects/knowledge", listProjectKnowledgeHandler)
		r.Get("/knowledge/business", getBusinessContextHandler) // Phase A: Business context for MCP tools
		r.Post("/knowledge/sync", syncKnowledgeHandler)
		r.With(rateLimitMiddleware).Post("/knowledge/gap-analysis", gapAnalysisHandler) // Phase 12
		r.With(rateLimitMiddleware).Post("/knowledge/migrate", migrateKnowledgeHandler) // Phase 13

		// Phase 12: Change Request endpoints
		r.Get("/change-requests", listChangeRequestsHandler)
		r.Get("/change-requests/{id}", getChangeRequestHandler)
		r.Post("/change-requests/{id}/approve", approveChangeRequestHandler)
		r.Post("/change-requests/{id}/reject", rejectChangeRequestHandler)
		r.Post("/change-requests/{id}/impact", analyzeImpactHandler)
		r.Post("/change-requests/{id}/start", startImplementationHandler)
		r.Post("/change-requests/{id}/complete", completeImplementationHandler)
		r.Post("/change-requests/{id}/update", updateImplementationHandler)
		r.Get("/change-requests/dashboard", getChangeRequestsDashboardHandler)

		// Telemetry endpoints
		r.Post("/telemetry", telemetryIngestionHandler)
		r.Get("/telemetry/recent", getRecentTelemetryHandler)
		r.Get("/metrics", getMetricsHandler)
		r.Get("/metrics/trends", getMetricsTrendsHandler)
		r.Get("/metrics/team/{teamId}", getTeamMetricsHandler)

		// Prometheus metrics endpoint (Phase G: Logging and Monitoring)
		r.Get("/metrics/prometheus", prometheusMetricsHandler)

		// AST Analysis endpoints (Phase 6) ✅ IMPLEMENTED - Full Tree-sitter AST analysis
		r.Post("/analyze/ast", astAnalysisHandler)
		r.Post("/analyze/vibe", vibeAnalysisHandler)
		r.Post("/analyze/cross-file", crossFileAnalysisHandler)
		r.Post("/analyze/complexity", complexityAnalysisHandler)
		r.Post("/analyze/dead-code", deadCodeAnalysisHandler)
		r.Post("/analyze/dependencies", dependencyAnalysisHandler)
		r.Post("/analyze/type-safety", typeSafetyAnalysisHandler)
		r.Post("/analyze/performance", performanceAnalysisHandler)
		r.Post("/format/code", formatCodeHandler)
		r.Post("/lint/code", lintCodeHandler)
		r.Post("/refactor/code", refactorCodeHandler)
		r.Post("/generate/docs", generateDocsHandler)
		r.Post("/analyze/coverage", analyzeCoverageHandler)
		r.Post("/analyze/cross-file", analyzeCrossFileHandler)

		// Security Analysis endpoint (Phase 8) ✅ IMPLEMENTED - Full security rule checking with AST analysis
		r.Post("/analyze/security", securityAnalysisHandler)
		r.Get("/security/context", getSecurityContextHandler) // Phase A: Security context for MCP tools

		// Validation endpoints (Phase B)
		r.Post("/validate/code", validateCodeHandler)         // Phase B: Code validation
		r.Post("/validate/business", validateBusinessHandler) // Phase B: Business rule validation

		// Action endpoints (Phase C)
		r.Post("/fixes/apply", applyFixHandler) // Phase C: Apply fixes

		// Architecture Analysis endpoint (Phase 9) ⏳ IMPLEMENTED - File structure analysis and split suggestions
		r.Post("/analyze/architecture", architectureAnalysisHandler)

		// Doc-Sync endpoint (Phase 11) - Code-Documentation Comparison
		r.Post("/analyze/doc-sync", docSyncHandler)
		r.Post("/analyze/business-rules", businessRulesComparisonHandler)
		r.Get("/doc-sync/review-queue", reviewQueueHandler)
		r.Post("/doc-sync/review/{id}", reviewHandler)

		// Test Enforcement endpoints (Phase 10)
		r.Post("/test-requirements/generate", generateTestRequirementsHandler)
		r.Post("/test-coverage/analyze", analyzeCoverageHandler)
		r.Get("/test-coverage/{knowledge_item_id}", getCoverageHandler)
		r.Post("/test-validations/validate", validateTestsHandler)
		r.Get("/test-validations/{test_requirement_id}", getValidationHandler)
		r.Post("/test-execution/run", testExecutionHandler)
		r.Get("/test-execution/{execution_id}", getTestExecutionHandler)
		r.Post("/mutation-test/run", mutationTestHandler)
		r.Get("/mutation-test/{test_requirement_id}", getMutationResultHandler)

		// Hook endpoints (Phase 9.5)
		r.Post("/api/v1/telemetry/hook", hookTelemetryHandler)
		r.Get("/api/v1/hooks/metrics", hookMetricsHandler)
		r.Get("/api/v1/hooks/metrics/team", hookMetricsHandler) // Same handler, different path
		r.Get("/api/v1/hooks/policies", hookPoliciesHandler)
		r.Post("/api/v1/hooks/policies", createOrUpdateHookPolicyHandler)
		r.Get("/api/v1/hooks/limits", hookLimitsHandler)
		r.Post("/api/v1/hooks/baselines", hookBaselineHandler)
		r.Post("/api/v1/hooks/baselines/{id}/review", reviewHookBaselineHandler)

		// Phase 14A: Comprehensive Feature Analysis endpoints
		r.With(rateLimitMiddleware).Post("/analyze/comprehensive", comprehensiveAnalysisHandler)
		r.Get("/validations/{id}", getComprehensiveValidationHandler)
		r.Get("/validations", listValidationsHandler)

		// Phase 15: Intent & Simple Language endpoints
		r.With(rateLimitMiddleware).Post("/analyze/intent", intentAnalysisHandler)
		r.Post("/intent/decisions", recordIntentDecisionHandler)
		r.Get("/intent/patterns", getIntentPatternsHandler)

		// Phase 14C: LLM Configuration endpoints
		r.With(rateLimitMiddleware).Post("/api/v1/llm/config", createLLMConfigHandler)
		r.With(rateLimitMiddleware).Get("/api/v1/llm/config/{id}", getLLMConfigHandler)
		r.With(rateLimitMiddleware).Put("/api/v1/llm/config/{id}", updateLLMConfigHandler)
		r.With(rateLimitMiddleware).Delete("/api/v1/llm/config/{id}", deleteLLMConfigHandler)
		r.With(rateLimitMiddleware).Get("/api/v1/llm/config/project/{projectId}", listLLMConfigsHandler)

		// Phase 14C: LLM Metadata endpoints
		r.Get("/api/v1/llm/providers", getProvidersHandler)
		r.Get("/api/v1/llm/models/{provider}", getModelsHandler)

		// Phase 14C: LLM Validation endpoint
		r.With(rateLimitMiddleware).Post("/api/v1/llm/config/validate", validateLLMConfigHandler)

		// Phase 14C: LLM Usage Reporting endpoints
		r.With(rateLimitMiddleware).Get("/api/v1/llm/usage/report", getUsageReportHandler)
		r.With(rateLimitMiddleware).Get("/api/v1/llm/usage/stats", getUsageStatsHandler)
		r.With(rateLimitMiddleware).Get("/api/v1/llm/usage/cost-breakdown", getCostBreakdownHandler)
		r.With(rateLimitMiddleware).Get("/api/v1/llm/usage/trends", getUsageTrendsHandler)

		// Phase 14D: Cost Optimization Metrics endpoints
		r.With(rateLimitMiddleware).Get("/api/v1/metrics/cache", getCacheMetricsHandler)
		r.With(rateLimitMiddleware).Get("/api/v1/metrics/cost", getCostMetricsHandler)

		// Binary distribution endpoints
		r.Get("/binary/version", getBinaryVersionHandler)
		r.Get("/binary/download", downloadBinaryHandler)
		r.Get("/binary/versions", listBinaryVersionsHandler)

		// Rules endpoints
		r.Get("/rules/latest", getLatestRulesHandler)

		// Phase 14E: Task Dependency & Verification endpoints
		r.Post("/tasks", createTaskHandler)
		r.Get("/tasks", listTasksHandler)
		r.Get("/tasks/{id}", getTaskHandler)
		r.Put("/tasks/{id}", updateTaskHandler)
		r.Delete("/tasks/{id}", deleteTaskHandler)
		r.Post("/tasks/scan", scanTasksHandler)
		r.Post("/tasks/{id}/verify", verifyTaskHandler)
		r.Post("/tasks/verify-all", verifyAllTasksHandler)
		r.Get("/tasks/{id}/dependencies", getTaskDependenciesHandler)
		r.Get("/tasks/dependencies/graph", getTaskDependencyGraphHandler)
		r.Get("/tasks/execution-plan", getTaskExecutionPlanHandler)
		r.Post("/tasks/{id}/dependencies", createTaskDependencyHandler)
		r.Delete("/tasks/{id}/dependencies/{depId}", deleteTaskDependencyHandler)
		r.Get("/tasks/{id}/impact", getTaskImpactAnalysisHandler)
		r.Post("/tasks/{id}/impact/analyze", analyzeTaskImpactHandler)
		r.Get("/tasks/{id}/changes", getTaskChangeHistoryHandler)
		r.Get("/tasks/changes", getAllTaskChangesHandler)
		r.Get("/tasks/impact/report", getImpactReportHandler)
		r.Post("/tasks/{id}/detect-dependencies", detectDependenciesHandler)

		// Phase 4: Cross-Repository Analysis endpoints
		r.Post("/repositories", createRepositoryHandler)
		r.Get("/repositories", listRepositoriesHandler)
		r.Get("/repositories/{id}", getRepositoryHandler)
		r.Put("/repositories/{id}", updateRepositoryHandler)
		r.Delete("/repositories/{id}", deleteRepositoryHandler)
		r.Post("/repositories/{id}/sync", syncRepositoryHandler)
		r.Get("/repositories/{id}/analysis", getRepositoryAnalysisHandler)
		r.Post("/repositories/{id}/analyze", analyzeRepositoryHandler)

		// Repository relationships and dependencies
		r.Post("/repositories/relationships", createRepositoryRelationshipHandler)
		r.Get("/repositories/network", getRepositoryNetworkHandler)
		r.Get("/repositories/{id}/impact", getCrossRepoImpactAnalysisHandler)
		r.Post("/repositories/{id}/impact/analyze", analyzeCrossRepoImpactHandler)

		// Phase 5: API Completeness - System endpoints
		r.Get("/versions", getAPIVersionsHandler)
		r.Get("/docs", getAPIDocumentationHandler)
		r.Get("/health", healthCheckHandler)
		r.Get("/stats", getAPIStatsHandler)
		r.Get("/rate-limit", getRateLimitStatusHandler)

		// Phase 5: MCP Error Classification & Monitoring
		r.Get("/errors/dashboard", getErrorDashboardHandler)
		r.Get("/errors/analysis", getErrorAnalysisHandler)
		r.Get("/errors/classify", classifyErrorHandler)
		r.Get("/errors/stats", getErrorStatsHandler)
		r.Post("/errors/report", reportErrorHandler)

		// Phase 5: MCP Graceful Degradation & Circuit Breakers
		r.Get("/degradation/status", getDegradationStatusHandler)
		r.Get("/circuit-breakers", getCircuitBreakersHandler)
		r.Get("/circuit-breakers/{tool}", getCircuitBreakerHandler)
		r.Post("/circuit-breakers/{tool}/reset", resetCircuitBreakerHandler)
		r.Get("/fallback-strategies", getFallbackStrategiesHandler)
		r.Get("/fallback-strategies/{tool}", getFallbackStrategyHandler)

		// Phase 5: MCP Workflow Orchestration
		r.Get("/workflows", getWorkflowsHandler)
		r.Get("/workflows/{id}", getWorkflowHandler)
		r.Post("/workflows", createWorkflowHandler)
		r.Delete("/workflows/{id}", deleteWorkflowHandler)
		r.Post("/workflows/{id}/execute", executeWorkflowHandler)
		r.Get("/executions", getWorkflowExecutionsHandler)
		r.Get("/executions/{id}", getWorkflowExecutionHandler)
		r.Post("/executions/{id}/cancel", cancelWorkflowExecutionHandler)

		// Phase 4.2: MCP Performance Monitoring & Optimization
		r.Get("/performance/dashboard", getPerformanceDashboardHandler)
		r.Get("/performance/tools", getToolPerformanceMetricsHandler)
		r.Get("/performance/tools/{tool}", getToolPerformanceMetricsHandler)
		r.Get("/performance/workflows", getWorkflowPerformanceMetricsHandler)
		r.Get("/performance/workflows/{workflow}", getWorkflowPerformanceMetricsHandler)
		r.Get("/performance/recommendations", getPerformanceRecommendationsHandler)
		r.Get("/performance/bottlenecks", getPerformanceBottlenecksHandler)
		r.Get("/performance/system", getSystemPerformanceSnapshotHandler)
		r.Get("/performance/analytics", getPerformanceAnalyticsHandler)

		// Phase 5: API Versioning and Deprecation Management
		r.Post("/admin/versions", createAPIVersionHandler)
		r.Get("/admin/versions", listAPIVersionsHandler)
		r.Get("/admin/versions/{version}", getAPIVersionHandler)
		r.Put("/admin/versions/{version}", updateAPIVersionHandler)
		r.Post("/admin/endpoints", createAPIEndpointHandler)
		r.Get("/compatibility", getVersionCompatibilityHandler)
		r.Post("/admin/migrations", createVersionMigrationHandler)
		r.Get("/repositories/relationships", listRepositoryRelationshipsHandler)
		r.Get("/repositories/{id}/relationships", getRepositoryRelationshipsHandler)
		r.Delete("/repositories/relationships/{id}", deleteRepositoryRelationshipHandler)

		r.Post("/repositories/dependencies", createCrossRepoDependencyHandler)
		r.Get("/repositories/dependencies", listCrossRepoDependenciesHandler)
		r.Get("/repositories/{id}/dependencies", getRepositoryDependenciesHandler)
		r.Delete("/repositories/dependencies/{id}", deleteCrossRepoDependencyHandler)

		// Cross-repository analysis
		r.Post("/repositories/analysis/cross-repo", createCrossRepoAnalysisHandler)
		r.Get("/repositories/analysis/cross-repo", listCrossRepoAnalysesHandler)
		r.Get("/repositories/analysis/cross-repo/{id}", getCrossRepoAnalysisHandler)
		r.Get("/repositories/analysis/cross-repo/{id}/status", getCrossRepoAnalysisStatusHandler)
		r.Post("/repositories/analysis/cross-repo/{id}/cancel", cancelCrossRepoAnalysisHandler)

		// Repository network and impact analysis
		r.Get("/repositories/network", getRepositoryNetworkHandler)
		r.Post("/repositories/{id}/impact", analyzeRepositoryImpactHandler)
		r.Get("/repositories/{id}/impact", getRepositoryImpactHandler)
		r.Get("/repositories/network/centrality", getRepositoryCentralityHandler)
		r.Get("/repositories/network/clusters", getRepositoryClustersHandler)
	})

	// Phase 14D: Start cache cleanup goroutine
	startCacheCleanup()
	LogInfo(context.Background(), "Cache cleanup goroutine started")

	// Phase 14E: Start task cache cleanup goroutine
	StartTaskCacheCleanup()
	LogInfo(context.Background(), "Task cache cleanup goroutine started")

	// Start server
	server := &http.Server{
		Addr:         ":" + config.Port,
		Handler:      r,
		ReadTimeout:  GetConfig().Timeouts.HTTP,
		WriteTimeout: GetConfig().Timeouts.HTTP,
		IdleTimeout:  120 * time.Second,
	}

	// Graceful shutdown
	go func() {
		sigChan := make(chan os.Signal, 1)
		signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
		<-sigChan

		log.Println("Shutting down server...")
		ctx, cancel := context.WithTimeout(context.Background(), GetConfig().Timeouts.Context)
		defer cancel()
		server.Shutdown(ctx)
	}()

	log.Printf("🚀 Sentinel Hub API starting on port %s", config.Port)
	if err := server.ListenAndServe(); err != http.ErrServerClosed {
		log.Fatalf("Server error: %v", err)
	}
}

// =============================================================================
// Phase 14A: Comprehensive Feature Analysis Handlers
// =============================================================================

// comprehensiveAnalysisHandler handles POST /api/v1/analyze/comprehensive
func comprehensiveAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	// Use configurable timeout (defaults to 60s, can be overridden via env var)
	ctx, cancel := context.WithTimeout(r.Context(), getAnalysisTimeout())
	defer cancel()

	project, err := getProjectFromContext(ctx)
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Feature                string              `json:"feature"`
		Mode                   string              `json:"mode"`            // "auto" or "manual"
		Files                  map[string][]string `json:"files,omitempty"` // For manual mode
		CodebasePath           string              `json:"codebasePath"`
		Depth                  string              `json:"depth"` // "surface", "medium", "deep"
		IncludeBusinessContext bool                `json:"includeBusinessContext"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		LogError(ctx, "Failed to decode request: %v", err)
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.Feature == "" {
		http.Error(w, "feature is required", http.StatusBadRequest)
		return
	}
	if req.CodebasePath == "" && req.Mode == "auto" {
		http.Error(w, "codebasePath is required for auto mode", http.StatusBadRequest)
		return
	}
	if req.Mode != "auto" && req.Mode != "manual" {
		http.Error(w, "mode must be 'auto' or 'manual'", http.StatusBadRequest)
		return
	}
	if req.Depth == "" {
		req.Depth = "medium" // Default depth
	}
	if req.Depth != "surface" && req.Depth != "medium" && req.Depth != "deep" {
		http.Error(w, "depth must be 'surface', 'medium', or 'deep'", http.StatusBadRequest)
		return
	}

	// Calculate timeout based on depth
	var timeout time.Duration
	config := GetConfig()
	switch req.Depth {
	case "surface":
		timeout = config.Timeouts.HTTP
	case "medium":
		timeout = config.Timeouts.Analysis
	case "deep":
		timeout = 3 * config.Timeouts.Analysis // 3x for deep analysis
	default:
		timeout = config.Timeouts.Analysis // Fallback to default
	}

	// Override context timeout with depth-based timeout
	ctx, cancel = context.WithTimeout(ctx, timeout)
	defer cancel()

	// Validate codebasePath exists (for auto mode)
	if req.Mode == "auto" {
		if _, err := os.Stat(req.CodebasePath); os.IsNotExist(err) {
			http.Error(w, fmt.Sprintf("codebasePath does not exist: %s", req.CodebasePath), http.StatusBadRequest)
			return
		}
	}

	// Validate manual files exist
	if req.Mode == "manual" && req.Files != nil {
		for layer, files := range req.Files {
			for _, filePath := range files {
				fullPath := filepath.Join(req.CodebasePath, filePath)
				if _, err := os.Stat(fullPath); os.IsNotExist(err) {
					http.Error(w, fmt.Sprintf("File does not exist: %s (layer: %s)", fullPath, layer), http.StatusBadRequest)
					return
				}
			}
		}
	}

	analysisStart := time.Now()

	// Phase 14D: Surface depth - Skip LLM calls, use AST/patterns only
	if req.Depth == "surface" {
		LogInfo(ctx, "Surface depth: Skipping LLM calls, using AST/patterns only")
		// Continue with AST/pattern-based analysis only (no LLM)
		// Business context analysis will be skipped (see below)
	}

	// Phase 14D: Check cache for comprehensive analysis result
	llmConfig, err := getLLMConfig(ctx, project.ID)
	if err == nil && llmConfig != nil {
		featureHash := generateFeatureHash(req.Feature, req.CodebasePath)
		if cachedResult, ok := getCachedAnalysisResult(project.ID, featureHash, req.Depth, req.Mode, llmConfig); ok {
			LogInfo(ctx, "Returning cached comprehensive analysis result")
			w.Header().Set("Content-Type", "application/json")
			response := map[string]interface{}{
				"success":       true,
				"validation_id": cachedResult.ValidationID,
				"hub_url":       cachedResult.HubURL,
				"report":        cachedResult,
				"cached":        true,
			}
			json.NewEncoder(w).Encode(response)
			return
		}
	}

	// Discover feature
	feature, err := discoverFeature(ctx, req.Feature, req.CodebasePath, req.Files)
	if err != nil {
		LogError(ctx, "Feature discovery failed: %v", err)
		http.Error(w, fmt.Sprintf("Feature discovery failed: %v", err), http.StatusInternalServerError)
		return
	}

	// Run analyses in parallel for better performance
	type analysisResult struct {
		businessFindings    []BusinessContextFinding
		apiFindings         []APILayerFinding
		testFindings        []TestLayerFinding
		logicFindings       []LogicLayerFinding
		uiFindings          []UILayerFinding
		dbFindings          []DatabaseLayerFinding
		integrationFindings []IntegrationLayerFinding
		errors              []string
		criticalErrors      []string // Track critical errors separately
	}

	result := analysisResult{}

	// Use goroutines for parallel execution
	var wg sync.WaitGroup
	var mu sync.Mutex

	// Analyze business context if requested
	// Phase 14D: Skip business context LLM extraction for surface depth
	if req.IncludeBusinessContext && req.Depth != "surface" {
		wg.Add(1)
		go func() {
			defer wg.Done()

			// Check context cancellation
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Business context analysis cancelled: %v", ctx.Err())
				return
			default:
			}

			// Phase 14D: Get codebaseHash and LLM config for caching
			featureHash := generateFeatureHash(req.Feature, req.CodebasePath)
			llmConfig, _ := getLLMConfig(ctx, project.ID)

			businessFindings, err := analyzeBusinessContext(ctx, project.ID, feature, featureHash, llmConfig)
			if err != nil {
				LogWarn(ctx, "Business context analysis failed: %v", err)
				// Check context again before writing error
				select {
				case <-ctx.Done():
					LogWarn(ctx, "Cancelled before writing error: %v", ctx.Err())
					return
				default:
				}
				mu.Lock()
				// Classify error severity
				if isCriticalError(err) {
					result.criticalErrors = append(result.criticalErrors, fmt.Sprintf("Business context: %v", err))
				} else {
					result.errors = append(result.errors, fmt.Sprintf("Business context: %v", err))
				}
				mu.Unlock()
				return
			}

			// Check context cancellation before journey adherence
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Journey adherence check cancelled: %v", ctx.Err())
				mu.Lock()
				result.businessFindings = businessFindings
				mu.Unlock()
				return
			default:
			}

			// Check journey adherence
			// Phase 14D: Pass codebaseHash and config for caching
			journeyFindings, err := checkJourneyAdherence(ctx, project.ID, feature, featureHash, llmConfig)
			if err == nil {
				businessFindings = append(businessFindings, journeyFindings...)
			}

			// Check context before writing results
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Cancelled before writing results: %v", ctx.Err())
				return
			default:
			}

			mu.Lock()
			result.businessFindings = businessFindings
			mu.Unlock()
		}()
	}

	// Analyze API layer
	wg.Add(1)
	go func() {
		defer wg.Done()

		// Check context cancellation
		select {
		case <-ctx.Done():
			LogWarn(ctx, "API layer analysis cancelled: %v", ctx.Err())
			return
		default:
		}

		apiFindings, err := analyzeAPILayer(ctx, feature)
		if err != nil {
			LogWarn(ctx, "API layer analysis failed: %v", err)
			apiFindings = []APILayerFinding{}
			// Check context again before writing error
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Cancelled before writing error: %v", ctx.Err())
				return
			default:
			}
			mu.Lock()
			// Classify error severity
			if isCriticalError(err) {
				result.criticalErrors = append(result.criticalErrors, fmt.Sprintf("API layer: %v", err))
			} else {
				result.errors = append(result.errors, fmt.Sprintf("API layer: %v", err))
			}
			mu.Unlock()
		}
		// Check context before writing results
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Cancelled before writing results: %v", ctx.Err())
			return
		default:
		}
		mu.Lock()
		result.apiFindings = apiFindings
		mu.Unlock()
	}()

	// Analyze test layer
	wg.Add(1)
	go func() {
		defer wg.Done()

		// Check context cancellation
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Test layer analysis cancelled: %v", ctx.Err())
			return
		default:
		}

		testFindings, err := analyzeTestLayer(ctx, feature)
		if err != nil {
			LogWarn(ctx, "Test layer analysis failed: %v", err)
			testFindings = []TestLayerFinding{}
			// Check context again before writing error
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Cancelled before writing error: %v", ctx.Err())
				return
			default:
			}
			mu.Lock()
			// Classify error severity
			if isCriticalError(err) {
				result.criticalErrors = append(result.criticalErrors, fmt.Sprintf("Test layer: %v", err))
			} else {
				result.errors = append(result.errors, fmt.Sprintf("Test layer: %v", err))
			}
			mu.Unlock()
		}
		// Check context before writing results
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Cancelled before writing results: %v", ctx.Err())
			return
		default:
		}
		mu.Lock()
		result.testFindings = testFindings
		mu.Unlock()
	}()

	// Analyze business logic layer
	wg.Add(1)
	go func() {
		defer wg.Done()

		// Check context cancellation
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Business logic analysis cancelled: %v", ctx.Err())
			return
		default:
		}

		// Phase 14D: Pass depth to skip LLM for surface depth
		logicFindings, err := analyzeBusinessLogicWithDepth(ctx, project.ID, feature, req.Depth)
		if err != nil {
			LogWarn(ctx, "Business logic analysis failed: %v", err)
			logicFindings = []LogicLayerFinding{}
			// Check context again before writing error
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Cancelled before writing error: %v", ctx.Err())
				return
			default:
			}
			mu.Lock()
			// Classify error severity
			if isCriticalError(err) {
				result.criticalErrors = append(result.criticalErrors, fmt.Sprintf("Business logic: %v", err))
			} else {
				result.errors = append(result.errors, fmt.Sprintf("Business logic: %v", err))
			}
			mu.Unlock()
		}
		// Check context before writing results
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Cancelled before writing results: %v", ctx.Err())
			return
		default:
		}
		mu.Lock()
		result.logicFindings = logicFindings
		mu.Unlock()
	}()

	// Analyze UI layer
	wg.Add(1)
	go func() {
		defer wg.Done()

		// Check context cancellation
		select {
		case <-ctx.Done():
			LogWarn(ctx, "UI layer analysis cancelled: %v", ctx.Err())
			return
		default:
		}

		uiFindings, err := analyzeUILayer(ctx, feature)
		if err != nil {
			LogWarn(ctx, "UI layer analysis failed: %v", err)
			uiFindings = []UILayerFinding{}
			// Check context again before writing error
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Cancelled before writing error: %v", ctx.Err())
				return
			default:
			}
			mu.Lock()
			// Classify error severity
			if isCriticalError(err) {
				result.criticalErrors = append(result.criticalErrors, fmt.Sprintf("UI layer: %v", err))
			} else {
				result.errors = append(result.errors, fmt.Sprintf("UI layer: %v", err))
			}
			mu.Unlock()
		}
		// Check context before writing results
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Cancelled before writing results: %v", ctx.Err())
			return
		default:
		}
		mu.Lock()
		result.uiFindings = uiFindings
		mu.Unlock()
	}()

	// Analyze database layer
	wg.Add(1)
	go func() {
		defer wg.Done()

		// Check context cancellation
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Database layer analysis cancelled: %v", ctx.Err())
			return
		default:
		}

		dbFindings, err := analyzeDatabaseLayer(ctx, feature)
		if err != nil {
			LogWarn(ctx, "Database layer analysis failed: %v", err)
			dbFindings = []DatabaseLayerFinding{}
			// Check context again before writing error
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Cancelled before writing error: %v", ctx.Err())
				return
			default:
			}
			mu.Lock()
			// Classify error severity
			if isCriticalError(err) {
				result.criticalErrors = append(result.criticalErrors, fmt.Sprintf("Database layer: %v", err))
			} else {
				result.errors = append(result.errors, fmt.Sprintf("Database layer: %v", err))
			}
			mu.Unlock()
		}
		// Check context before writing results
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Cancelled before writing results: %v", ctx.Err())
			return
		default:
		}
		mu.Lock()
		result.dbFindings = dbFindings
		mu.Unlock()
	}()

	// Analyze integration layer
	wg.Add(1)
	go func() {
		defer wg.Done()

		// Check context cancellation
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Integration layer analysis cancelled: %v", ctx.Err())
			return
		default:
		}

		integrationFindings, err := analyzeIntegrationLayer(ctx, feature)
		if err != nil {
			LogWarn(ctx, "Integration layer analysis failed: %v", err)
			integrationFindings = []IntegrationLayerFinding{}
			// Check context again before writing error
			select {
			case <-ctx.Done():
				LogWarn(ctx, "Cancelled before writing error: %v", ctx.Err())
				return
			default:
			}
			mu.Lock()
			// Classify error severity
			if isCriticalError(err) {
				result.criticalErrors = append(result.criticalErrors, fmt.Sprintf("Integration layer: %v", err))
			} else {
				result.errors = append(result.errors, fmt.Sprintf("Integration layer: %v", err))
			}
			mu.Unlock()
		}
		// Check context before writing results
		select {
		case <-ctx.Done():
			LogWarn(ctx, "Cancelled before writing results: %v", ctx.Err())
			return
		default:
		}
		mu.Lock()
		result.integrationFindings = integrationFindings
		mu.Unlock()
	}()

	// Wait for all analyses to complete
	wg.Wait()

	// Check for critical errors
	if len(result.criticalErrors) > 0 {
		LogError(ctx, "Critical errors in comprehensive analysis: %v", result.criticalErrors)
		// Continue with partial analysis rather than failing completely
	}

	// Extract results
	businessFindings := result.businessFindings
	apiFindings := result.apiFindings
	testFindings := result.testFindings
	logicFindings := result.logicFindings
	uiFindings := result.uiFindings
	dbFindings := result.dbFindings
	integrationFindings := result.integrationFindings

	// Generate checklist from all findings (pass correct types directly)
	checklist := generateChecklist(
		businessFindings,
		uiFindings,
		apiFindings,
		dbFindings,
		logicFindings,
		integrationFindings,
		testFindings,
	)

	// Verify end-to-end flows
	flows, err := verifyEndToEndFlows(ctx, feature)
	if err != nil {
		LogWarn(ctx, "Flow verification failed: %v", err)
		flows = []Flow{}
	}

	// Verify integration points
	integrationBreakpoints, err := verifyIntegrationPoints(ctx, flows, feature)
	if err == nil {
		// Add integration breakpoints to flows
		for i := range flows {
			flows[i].Breakpoints = append(flows[i].Breakpoints, integrationBreakpoints...)
		}
	}

	// Convert flows to interface{} for storage
	flowsInterface := make([]interface{}, len(flows))
	for i, f := range flows {
		flowsInterface[i] = f
	}

	// Build layer analysis map
	layerAnalysis := map[string]interface{}{
		"business":    businessFindings,
		"ui":          uiFindings,
		"api":         apiFindings,
		"database":    dbFindings,
		"logic":       logicFindings,
		"integration": integrationFindings,
		"test":        testFindings,
	}

	// Generate summary
	analysisTime := time.Since(analysisStart)
	flowsVerified := len(flows)
	flowsBroken := 0
	for _, flow := range flows {
		if flow.Status == "broken" {
			flowsBroken++
		}
	}
	summary := generateSummary(checklist, flowsVerified, flowsBroken, analysisTime)

	// Format report
	hubConfig := loadConfig()
	report, err := formatReport(
		ctx,
		project.ID,
		req.Feature,
		req.Mode,
		req.Depth,
		checklist,
		summary,
		layerAnalysis,
		flowsInterface,
		hubConfig.HubURL,
	)
	if err != nil {
		LogError(ctx, "Failed to format report: %v", err)
		http.Error(w, "Failed to generate report", http.StatusInternalServerError)
		return
	}

	// Store report
	if err := storeComprehensiveValidation(ctx, report, project.ID); err != nil {
		LogWarn(ctx, "Failed to store validation: %v", err)
		// Continue and return report anyway
	} else {
		// Update LLM usage records with validation ID
		if err := updateLLMUsageValidationID(ctx, report.ValidationID, project.ID); err != nil {
			LogWarn(ctx, "Failed to update LLM usage validation ID: %v", err)
		}
	}

	// Phase 14D: Cache the analysis result
	if llmConfig != nil {
		featureHash := generateFeatureHash(req.Feature, req.CodebasePath)
		setCachedAnalysisResult(project.ID, featureHash, req.Depth, req.Mode, report, llmConfig)
	}

	// Return response
	w.Header().Set("Content-Type", "application/json")
	response := map[string]interface{}{
		"success":       true,
		"validation_id": report.ValidationID,
		"hub_url":       report.HubURL,
		"report":        report,
	}
	// Add warnings if there were errors
	if len(result.errors) > 0 || len(result.criticalErrors) > 0 {
		response["warnings"] = result.errors
		if len(result.criticalErrors) > 0 {
			response["critical_errors"] = result.criticalErrors
			response["partial_analysis"] = true
		}
	}
	json.NewEncoder(w).Encode(response)
}

// getComprehensiveValidationHandler handles GET /api/v1/validations/{id}
func getComprehensiveValidationHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	validationID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	query := `
		SELECT validation_id, feature, mode, depth, findings, summary,
		       layer_analysis, end_to_end_flows, checklist, created_at, completed_at
		FROM comprehensive_validations
		WHERE validation_id = $1 AND project_id = $2
	`

	var valID, feature, mode, depth string
	var findingsJSON, summaryJSON, layerAnalysisJSON, flowsJSON, checklistJSON sql.NullString
	var createdAt time.Time
	var completedAt sql.NullTime

	err = queryRowWithTimeout(ctx, query, validationID, project.ID).Scan(
		&valID, &feature, &mode, &depth,
		&findingsJSON, &summaryJSON, &layerAnalysisJSON, &flowsJSON, &checklistJSON,
		&createdAt, &completedAt,
	)

	if err == sql.ErrNoRows {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "validation",
			ID:       validationID,
			Message:  "Validation not found",
		}, http.StatusNotFound)
		return
	}
	if err != nil {
		LogError(ctx, "Failed to query validation: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// Unmarshal JSONB fields
	var findings []ChecklistItem
	var summary AnalysisSummary
	var layerAnalysis map[string]interface{}
	var flows []interface{}
	var checklist []ChecklistItem

	if findingsJSON.Valid {
		unmarshalJSONB(findingsJSON.String, &findings)
	}
	if summaryJSON.Valid {
		unmarshalJSONB(summaryJSON.String, &summary)
	}
	if layerAnalysisJSON.Valid {
		unmarshalJSONB(layerAnalysisJSON.String, &layerAnalysis)
	}
	if flowsJSON.Valid {
		unmarshalJSONB(flowsJSON.String, &flows)
	}
	if checklistJSON.Valid {
		unmarshalJSONB(checklistJSON.String, &checklist)
	}

	report := ComprehensiveAnalysisReport{
		ValidationID:  valID,
		Feature:       feature,
		Mode:          mode,
		Depth:         depth,
		Summary:       &summary,
		Checklist:     checklist,
		LayerAnalysis: layerAnalysis,
		EndToEndFlows: flows,
		HubURL:        fmt.Sprintf("https://hub.example.com/validations/%s", valID),
		CreatedAt:     createdAt,
	}
	if completedAt.Valid {
		report.CompletedAt = &completedAt.Time
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(report)
}

// listValidationsHandler handles GET /api/v1/validations?project={id}
func listValidationsHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Parse query parameters
	limit := 20
	offset := 0
	if limitStr := r.URL.Query().Get("limit"); limitStr != "" {
		if l, err := strconv.Atoi(limitStr); err == nil && l > 0 && l <= 100 {
			limit = l
		}
	}
	if offsetStr := r.URL.Query().Get("offset"); offsetStr != "" {
		if o, err := strconv.Atoi(offsetStr); err == nil && o >= 0 {
			offset = o
		}
	}

	query := `
		SELECT validation_id, feature, mode, depth, summary, created_at, completed_at
		FROM comprehensive_validations
		WHERE project_id = $1
		ORDER BY created_at DESC
		LIMIT $2 OFFSET $3
	`

	rows, err := queryWithTimeout(ctx, query, project.ID, limit, offset)
	if err != nil {
		LogError(ctx, "Failed to query validations: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	validations := []map[string]interface{}{}
	for rows.Next() {
		var valID, feature, mode, depth string
		var summaryJSON sql.NullString
		var createdAt time.Time
		var completedAt sql.NullTime

		err := rows.Scan(&valID, &feature, &mode, &depth, &summaryJSON, &createdAt, &completedAt)
		if err != nil {
			LogWarn(ctx, "Failed to scan validation: %v", err)
			continue
		}

		var summary AnalysisSummary
		if summaryJSON.Valid {
			unmarshalJSONB(summaryJSON.String, &summary)
		}

		val := map[string]interface{}{
			"validation_id": valID,
			"feature":       feature,
			"mode":          mode,
			"depth":         depth,
			"summary":       summary,
			"hub_url":       fmt.Sprintf("https://hub.example.com/validations/%s", valID),
			"created_at":    createdAt,
		}
		if completedAt.Valid {
			val["completed_at"] = completedAt.Time
		}

		validations = append(validations, val)
	}

	// Get total count
	var total int
	countQuery := `SELECT COUNT(*) FROM comprehensive_validations WHERE project_id = $1`
	err = queryRowWithTimeout(ctx, countQuery, project.ID).Scan(&total)
	if err != nil {
		LogWarn(ctx, "Failed to count validations: %v", err)
		total = len(validations)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"validations":  validations,
		"total":        total,
		"limit":        limit,
		"offset":       offset,
		"has_next":     offset+limit < total,
		"has_previous": offset > 0,
	})
}

// =============================================================================
// Phase 15: Intent & Simple Language Handlers
// =============================================================================

// intentAnalysisHandler handles POST /api/v1/analyze/intent
func intentAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), GetConfig().Timeouts.Analysis)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req IntentAnalysisRequest
	if err = json.NewDecoder(r.Body).Decode(&req); err != nil {
		LogError(ctx, "Failed to decode intent analysis request: %v", err)
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.Prompt == "" {
		http.Error(w, "prompt is required", http.StatusBadRequest)
		return
	}

	// Gather context if requested
	var contextData *ContextData
	if req.IncludeContext {
		codebasePath := req.CodebasePath
		if codebasePath == "" {
			codebasePath = "." // Default to current directory
		}
		contextData, err = GatherContext(ctx, project.ID, codebasePath)
		if err != nil {
			LogWarn(ctx, "Failed to gather context: %v", err)
			// Continue without context
		}
	}

	// Analyze intent
	result, err := AnalyzeIntent(ctx, req.Prompt, contextData, project.ID)
	if err != nil {
		LogError(ctx, "Intent analysis failed: %v", err)
		http.Error(w, fmt.Sprintf("Intent analysis failed: %v", err), http.StatusInternalServerError)
		return
	}

	// Create decision record if clarification is needed
	if result.RequiresClarification {
		decision := &IntentDecision{
			ProjectID:          project.ID,
			OriginalPrompt:     req.Prompt,
			IntentType:         result.IntentType,
			ClarifyingQuestion: result.ClarifyingQuestion,
			UserChoice:         "", // Will be filled when user responds
			ResolvedPrompt:     "",
			ContextData:        nil,
		}

		err = RecordDecision(ctx, project.ID, decision)
		if err != nil {
			LogWarn(ctx, "Failed to create decision record: %v", err)
			// Continue without decision_id - non-fatal
		} else {
			result.DecisionID = decision.ID
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// recordIntentDecisionHandler handles POST /api/v1/intent/decisions
func recordIntentDecisionHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req IntentDecisionRequest
	if err = json.NewDecoder(r.Body).Decode(&req); err != nil {
		LogError(ctx, "Failed to decode decision request: %v", err)
		http.Error(w, "Invalid request body", http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.DecisionID == "" || req.UserChoice == "" {
		http.Error(w, "decision_id and user_choice are required", http.StatusBadRequest)
		return
	}

	// Retrieve original decision from database
	query := `
		SELECT original_prompt, intent_type, clarifying_question
		FROM intent_decisions
		WHERE id = $1 AND project_id = $2
	`

	var originalPrompt, intentTypeStr, clarifyingQuestion string
	err = queryRowWithTimeout(ctx, query, req.DecisionID, project.ID).Scan(
		&originalPrompt, &intentTypeStr, &clarifyingQuestion,
	)
	if err == sql.ErrNoRows {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "decision",
			ID:       req.DecisionID,
			Message:  "Decision not found",
		}, http.StatusNotFound)
		return
	}
	if err != nil {
		LogError(ctx, "Failed to query decision: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// Create decision object
	decision := &IntentDecision{
		ID:                 req.DecisionID,
		ProjectID:          project.ID,
		OriginalPrompt:     originalPrompt,
		IntentType:         IntentType(intentTypeStr),
		ClarifyingQuestion: clarifyingQuestion,
		UserChoice:         req.UserChoice,
		ResolvedPrompt:     req.ResolvedPrompt,
		ContextData:        req.AdditionalContext,
	}

	// Record decision
	err = RecordDecision(ctx, project.ID, decision)
	if err != nil {
		LogError(ctx, "Failed to record decision: %v", err)
		http.Error(w, fmt.Sprintf("Failed to record decision: %v", err), http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":     true,
		"decision_id": req.DecisionID, // Use original ID, not decision.ID (which may be new for inserts)
		"message":     "Decision recorded successfully",
	})
}

// getIntentPatternsHandler handles GET /api/v1/intent/patterns
func getIntentPatternsHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get query parameters
	patternType := r.URL.Query().Get("type")
	limitStr := r.URL.Query().Get("limit")
	limit := 50
	if limitStr != "" {
		if l, err := strconv.Atoi(limitStr); err == nil && l > 0 && l <= 100 {
			limit = l
		}
	}

	// Query patterns
	var patterns []IntentPattern

	if patternType != "" {
		// Filter by type
		query := `
			SELECT id, project_id, pattern_type, pattern_data, frequency, last_used, created_at
			FROM intent_patterns
			WHERE project_id = $1 AND pattern_type = $2
			ORDER BY frequency DESC, last_used DESC
			LIMIT $3
		`
		rows, err := queryWithTimeout(ctx, query, project.ID, patternType, limit)
		if err != nil {
			LogError(ctx, "Failed to query patterns: %v", err)
			WriteErrorResponse(w, &DatabaseError{
				Operation:     "database_query",
				Message:       "Database error",
				OriginalError: err,
			}, http.StatusInternalServerError)
			return
		}
		defer rows.Close()

		patterns, err = scanIntentPatterns(rows)
		if err != nil {
			LogError(ctx, "Failed to scan patterns: %v", err)
			WriteErrorResponse(w, &DatabaseError{
				Operation:     "database_query",
				Message:       "Database error",
				OriginalError: err,
			}, http.StatusInternalServerError)
			return
		}
	} else {
		// Get all patterns
		patterns, err = GetLearnedPatterns(ctx, project.ID)
		if err != nil {
			LogError(ctx, "Failed to get patterns: %v", err)
			http.Error(w, fmt.Sprintf("Failed to get patterns: %v", err), http.StatusInternalServerError)
			return
		}
	}

	// Limit results
	if len(patterns) > limit {
		patterns = patterns[:limit]
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":  true,
		"patterns": patterns,
		"count":    len(patterns),
	})
}

// =============================================================================
// Phase 14C: LLM Configuration Handlers
// =============================================================================

// createLLMConfigHandler handles POST /api/v1/llm/config
func createLLMConfigHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Provider         string                 `json:"provider"`
		APIKey           string                 `json:"api_key"`
		Model            string                 `json:"model"`
		KeyType          string                 `json:"key_type"`
		CostOptimization CostOptimizationConfig `json:"cost_optimization,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Validate input
	if req.Provider == "" || req.APIKey == "" || req.Model == "" {
		http.Error(w, "provider, api_key, and model are required", http.StatusBadRequest)
		return
	}

	// Validate provider
	if err := validateProvider(req.Provider); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	// Validate model
	if err := validateModel(req.Provider, req.Model); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	// Validate API key format
	if err := validateAPIKeyFormat(req.Provider, req.APIKey); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	// Validate cost optimization
	if err := validateCostOptimization(req.CostOptimization); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	if req.KeyType == "" {
		req.KeyType = "user-provided"
	}

	// Set default cost optimization if not provided
	if req.CostOptimization.CacheTTLHours == 0 {
		req.CostOptimization = CostOptimizationConfig{
			UseCache:         true,
			CacheTTLHours:    24,
			ProgressiveDepth: true,
		}
	}

	config := &LLMConfig{
		Provider:         req.Provider,
		APIKey:           req.APIKey,
		Model:            req.Model,
		KeyType:          req.KeyType,
		CostOptimization: req.CostOptimization,
	}

	configID, err := saveLLMConfig(ctx, project.ID, config)
	if err != nil {
		LogError(ctx, "Failed to save LLM config: %v", err)
		http.Error(w, fmt.Sprintf("Failed to save config: %v", err), http.StatusInternalServerError)
		return
	}

	// Log audit entry
	ipAddress := getIPAddress(r)
	changedBy := r.Header.Get("X-User-Email") // Or extract from auth token
	if changedBy == "" {
		changedBy = "system"
	}
	logConfigChange(ctx, project.ID, configID, "create", changedBy, nil, config, ipAddress)

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":   true,
		"config_id": configID,
		"message":   "Configuration saved successfully",
	})
}

// getLLMConfigHandler handles GET /api/v1/llm/config/{id}
func getLLMConfigHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	configID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Query config
	query := `
		SELECT provider, api_key_encrypted, model, key_type, cost_optimization
		FROM llm_configurations
		WHERE id = $1 AND project_id = $2
	`

	var provider, model, keyType string
	var apiKeyEncrypted []byte
	var costOptJSON sql.NullString

	err = queryRowWithTimeout(ctx, query, configID, project.ID).Scan(
		&provider, &apiKeyEncrypted, &model, &keyType, &costOptJSON,
	)

	if err == sql.ErrNoRows {
		http.Error(w, "Configuration not found", http.StatusNotFound)
		return
	}
	if err != nil {
		LogError(ctx, "Failed to query config: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// Decrypt and mask API key
	apiKey, err := decryptAPIKey(apiKeyEncrypted)
	if err != nil {
		LogError(ctx, "Failed to decrypt API key: %v", err)
		http.Error(w, "Failed to decrypt API key", http.StatusInternalServerError)
		return
	}

	maskedKey := maskAPIKey(apiKey)

	// Parse cost optimization
	var costOpt CostOptimizationConfig
	if costOptJSON.Valid && costOptJSON.String != "" {
		if err := json.Unmarshal([]byte(costOptJSON.String), &costOpt); err != nil {
			costOpt = CostOptimizationConfig{
				UseCache:         true,
				CacheTTLHours:    24,
				ProgressiveDepth: true,
			}
		}
	} else {
		costOpt = CostOptimizationConfig{
			UseCache:         true,
			CacheTTLHours:    24,
			ProgressiveDepth: true,
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"id":                configID,
		"provider":          provider,
		"api_key":           maskedKey,
		"model":             model,
		"key_type":          keyType,
		"cost_optimization": costOpt,
	})
}

// updateLLMConfigHandler handles PUT /api/v1/llm/config/{id}
func updateLLMConfigHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	configID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Provider         string                 `json:"provider"`
		APIKey           string                 `json:"api_key,omitempty"`
		Model            string                 `json:"model"`
		KeyType          string                 `json:"key_type"`
		CostOptimization CostOptimizationConfig `json:"cost_optimization,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.Provider == "" || req.Model == "" {
		http.Error(w, "provider and model are required", http.StatusBadRequest)
		return
	}

	// Validate provider
	if err := validateProvider(req.Provider); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	// Validate model
	if err := validateModel(req.Provider, req.Model); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	// Validate API key format if provided
	if req.APIKey != "" {
		if err := validateAPIKeyFormat(req.Provider, req.APIKey); err != nil {
			http.Error(w, err.Error(), http.StatusBadRequest)
			return
		}
	}

	// Validate cost optimization
	if err := validateCostOptimization(req.CostOptimization); err != nil {
		http.Error(w, err.Error(), http.StatusBadRequest)
		return
	}

	if req.KeyType == "" {
		req.KeyType = "user-provided"
	}

	// Set default cost optimization if not provided
	if req.CostOptimization.CacheTTLHours == 0 {
		req.CostOptimization = CostOptimizationConfig{
			UseCache:         true,
			CacheTTLHours:    24,
			ProgressiveDepth: true,
		}
	}

	config := &LLMConfig{
		Provider:         req.Provider,
		APIKey:           req.APIKey,
		Model:            req.Model,
		KeyType:          req.KeyType,
		CostOptimization: req.CostOptimization,
	}

	// Get old config for audit log
	oldConfig, err := getLLMConfigByID(ctx, configID, project.ID)
	if err != nil {
		// Log warning but continue - config might not exist yet or might have been deleted
		// This is non-critical for audit logging, but we should log it
		LogWarn(ctx, "Could not retrieve old config for audit log (config may not exist): %v", err)
		oldConfig = nil
	}

	err = updateLLMConfig(ctx, configID, project.ID, config)
	if err != nil {
		LogError(ctx, "Failed to update LLM config: %v", err)
		if strings.Contains(err.Error(), "not found") {
			http.Error(w, "Configuration not found", http.StatusNotFound)
		} else {
			http.Error(w, fmt.Sprintf("Failed to update config: %v", err), http.StatusInternalServerError)
		}
		return
	}

	// Log audit entry
	ipAddress := getIPAddress(r)
	changedBy := r.Header.Get("X-User-Email")
	if changedBy == "" {
		changedBy = "system"
	}
	logConfigChange(ctx, project.ID, configID, "update", changedBy, oldConfig, config, ipAddress)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Configuration updated successfully",
	})
}

// deleteLLMConfigHandler handles DELETE /api/v1/llm/config/{id}
func deleteLLMConfigHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	configID := chi.URLParam(r, "id")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Get config before deletion for audit log
	oldConfig, err := getLLMConfigByID(ctx, configID, project.ID)
	if err != nil {
		// Log warning but continue - config might already be deleted or not exist
		// This is non-critical for audit logging, but we should log it
		LogWarn(ctx, "Could not retrieve config for audit log before deletion (config may not exist): %v", err)
		oldConfig = nil
	}

	err = deleteLLMConfig(ctx, configID, project.ID)
	if err != nil {
		LogError(ctx, "Failed to delete LLM config: %v", err)
		if strings.Contains(err.Error(), "not found") {
			http.Error(w, "Configuration not found", http.StatusNotFound)
		} else {
			http.Error(w, fmt.Sprintf("Failed to delete config: %v", err), http.StatusInternalServerError)
		}
		return
	}

	// Log audit entry
	ipAddress := getIPAddress(r)
	changedBy := r.Header.Get("X-User-Email")
	if changedBy == "" {
		changedBy = "system"
	}
	logConfigChange(ctx, project.ID, configID, "delete", changedBy, oldConfig, nil, ipAddress)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"message": "Configuration deleted successfully",
	})
}

// listLLMConfigsHandler handles GET /api/v1/llm/config/project/{projectId}
func listLLMConfigsHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	projectID := chi.URLParam(r, "projectId")
	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Verify project ownership
	if project.ID != projectID {
		http.Error(w, "Access denied", http.StatusForbidden)
		return
	}

	configs, err := listLLMConfigs(ctx, projectID)
	if err != nil {
		LogError(ctx, "Failed to list LLM configs: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"configs": configs,
		"count":   len(configs),
	})
}

// getProvidersHandler handles GET /api/v1/llm/providers
func getProvidersHandler(w http.ResponseWriter, r *http.Request) {
	providers := getSupportedProviders()
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":   true,
		"providers": providers,
	})
}

// getModelsHandler handles GET /api/v1/llm/models/{provider}
func getModelsHandler(w http.ResponseWriter, r *http.Request) {
	provider := chi.URLParam(r, "provider")
	models := getSupportedModels(provider)

	if len(models) == 0 {
		http.Error(w, "Provider not found", http.StatusNotFound)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"models":  models,
	})
}

// validateLLMConfigHandler handles POST /api/v1/llm/config/validate
func validateLLMConfigHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), GetConfig().Timeouts.Context)
	defer cancel()

	var req struct {
		Provider string `json:"provider"`
		APIKey   string `json:"api_key"`
		Model    string `json:"model"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid request body",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.Provider == "" || req.APIKey == "" || req.Model == "" {
		http.Error(w, "provider, api_key, and model are required", http.StatusBadRequest)
		return
	}

	config := &LLMConfig{
		Provider: req.Provider,
		APIKey:   req.APIKey,
		Model:    req.Model,
		KeyType:  "user-provided",
		CostOptimization: CostOptimizationConfig{
			UseCache:         true,
			CacheTTLHours:    24,
			ProgressiveDepth: true,
		},
	}

	err := testLLMConnection(ctx, config)
	if err != nil {
		w.Header().Set("Content-Type", "application/json")
		w.WriteHeader(http.StatusBadRequest)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"success": false,
			"valid":   false,
			"error":   err.Error(),
		})
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success": true,
		"valid":   true,
		"message": "Connection test successful",
	})
}

// getUsageReportHandler handles GET /api/v1/llm/usage/report
func getUsageReportHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	// Parse query parameters
	startDateStr := r.URL.Query().Get("start_date")
	endDateStr := r.URL.Query().Get("end_date")

	var startDate, endDate time.Time
	if startDateStr != "" {
		startDate, err = time.Parse("2006-01-02", startDateStr)
		if err != nil {
			http.Error(w, "Invalid start_date format (expected YYYY-MM-DD)", http.StatusBadRequest)
			return
		}
	} else {
		startDate = time.Now().AddDate(0, 0, -GetConfig().Limits.DefaultDateRangeDays) // Default to configured days ago
	}

	if endDateStr != "" {
		endDate, err = time.Parse("2006-01-02", endDateStr)
		if err != nil {
			http.Error(w, "Invalid end_date format (expected YYYY-MM-DD)", http.StatusBadRequest)
			return
		}
	} else {
		endDate = time.Now()
	}

	report, err := getUsageReport(ctx, project.ID, startDate, endDate)
	if err != nil {
		LogError(ctx, "Failed to get usage report: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(report)
}

// getUsageStatsHandler handles GET /api/v1/llm/usage/stats
func getUsageStatsHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	period := r.URL.Query().Get("period")
	if period == "" {
		period = "monthly"
	}

	stats, err := getUsageStats(ctx, project.ID, period)
	if err != nil {
		LogError(ctx, "Failed to get usage stats: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(stats)
}

// getCostBreakdownHandler handles GET /api/v1/llm/usage/cost-breakdown
func getCostBreakdownHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	period := r.URL.Query().Get("period")
	if period == "" {
		period = "monthly"
	}

	breakdown, err := getCostBreakdown(ctx, project.ID, period)
	if err != nil {
		LogError(ctx, "Failed to get cost breakdown: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(breakdown)
}

// getUsageTrendsHandler handles GET /api/v1/llm/usage/trends
func getUsageTrendsHandler(w http.ResponseWriter, r *http.Request) {
	ctx, cancel := context.WithTimeout(r.Context(), DefaultContextTimeout)
	defer cancel()

	project, err := getProjectFromContext(r.Context())
	if err != nil {
		LogErrorWithContext(r.Context(), err, "Failed to get project from context")
		LogErrorWithContext(r.Context(), err, "Internal server error")
		LogErrorWithContext(r.Context(), fmt.Errorf("internal server error"), "Internal server error")
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "internal_operation",
			Message:       "Internal server error",
			OriginalError: fmt.Errorf("internal server error"),
		}, http.StatusInternalServerError)
		return
	}

	period := r.URL.Query().Get("period")
	if period == "" {
		period = "monthly"
	}

	groupBy := r.URL.Query().Get("group_by")
	if groupBy == "" {
		groupBy = "day"
	}

	// Calculate date range
	var startDate time.Time
	endDate := time.Now()

	switch period {
	case "daily":
		startDate = endDate.AddDate(0, 0, -1)
	case "weekly":
		startDate = endDate.AddDate(0, 0, -7)
	case "monthly":
		startDate = endDate.AddDate(0, -1, 0)
	case "yearly":
		startDate = endDate.AddDate(-1, 0, 0)
	default:
		startDate = endDate.AddDate(0, 0, -GetConfig().Limits.DefaultDateRangeDays)
	}

	// Query trends based on group_by
	var query string
	var rows *sql.Rows

	switch groupBy {
	case "day":
		query = `
			SELECT DATE(created_at) as date, SUM(tokens_used) as tokens, SUM(estimated_cost) as cost, COUNT(*) as requests
			FROM llm_usage
			WHERE project_id = $1 AND created_at >= $2 AND created_at <= $3
			GROUP BY DATE(created_at)
			ORDER BY DATE(created_at) ASC
		`
		rows, err = queryWithTimeout(ctx, query, project.ID, startDate, endDate)
	case "provider":
		query = `
			SELECT provider, SUM(tokens_used) as tokens, SUM(estimated_cost) as cost, COUNT(*) as requests
			FROM llm_usage
			WHERE project_id = $1 AND created_at >= $2 AND created_at <= $3
			GROUP BY provider
			ORDER BY SUM(estimated_cost) DESC
		`
		rows, err = queryWithTimeout(ctx, query, project.ID, startDate, endDate)
	case "model":
		query = `
			SELECT model, SUM(tokens_used) as tokens, SUM(estimated_cost) as cost, COUNT(*) as requests
			FROM llm_usage
			WHERE project_id = $1 AND created_at >= $2 AND created_at <= $3
			GROUP BY model
			ORDER BY SUM(estimated_cost) DESC
		`
		rows, err = queryWithTimeout(ctx, query, project.ID, startDate, endDate)
	default:
		http.Error(w, "Invalid group_by parameter (must be day, provider, or model)", http.StatusBadRequest)
		return
	}

	if err != nil {
		LogError(ctx, "Failed to query trends: %v", err)
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "database_query",
			Message:       "Database error",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	type TrendPoint struct {
		Label    string  `json:"label"`
		Tokens   int64   `json:"tokens"`
		Cost     float64 `json:"cost"`
		Requests int64   `json:"requests"`
	}

	var trends []TrendPoint
	for rows.Next() {
		var label string
		var tokens int64
		var cost float64
		var requests int64

		err := rows.Scan(&label, &tokens, &cost, &requests)
		if err != nil {
			LogError(ctx, "Failed to scan trend row: %v", err)
			continue
		}

		trends = append(trends, TrendPoint{
			Label:    label,
			Tokens:   tokens,
			Cost:     cost,
			Requests: requests,
		})
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":  true,
		"period":   period,
		"group_by": groupBy,
		"trends":   trends,
	})
}

// scanIntentPatterns scans rows into IntentPattern slice
func scanIntentPatterns(rows *sql.Rows) ([]IntentPattern, error) {
	patterns := []IntentPattern{}
	for rows.Next() {
		var pattern IntentPattern
		var patternDataJSON sql.NullString
		var lastUsed, createdAt sql.NullTime

		err := rows.Scan(
			&pattern.ID,
			&pattern.ProjectID,
			&pattern.PatternType,
			&patternDataJSON,
			&pattern.Frequency,
			&lastUsed,
			&createdAt,
		)
		if err != nil {
			return nil, err
		}

		// Unmarshal pattern data
		if patternDataJSON.Valid {
			if err := unmarshalJSONB(patternDataJSON.String, &pattern.PatternData); err != nil {
				pattern.PatternData = make(map[string]interface{})
			}
		} else {
			pattern.PatternData = make(map[string]interface{})
		}

		if lastUsed.Valid {
			pattern.LastUsed = lastUsed.Time.Format(time.RFC3339)
		}
		if createdAt.Valid {
			pattern.CreatedAt = createdAt.Time.Format(time.RFC3339)
		}

		patterns = append(patterns, pattern)
	}
	return patterns, nil
}

// =============================================================================
// Phase 14D: Cost Optimization Metrics Handlers
// =============================================================================

// getCacheMetricsHandler handles GET /api/v1/metrics/cache?project_id={id}
func getCacheMetricsHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	projectID := r.URL.Query().Get("project_id")
	if projectID == "" {
		project, err := getProjectFromContext(ctx)
		if err != nil {
			http.Error(w, "project_id is required", http.StatusBadRequest)
			return
		}
		projectID = project.ID
	}

	// Get cache hit rate
	hitRate := getCacheHitRate(projectID)

	// Get hit/miss counts
	var hits, misses int64
	if val, ok := cacheHitCounter.Load(projectID); ok {
		hits = val.(int64)
	}
	if val, ok := cacheMissCounter.Load(projectID); ok {
		misses = val.(int64)
	}

	// Phase 14D: Get cache size from counter (O(1) instead of O(n))
	var cacheSize int64
	if val, ok := cacheSizeCounter.Load(projectID); ok {
		cacheSize = val.(int64)
	}

	// Get LLM config for TTL
	llmConfig, err := getLLMConfig(ctx, projectID)
	cacheTTLHours := 24 // default
	if err == nil && llmConfig != nil && llmConfig.CostOptimization.CacheTTLHours > 0 {
		cacheTTLHours = llmConfig.CostOptimization.CacheTTLHours
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":         true,
		"project_id":      projectID,
		"hit_rate":        hitRate,
		"total_hits":      hits,
		"total_misses":    misses,
		"cache_size":      cacheSize,
		"cache_ttl_hours": cacheTTLHours,
	})
}

// getCostMetricsHandler handles GET /api/v1/metrics/cost?project_id={id}&period={daily|weekly|monthly}
func getCostMetricsHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	projectID := r.URL.Query().Get("project_id")
	if projectID == "" {
		project, err := getProjectFromContext(ctx)
		if err != nil {
			http.Error(w, "project_id is required", http.StatusBadRequest)
			return
		}
		projectID = project.ID
	}

	period := r.URL.Query().Get("period")
	if period == "" {
		period = "monthly"
	}
	if period != "daily" && period != "weekly" && period != "monthly" {
		http.Error(w, "period must be 'daily', 'weekly', or 'monthly'", http.StatusBadRequest)
		return
	}

	// Calculate date range based on period
	var startDate time.Time
	now := time.Now()
	switch period {
	case "daily":
		startDate = now.AddDate(0, 0, -1)
	case "weekly":
		startDate = now.AddDate(0, 0, -7)
	case "monthly":
		startDate = now.AddDate(0, -1, 0)
	}

	// Query usage data
	query := `
		SELECT 
			COALESCE(SUM(estimated_cost), 0) as total_cost,
			COUNT(*) as total_requests
		FROM llm_usage
		WHERE project_id = $1 AND created_at >= $2
	`

	var totalCost float64
	var totalRequests int
	err := db.QueryRowContext(ctx, query, projectID, startDate).Scan(&totalCost, &totalRequests)
	if err != nil {
		LogError(ctx, "Failed to query cost metrics: %v", err)
		http.Error(w, "Failed to retrieve cost metrics", http.StatusInternalServerError)
		return
	}

	// Phase 14D: Calculate cache hit savings from actual tracked metrics
	cacheHitRate := getCacheHitRate(projectID)
	// Calculate actual cache hit savings: hit rate * total cost (cache hits save 100% of cost)
	cacheHitSavings := totalCost * cacheHitRate

	// Phase 14D: Get actual model selection savings from tracked metrics
	modelSelectionSavings := getModelSelectionSavings(projectID)

	// Total savings
	totalSavings := cacheHitSavings + modelSelectionSavings
	savingsPercentage := 0.0
	if totalCost > 0 {
		savingsPercentage = (totalSavings / totalCost) * 100.0
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"success":                 true,
		"project_id":              projectID,
		"period":                  period,
		"total_cost":              totalCost,
		"cost_savings":            totalSavings,
		"savings_percentage":      savingsPercentage,
		"cache_hit_savings":       cacheHitSavings,
		"model_selection_savings": modelSelectionSavings,
		"total_requests":          totalRequests,
	})
}

// Cross-file dependency analysis helper functions

func analyzeCrossFileDependencies(files []string, language string, entryPoints []string, analysisDepth string, detectCircular, findUnused bool) map[string]interface{} {
	analysis := map[string]interface{}{
		"dependencyGraph": map[string]interface{}{
			"nodes": []map[string]interface{}{},
			"edges": []map[string]interface{}{},
		},
		"circularDependencies":   []map[string]interface{}{},
		"unusedExports":          []map[string]interface{}{},
		"unusedImports":          []map[string]interface{}{},
		"entryPointCoverage":     []string{},
		"refactoringSuggestions": []map[string]interface{}{},
	}

	// Build dependency graph
	dependencyGraph := buildDependencyGraph(files, language)

	// Convert to analysis format
	for file, deps := range dependencyGraph {
		// Add node
		analysis["dependencyGraph"].(map[string]interface{})["nodes"] = append(
			analysis["dependencyGraph"].(map[string]interface{})["nodes"].([]map[string]interface{}),
			map[string]interface{}{
				"id":      file,
				"type":    getFileType(file, language),
				"imports": len(deps.Imports),
				"exports": len(deps.Exports),
			},
		)

		// Add edges
		for _, imp := range deps.Imports {
			analysis["dependencyGraph"].(map[string]interface{})["edges"] = append(
				analysis["dependencyGraph"].(map[string]interface{})["edges"].([]map[string]interface{}),
				map[string]interface{}{
					"source": file,
					"target": imp.Module,
					"type":   "import",
					"items":  imp.Items,
				},
			)
		}
	}

	if detectCircular {
		analysis["circularDependencies"] = detectCircularDependencies(dependencyGraph)
	}

	if findUnused {
		analysis["unusedExports"] = findUnusedExports(dependencyGraph, entryPoints)
		analysis["unusedImports"] = findUnusedImports(dependencyGraph)
	}

	// Entry point coverage
	analysis["entryPointCoverage"] = calculateEntryPointCoverage(dependencyGraph, entryPoints)

	// Refactoring suggestions
	analysis["refactoringSuggestions"] = generateDependencyRefactoringSuggestions(dependencyGraph, analysis["circularDependencies"].([]map[string]interface{}))

	return analysis
}

// DependencyGraph represents the dependency relationships between files
type DependencyGraph map[string]*FileDependencies

type FileDependencies struct {
	Imports []ImportStatement
	Exports []ExportStatement
}

type ImportStatement struct {
	Module string
	Items  []string
}

type ExportStatement struct {
	Name string
	Type string // "function", "class", "variable", "default"
}

func buildDependencyGraph(files []string, language string) DependencyGraph {
	graph := make(DependencyGraph)

	for _, file := range files {
		deps := analyzeFileDependencies(file, language)
		graph[file] = deps
	}

	return graph
}

func analyzeFileDependencies(file, language string) *FileDependencies {
	deps := &FileDependencies{
		Imports: []ImportStatement{},
		Exports: []ExportStatement{},
	}

	// Read file content (simplified - would use AST parsing in real implementation)
	content, err := os.ReadFile(file)
	if err != nil {
		return deps
	}

	contentStr := string(content)

	switch language {
	case "javascript", "typescript":
		deps.Imports = parseJSImports(contentStr)
		deps.Exports = parseJSExports(contentStr)
	case "python":
		deps.Imports = parsePythonImports(contentStr)
		deps.Exports = parsePythonExports(contentStr)
	case "go":
		deps.Imports = parseGoImports(contentStr)
		deps.Exports = parseGoExports(contentStr)
	}

	return deps
}

func parseJSImports(content string) []ImportStatement {
	imports := []ImportStatement{}
	lines := strings.Split(content, "\n")

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "import") {
			// Parse import statements
			if strings.Contains(line, "from") {
				parts := strings.Split(line, "from")
				if len(parts) == 2 {
					module := strings.Trim(strings.TrimSpace(parts[1]), `'"`)
					// Extract imported items (simplified)
					importPart := strings.TrimSpace(parts[0])
					var items []string
					if strings.Contains(importPart, "{") {
						// Named imports
						start := strings.Index(importPart, "{")
						end := strings.Index(importPart, "}")
						if start >= 0 && end > start {
							importList := importPart[start+1 : end]
							items = strings.Split(importList, ",")
							for i, item := range items {
								items[i] = strings.TrimSpace(item)
							}
						}
					} else if strings.HasPrefix(importPart, "import ") {
						// Default import
						defaultImport := strings.TrimPrefix(importPart, "import ")
						defaultImport = strings.TrimSpace(defaultImport)
						if defaultImport != "" {
							items = []string{defaultImport}
						}
					}

					imports = append(imports, ImportStatement{
						Module: module,
						Items:  items,
					})
				}
			}
		}
	}

	return imports
}

func parseJSExports(content string) []ExportStatement {
	exports := []ExportStatement{}
	lines := strings.Split(content, "\n")

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "export") {
			if strings.HasPrefix(line, "export default") {
				exports = append(exports, ExportStatement{
					Name: "default",
					Type: "default",
				})
			} else if strings.Contains(line, "function") {
				// export function name
				parts := strings.Split(line, "function")
				if len(parts) > 1 {
					funcName := strings.TrimSpace(parts[1])
					if idx := strings.Index(funcName, "("); idx > 0 {
						funcName = funcName[:idx]
						exports = append(exports, ExportStatement{
							Name: funcName,
							Type: "function",
						})
					}
				}
			} else if strings.Contains(line, "const") || strings.Contains(line, "let") || strings.Contains(line, "var") {
				// export const name
				parts := strings.Split(line, "=")
				if len(parts) > 0 {
					varDecl := strings.TrimSpace(parts[0])
					if strings.HasPrefix(varDecl, "export const") {
						varName := strings.TrimPrefix(varDecl, "export const")
						varName = strings.TrimSpace(varName)
						exports = append(exports, ExportStatement{
							Name: varName,
							Type: "variable",
						})
					}
				}
			}
		}
	}

	return exports
}

func parsePythonImports(content string) []ImportStatement {
	imports := []ImportStatement{}
	lines := strings.Split(content, "\n")

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "import ") || strings.HasPrefix(line, "from ") {
			if strings.HasPrefix(line, "import ") {
				// import module
				module := strings.TrimPrefix(line, "import ")
				if idx := strings.Index(module, " as "); idx > 0 {
					module = module[:idx]
				}
				imports = append(imports, ImportStatement{
					Module: strings.TrimSpace(module),
					Items:  []string{}, // Module-level import
				})
			} else if strings.HasPrefix(line, "from ") {
				// from module import items
				parts := strings.Split(line, " import ")
				if len(parts) == 2 {
					module := strings.TrimPrefix(parts[0], "from ")
					itemsStr := parts[1]
					var items []string

					if itemsStr != "*" {
						// Parse imported items
						if strings.Contains(itemsStr, "(") {
							// Multi-line import
							continue // Skip for simplicity
						} else {
							items = strings.Split(itemsStr, ",")
							for i, item := range items {
								item = strings.TrimSpace(item)
								if idx := strings.Index(item, " as "); idx > 0 {
									item = item[:idx]
								}
								items[i] = strings.TrimSpace(item)
							}
						}
					}

					imports = append(imports, ImportStatement{
						Module: strings.TrimSpace(module),
						Items:  items,
					})
				}
			}
		}
	}

	return imports
}

func parsePythonExports(content string) []ExportStatement {
	exports := []ExportStatement{}
	lines := strings.Split(content, "\n")

	for _, line := range lines {
		line = strings.TrimSpace(line)
		if strings.HasPrefix(line, "def ") {
			// Function definition
			parts := strings.Split(line, "def ")
			if len(parts) > 1 {
				funcName := strings.TrimSpace(parts[1])
				if idx := strings.Index(funcName, "("); idx > 0 {
					funcName = funcName[:idx]
					exports = append(exports, ExportStatement{
						Name: funcName,
						Type: "function",
					})
				}
			}
		} else if strings.HasPrefix(line, "class ") {
			// Class definition
			parts := strings.Split(line, "class ")
			if len(parts) > 1 {
				className := strings.TrimSpace(parts[1])
				if idx := strings.Index(className, "("); idx > 0 {
					className = className[:idx]
				} else if idx := strings.Index(className, ":"); idx > 0 {
					className = className[:idx]
				}
				exports = append(exports, ExportStatement{
					Name: strings.TrimSpace(className),
					Type: "class",
				})
			}
		}
	}

	return exports
}

func parseGoImports(content string) []ImportStatement {
	imports := []ImportStatement{}
	lines := strings.Split(content, "\n")

	inImportBlock := false
	for _, line := range lines {
		line = strings.TrimSpace(line)

		if line == "import (" {
			inImportBlock = true
			continue
		} else if inImportBlock && line == ")" {
			inImportBlock = false
			continue
		}

		if inImportBlock || strings.HasPrefix(line, "import ") {
			var module string
			if inImportBlock {
				// Multi-line import
				if strings.Contains(line, `"`) {
					start := strings.Index(line, `"`)
					end := strings.LastIndex(line, `"`)
					if start >= 0 && end > start {
						module = line[start+1 : end]
					}
				}
			} else {
				// Single import
				if strings.Contains(line, `"`) {
					start := strings.Index(line, `"`)
					end := strings.LastIndex(line, `"`)
					if start >= 0 && end > start {
						module = line[start+1 : end]
					}
				}
			}

			if module != "" {
				imports = append(imports, ImportStatement{
					Module: module,
					Items:  []string{}, // Go imports whole packages
				})
			}
		}
	}

	return imports
}

func parseGoExports(content string) []ExportStatement {
	exports := []ExportStatement{}
	lines := strings.Split(content, "\n")

	for _, line := range lines {
		line = strings.TrimSpace(line)

		// Go exports are capitalized identifiers
		if strings.HasPrefix(line, "func ") {
			// Function export
			parts := strings.Split(line, "func ")
			if len(parts) > 1 {
				funcSig := strings.TrimSpace(parts[1])
				if idx := strings.Index(funcSig, "("); idx > 0 {
					funcName := funcSig[:idx]
					if len(funcName) > 0 && unicode.IsUpper(rune(funcName[0])) {
						exports = append(exports, ExportStatement{
							Name: funcName,
							Type: "function",
						})
					}
				}
			}
		} else if strings.HasPrefix(line, "type ") {
			// Type export
			parts := strings.Split(line, "type ")
			if len(parts) > 1 {
				typeDecl := strings.TrimSpace(parts[1])
				words := strings.Fields(typeDecl)
				if len(words) >= 1 {
					typeName := words[0]
					if len(typeName) > 0 && unicode.IsUpper(rune(typeName[0])) {
						exports = append(exports, ExportStatement{
							Name: typeName,
							Type: "type",
						})
					}
				}
			}
		}
	}

	return exports
}

func detectCircularDependencies(graph DependencyGraph) []map[string]interface{} {
	circularDeps := []map[string]interface{}{}

	// Simple cycle detection (would be more sophisticated in real implementation)
	visited := make(map[string]bool)
	recursionStack := make(map[string]bool)

	var dfs func(string, []string) []string
	dfs = func(node string, path []string) []string {
		visited[node] = true
		recursionStack[node] = true

		// Check if this creates a cycle
		for _, p := range path {
			if p == node {
				// Found cycle
				cycleStart := -1
				for i, n := range path {
					if n == node {
						cycleStart = i
						break
					}
				}
				if cycleStart >= 0 {
					cycle := append(path[cycleStart:], node)
					return cycle
				}
			}
		}

		newPath := append(path, node)

		if deps, exists := graph[node]; exists {
			for _, imp := range deps.Imports {
				if !visited[imp.Module] {
					if cycle := dfs(imp.Module, newPath); len(cycle) > 0 {
						return cycle
					}
				} else if recursionStack[imp.Module] {
					// Found back edge - cycle
					return append(newPath, imp.Module)
				}
			}
		}

		recursionStack[node] = false
		return nil
	}

	for node := range graph {
		if !visited[node] {
			if cycle := dfs(node, []string{}); len(cycle) > 0 {
				circularDeps = append(circularDeps, map[string]interface{}{
					"cycle":       cycle,
					"length":      len(cycle),
					"description": fmt.Sprintf("Circular dependency: %s", strings.Join(cycle, " -> ")),
				})
			}
		}
	}

	return circularDeps
}

func findUnusedExports(graph DependencyGraph, entryPoints []string) []map[string]interface{} {
	unusedExports := []map[string]interface{}{}

	// Build usage map
	usageMap := make(map[string]map[string]bool) // module -> export -> used

	// Initialize usage map
	for file, deps := range graph {
		usageMap[file] = make(map[string]bool)
		for _, exp := range deps.Exports {
			usageMap[file][exp.Name] = false
		}
	}

	// Mark exports as used based on imports
	for _, deps := range graph {
		for _, imp := range deps.Imports {
			if moduleUsage, exists := usageMap[imp.Module]; exists {
				for _, item := range imp.Items {
					if _, exportExists := moduleUsage[item]; exportExists {
						moduleUsage[item] = true
					}
				}
			}
		}
	}

	// Special handling for entry points - their exports are considered used
	for _, entry := range entryPoints {
		if moduleUsage, exists := usageMap[entry]; exists {
			for export := range moduleUsage {
				moduleUsage[export] = true
			}
		}
	}

	// Collect unused exports
	for module, exports := range usageMap {
		for export, used := range exports {
			if !used {
				unusedExports = append(unusedExports, map[string]interface{}{
					"module":      module,
					"export":      export,
					"type":        "unused_export",
					"description": fmt.Sprintf("Export '%s' in %s is not used", export, module),
				})
			}
		}
	}

	return unusedExports
}

func findUnusedImports(graph DependencyGraph) []map[string]interface{} {
	unusedImports := []map[string]interface{}{}

	// This is a simplified version - real implementation would need more sophisticated analysis
	// For now, we'll just report potential issues based on basic patterns

	for file, deps := range graph {
		importCount := make(map[string]int)
		for _, imp := range deps.Imports {
			importCount[imp.Module]++
		}

		// Flag modules imported multiple times (potential cleanup)
		for module, count := range importCount {
			if count > 1 {
				unusedImports = append(unusedImports, map[string]interface{}{
					"file":        file,
					"module":      module,
					"type":        "duplicate_import",
					"description": fmt.Sprintf("Module '%s' imported %d times in %s", module, count, file),
				})
			}
		}
	}

	return unusedImports
}

func calculateEntryPointCoverage(graph DependencyGraph, entryPoints []string) []string {
	coveredFiles := make(map[string]bool)
	visited := make(map[string]bool)

	var dfs func(string)
	dfs = func(file string) {
		if visited[file] {
			return
		}
		visited[file] = true
		coveredFiles[file] = true

		if deps, exists := graph[file]; exists {
			for _, imp := range deps.Imports {
				dfs(imp.Module)
			}
		}
	}

	// Start from entry points
	for _, entry := range entryPoints {
		dfs(entry)
	}

	// Convert to sorted list
	var coverage []string
	for file := range coveredFiles {
		coverage = append(coverage, file)
	}
	sort.Strings(coverage)

	return coverage
}

func generateDependencyRefactoringSuggestions(graph DependencyGraph, circularDeps []map[string]interface{}) []map[string]interface{} {
	suggestions := []map[string]interface{}{}

	// Suggestions based on circular dependencies
	for _, circular := range circularDeps {
		if cycle, ok := circular["cycle"].([]string); ok && len(cycle) > 0 {
			suggestions = append(suggestions, map[string]interface{}{
				"type":           "break_circular_dependency",
				"priority":       "high",
				"description":    fmt.Sprintf("Break circular dependency: %s", strings.Join(cycle, " -> ")),
				"suggestion":     "Consider extracting common functionality to a separate module",
				"affected_files": cycle,
			})
		}
	}

	// Suggestions based on module coupling
	for file, deps := range graph {
		importCount := len(deps.Imports)
		if importCount > 10 {
			suggestions = append(suggestions, map[string]interface{}{
				"type":           "high_coupling",
				"priority":       "medium",
				"description":    fmt.Sprintf("File %s has %d imports - consider reducing coupling", file, importCount),
				"suggestion":     "Break down into smaller, more focused modules",
				"affected_files": []string{file},
			})
		}
	}

	return suggestions
}

func getFileType(filename, language string) string {
	switch language {
	case "javascript":
		if strings.HasSuffix(filename, ".js") {
			return "javascript"
		} else if strings.HasSuffix(filename, ".mjs") {
			return "javascript_module"
		}
	case "typescript":
		if strings.HasSuffix(filename, ".ts") {
			return "typescript"
		} else if strings.HasSuffix(filename, ".tsx") {
			return "typescript_react"
		}
	case "python":
		if strings.HasSuffix(filename, ".py") {
			return "python"
		}
	case "go":
		if strings.HasSuffix(filename, ".go") {
			return "go"
		}
	}
	return "unknown"
}

// =============================================================================
// Phase 14E: Task Dependency Management Handlers
// =============================================================================

// createTaskHandler creates a new task
func getTaskWithRelations(projectID, taskID string) (*Task, error) {
	var task Task
	var tags pq.StringArray
	var completedAt, verifiedAt sql.NullTime

	err := db.QueryRow(`
		SELECT id, project_id, source, title, description, file_path, line_number, status, priority,
		       assigned_to, estimated_effort, actual_effort, tags, verification_confidence,
		       version, created_at, updated_at, completed_at, verified_at
		FROM tasks WHERE id = $1 AND project_id = $2
	`, taskID, projectID).Scan(
		&task.ID, &task.ProjectID, &task.Source, &task.Title, &task.Description, &task.FilePath,
		&task.LineNumber, &task.Status, &task.Priority, &task.AssignedTo, &task.EstimatedEffort,
		&task.ActualEffort, &tags, &task.VerificationConfidence, &task.Version,
		&task.CreatedAt, &task.UpdatedAt, &completedAt, &verifiedAt,
	)

	if err != nil {
		return nil, err
	}

	task.Tags = []string(tags)
	if completedAt.Valid {
		task.CompletedAt = &completedAt.Time
	}
	if verifiedAt.Valid {
		task.VerifiedAt = &verifiedAt.Time
	}

	// Load dependencies
	// Dependencies would be fetched separately via dependency repository, _ = getTaskDependencies(taskID, projectID)

	// Load dependents
	// Dependents would be fetched separately via dependency repository, _ = getTaskDependents(taskID, projectID)

	// Load verification status - would be fetched separately via verification repository
	_, _ = getTaskVerifications(taskID)

	// Load links
	// Links would be fetched separately, _ = getTaskLinks(taskID)

	return &task, nil
}

func getTaskDependencies(taskID, projectID string) ([]TaskDependency, error) {
	rows, err := db.Query(`
		SELECT td.id, td.task_id, td.depends_on_task_id, td.dependency_type, td.confidence, td.created_at,
		       t.title, t.status, t.priority
		FROM task_dependencies td
		JOIN tasks t ON td.depends_on_task_id = t.id
		WHERE td.task_id = $1 AND t.project_id = $2
		ORDER BY td.created_at DESC
	`, taskID, projectID)

	if err != nil {
		return nil, err
	}
	defer rows.Close()

	dependencies := []TaskDependency{}
	for rows.Next() {
		var dep TaskDependency
		var dependsOnTitle, dependsOnStatus, dependsOnPriority string

		err := rows.Scan(&dep.ID, &dep.TaskID, &dep.DependsOnTaskID, &dep.DependencyType,
			&dep.Confidence, &dep.CreatedAt, &dependsOnTitle, &dependsOnStatus, &dependsOnPriority)
		if err != nil {
			continue
		}

		// DependsOnTask would be fetched separately - TaskDependency only contains TaskID reference
		_ = &models.Task{
			ID:       dep.DependsOnTaskID,
			Title:    dependsOnTitle,
			Status:   models.TaskStatus(dependsOnStatus),
			Priority: models.TaskPriority(dependsOnPriority),
		}

		dependencies = append(dependencies, dep)
	}

	return dependencies, nil
}

func getTaskDependents(taskID, projectID string) ([]TaskDependency, error) {
	rows, err := db.Query(`
		SELECT td.id, td.task_id, td.depends_on_task_id, td.dependency_type, td.confidence, td.created_at,
		       t.title, t.status, t.priority
		FROM task_dependencies td
		JOIN tasks t ON td.task_id = t.id
		WHERE td.depends_on_task_id = $1 AND t.project_id = $2
		ORDER BY td.created_at DESC
	`, taskID, projectID)

	if err != nil {
		return nil, err
	}
	defer rows.Close()

	dependents := []TaskDependency{}
	for rows.Next() {
		var dep TaskDependency
		var dependentTitle, dependentStatus, dependentPriority string

		err := rows.Scan(&dep.ID, &dep.TaskID, &dep.DependsOnTaskID, &dep.DependencyType,
			&dep.Confidence, &dep.CreatedAt, &dependentTitle, &dependentStatus, &dependentPriority)
		if err != nil {
			continue
		}

		// DependsOnTask would be fetched separately - TaskDependency only contains TaskID reference
		_ = &models.Task{
			ID:       dep.TaskID,
			Title:    dependentTitle,
			Status:   models.TaskStatus(dependentStatus),
			Priority: models.TaskPriority(dependentPriority),
		}

		dependents = append(dependents, dep)
	}

	return dependents, nil
}

func getTaskVerifications(taskID string) ([]TaskVerification, error) {
	rows, err := db.Query(`
		SELECT id, task_id, verification_type, status, confidence, evidence, retry_count, verified_at, created_at
		FROM task_verifications WHERE task_id = $1 ORDER BY created_at DESC
	`, taskID)

	if err != nil {
		return nil, err
	}
	defer rows.Close()

	verifications := []TaskVerification{}
	for rows.Next() {
		var verification TaskVerification
		var evidenceBytes []byte
		var verifiedAt sql.NullTime

		err := rows.Scan(&verification.ID, &verification.TaskID, &verification.VerificationType,
			&verification.Status, &verification.Confidence, &evidenceBytes,
			&verification.RetryCount, &verifiedAt, &verification.CreatedAt)
		if err != nil {
			continue
		}

		if len(evidenceBytes) > 0 {
			json.Unmarshal(evidenceBytes, &verification.Evidence)
		}

		if verifiedAt.Valid {
			verification.VerifiedAt = &verifiedAt.Time
		}

		verifications = append(verifications, verification)
	}

	return verifications, nil
}

func getTaskLinks(taskID string) ([]TaskLink, error) {
	rows, err := db.Query(`
		SELECT id, task_id, link_type, linked_id, created_at
		FROM task_links WHERE task_id = $1 ORDER BY created_at DESC
	`, taskID)

	if err != nil {
		return nil, err
	}
	defer rows.Close()

	links := []TaskLink{}
	for rows.Next() {
		var link TaskLink

		err := rows.Scan(&link.ID, &link.TaskID, &link.LinkType, &link.LinkedID, &link.CreatedAt)
		if err != nil {
			continue
		}

		links = append(links, link)
	}

	return links, nil
}

// =============================================================================
// =============================================================================

// createRepositoryHandler creates a new repository
func createRepositoryHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		Name          string `json:"name"`
		FullName      string `json:"full_name"`
		Description   string `json:"description,omitempty"`
		URL           string `json:"url,omitempty"`
		CloneURL      string `json:"clone_url,omitempty"`
		SSHURL        string `json:"ssh_url,omitempty"`
		DefaultBranch string `json:"default_branch,omitempty"`
		Language      string `json:"language,omitempty"`
		IsPrivate     bool   `json:"is_private"`
		IsArchived    bool   `json:"is_archived"`
		IsTemplate    bool   `json:"is_template"`
		IsFork        bool   `json:"is_fork"`
		ParentRepoID  string `json:"parent_repo_id,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.Name == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "name",
			Message: "Repository name is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	if req.FullName == "" {
		req.FullName = req.Name
	}

	if req.DefaultBranch == "" {
		req.DefaultBranch = "main"
	}

	// Generate repository ID
	repoID := uuid.New().String()
	now := time.Now()

	// Insert repository
	_, err = db.Exec(`
		INSERT INTO repositories (
			id, org_id, name, full_name, description, url, clone_url, ssh_url,
			default_branch, language, is_private, is_archived, is_template, is_fork,
			parent_repo_id, created_at, updated_at, sync_status
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18)
	`, repoID, org.ID, req.Name, req.FullName, req.Description, req.URL,
		req.CloneURL, req.SSHURL, req.DefaultBranch, req.Language, req.IsPrivate,
		req.IsArchived, req.IsTemplate, req.IsFork, req.ParentRepoID, now, now, "pending")

	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "repository_create",
			Message:       "Failed to create repository",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// Return created repository
	repo := Repository{
		ID:            repoID,
		OrgID:         org.ID,
		Name:          req.Name,
		FullName:      req.FullName,
		Description:   req.Description,
		URL:           req.URL,
		CloneURL:      req.CloneURL,
		SSHURL:        req.SSHURL,
		DefaultBranch: req.DefaultBranch,
		Language:      req.Language,
		IsPrivate:     req.IsPrivate,
		IsArchived:    req.IsArchived,
		IsTemplate:    req.IsTemplate,
		IsFork:        req.IsFork,
		ParentRepoID:  req.ParentRepoID,
		CreatedAt:     now,
		UpdatedAt:     now,
		SyncStatus:    "pending",
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(repo)
}

// getRepositoryHandler retrieves a repository by ID
func getRepositoryHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	repoID := chi.URLParam(r, "id")
	if repoID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Repository ID is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	repo, err := getRepositoryByID(repoID, org.ID)
	if err != nil {
		if err == sql.ErrNoRows {
			WriteErrorResponse(w, &NotFoundError{
				Resource: "repository",
				ID:       repoID,
				Message:  "Repository not found",
			}, http.StatusNotFound)
		} else {
			WriteErrorResponse(w, &DatabaseError{
				Operation:     "repository_get",
				Message:       "Failed to get repository",
				OriginalError: err,
			}, http.StatusInternalServerError)
		}
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(repo)
}

// updateRepositoryHandler updates a repository
func updateRepositoryHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	repoID := chi.URLParam(r, "id")
	if repoID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Repository ID is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		Description   string `json:"description,omitempty"`
		URL           string `json:"url,omitempty"`
		CloneURL      string `json:"clone_url,omitempty"`
		SSHURL        string `json:"ssh_url,omitempty"`
		DefaultBranch string `json:"default_branch,omitempty"`
		Language      string `json:"language,omitempty"`
		IsArchived    *bool  `json:"is_archived,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Build update query dynamically
	setParts := []string{}
	args := []interface{}{}
	argCount := 1

	if req.Description != "" {
		setParts = append(setParts, fmt.Sprintf("description = $%d", argCount))
		args = append(args, req.Description)
		argCount++
	}

	if req.URL != "" {
		setParts = append(setParts, fmt.Sprintf("url = $%d", argCount))
		args = append(args, req.URL)
		argCount++
	}

	if req.CloneURL != "" {
		setParts = append(setParts, fmt.Sprintf("clone_url = $%d", argCount))
		args = append(args, req.CloneURL)
		argCount++
	}

	if req.SSHURL != "" {
		setParts = append(setParts, fmt.Sprintf("ssh_url = $%d", argCount))
		args = append(args, req.SSHURL)
		argCount++
	}

	if req.DefaultBranch != "" {
		setParts = append(setParts, fmt.Sprintf("default_branch = $%d", argCount))
		args = append(args, req.DefaultBranch)
		argCount++
	}

	if req.Language != "" {
		setParts = append(setParts, fmt.Sprintf("language = $%d", argCount))
		args = append(args, req.Language)
		argCount++
	}

	if req.IsArchived != nil {
		setParts = append(setParts, fmt.Sprintf("is_archived = $%d", argCount))
		args = append(args, *req.IsArchived)
		argCount++
	}

	if len(setParts) == 0 {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "No valid fields to update",
			Code:    "no_updates",
		}, http.StatusBadRequest)
		return
	}

	setParts = append(setParts, "updated_at = NOW()")

	query := fmt.Sprintf("UPDATE repositories SET %s WHERE id = $%d AND org_id = $%d",
		strings.Join(setParts, ", "), argCount, argCount+1)

	args = append(args, repoID, org.ID)

	result, err := db.Exec(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "repository_update",
			Message:       "Failed to update repository",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	if rowsAffected, _ := result.RowsAffected(); rowsAffected == 0 {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "repository",
			ID:       repoID,
			Message:  "Repository not found or access denied",
		}, http.StatusNotFound)
		return
	}

	// Return updated repository
	repo, err := getRepositoryByID(repoID, org.ID)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "repository_get_after_update",
			Message:       "Failed to get updated repository",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(repo)
}

// deleteRepositoryHandler deletes a repository
func deleteRepositoryHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	repoID := chi.URLParam(r, "id")
	if repoID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Repository ID is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	result, err := db.Exec("DELETE FROM repositories WHERE id = $1 AND org_id = $2", repoID, org.ID)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "repository_delete",
			Message:       "Failed to delete repository",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	if rowsAffected, _ := result.RowsAffected(); rowsAffected == 0 {
		WriteErrorResponse(w, &NotFoundError{
			Resource: "repository",
			ID:       repoID,
			Message:  "Repository not found or access denied",
		}, http.StatusNotFound)
		return
	}

	w.WriteHeader(http.StatusNoContent)
}

// listRepositoriesHandler lists repositories with filtering and pagination
func listRepositoriesHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// Parse query parameters
	language := r.URL.Query().Get("language")
	isPrivate := r.URL.Query().Get("is_private")
	isArchived := r.URL.Query().Get("is_archived")
	syncStatus := r.URL.Query().Get("sync_status")

	limitStr := r.URL.Query().Get("limit")
	limit := 50
	if limitStr != "" {
		if parsedLimit, err := strconv.Atoi(limitStr); err == nil && parsedLimit > 0 && parsedLimit <= 100 {
			limit = parsedLimit
		}
	}

	offsetStr := r.URL.Query().Get("offset")
	offset := 0
	if offsetStr != "" {
		if parsedOffset, err := strconv.Atoi(offsetStr); err == nil && parsedOffset >= 0 {
			offset = parsedOffset
		}
	}

	// Build query
	query := `
		SELECT id, name, full_name, description, url, clone_url, ssh_url, default_branch,
		       language, size_bytes, stars_count, forks_count, watchers_count,
		       is_private, is_archived, is_template, is_fork, parent_repo_id,
		       created_at, updated_at, last_synced_at, sync_status
		FROM repositories
		WHERE org_id = $1`
	args := []interface{}{org.ID}
	argCount := 1

	if language != "" {
		argCount++
		query += fmt.Sprintf(" AND language = $%d", argCount)
		args = append(args, language)
	}

	if isPrivate != "" {
		isPrivateBool, _ := strconv.ParseBool(isPrivate)
		argCount++
		query += fmt.Sprintf(" AND is_private = $%d", argCount)
		args = append(args, isPrivateBool)
	}

	if isArchived != "" {
		isArchivedBool, _ := strconv.ParseBool(isArchived)
		argCount++
		query += fmt.Sprintf(" AND is_archived = $%d", argCount)
		args = append(args, isArchivedBool)
	}

	if syncStatus != "" {
		argCount++
		query += fmt.Sprintf(" AND sync_status = $%d", argCount)
		args = append(args, syncStatus)
	}

	query += " ORDER BY updated_at DESC LIMIT $" + strconv.Itoa(argCount+1) +
		" OFFSET $" + strconv.Itoa(argCount+2)
	args = append(args, limit, offset)

	rows, err := db.Query(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "repository_list",
			Message:       "Failed to list repositories",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	repos := []Repository{}
	for rows.Next() {
		var repo Repository
		var lastSyncedAt sql.NullTime

		err := rows.Scan(
			&repo.ID, &repo.Name, &repo.FullName, &repo.Description, &repo.URL,
			&repo.CloneURL, &repo.SSHURL, &repo.DefaultBranch, &repo.Language,
			&repo.SizeBytes, &repo.StarsCount, &repo.ForksCount, &repo.WatchersCount,
			&repo.IsPrivate, &repo.IsArchived, &repo.IsTemplate, &repo.IsFork,
			&repo.ParentRepoID, &repo.CreatedAt, &repo.UpdatedAt, &lastSyncedAt, &repo.SyncStatus,
		)
		if err != nil {
			continue
		}

		if lastSyncedAt.Valid {
			repo.LastSyncedAt = &lastSyncedAt.Time
		}

		repos = append(repos, repo)
	}

	// Get total count
	countQuery := "SELECT COUNT(*) FROM repositories WHERE org_id = $1"
	countArgs := []interface{}{org.ID}
	if language != "" {
		countQuery += " AND language = $2"
		countArgs = append(countArgs, language)
	}
	if isPrivate != "" {
		isPrivateBool, _ := strconv.ParseBool(isPrivate)
		countQuery += " AND is_private = $3"
		countArgs = append(countArgs, isPrivateBool)
	}
	if isArchived != "" {
		isArchivedBool, _ := strconv.ParseBool(isArchived)
		countQuery += " AND is_archived = $4"
		countArgs = append(countArgs, isArchivedBool)
	}
	if syncStatus != "" {
		countQuery += " AND sync_status = $5"
		countArgs = append(countArgs, syncStatus)
	}

	var totalCount int
	db.QueryRow(countQuery, countArgs...).Scan(&totalCount)

	result := map[string]interface{}{
		"repositories": repos,
		"total":        totalCount,
		"limit":        limit,
		"offset":       offset,
		"has_more":     offset+limit < totalCount,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// Helper functions for repository management

func getRepositoryByID(repoID, orgID string) (*Repository, error) {
	var repo Repository
	var lastSyncedAt sql.NullTime

	err := db.QueryRow(`
		SELECT id, org_id, name, full_name, description, url, clone_url, ssh_url, default_branch,
		       language, size_bytes, stars_count, forks_count, watchers_count,
		       is_private, is_archived, is_template, is_fork, parent_repo_id,
		       created_at, updated_at, last_synced_at, sync_status
		FROM repositories WHERE id = $1 AND org_id = $2
	`, repoID, orgID).Scan(
		&repo.ID, &repo.OrgID, &repo.Name, &repo.FullName, &repo.Description, &repo.URL,
		&repo.CloneURL, &repo.SSHURL, &repo.DefaultBranch, &repo.Language, &repo.SizeBytes,
		&repo.StarsCount, &repo.ForksCount, &repo.WatchersCount, &repo.IsPrivate,
		&repo.IsArchived, &repo.IsTemplate, &repo.IsFork, &repo.ParentRepoID,
		&repo.CreatedAt, &repo.UpdatedAt, &lastSyncedAt, &repo.SyncStatus,
	)

	if err != nil {
		return nil, err
	}

	if lastSyncedAt.Valid {
		repo.LastSyncedAt = &lastSyncedAt.Time
	}

	return &repo, nil
}

// syncRepositoryHandler synchronizes repository data from external source
func syncRepositoryHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	repoID := chi.URLParam(r, "id")
	if repoID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Repository ID is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Update sync status to running
	_, err = db.Exec("UPDATE repositories SET sync_status = 'running', last_synced_at = NOW() WHERE id = $1 AND org_id = $2",
		repoID, org.ID)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "sync_status_update",
			Message:       "Failed to update sync status",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// Start background sync
	go performRepositorySync(repoID, org.ID)

	result := map[string]interface{}{
		"repository_id": repoID,
		"status":        "sync_started",
		"message":       "Repository synchronization has been initiated",
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// getRepositoryAnalysisHandler gets analysis results for a repository
func getRepositoryAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	_, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	repoID := chi.URLParam(r, "id")
	if repoID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Repository ID is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Parse query parameters
	analysisType := r.URL.Query().Get("type")
	limitStr := r.URL.Query().Get("limit")
	limit := 10
	if limitStr != "" {
		if parsedLimit, err := strconv.Atoi(limitStr); err == nil && parsedLimit > 0 && parsedLimit <= 50 {
			limit = parsedLimit
		}
	}

	// Build query
	query := `
		SELECT id, analysis_type, analysis_scope, result_data, confidence_score,
		       analyzed_at, duration_ms, status
		FROM repository_analysis
		WHERE repo_id = $1`
	args := []interface{}{repoID}
	argCount := 1

	if analysisType != "" {
		argCount++
		query += fmt.Sprintf(" AND analysis_type = $%d", argCount)
		args = append(args, analysisType)
	}

	query += " ORDER BY analyzed_at DESC LIMIT $" + strconv.Itoa(argCount+1)
	args = append(args, limit)

	rows, err := db.Query(query, args...)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "analysis_get",
			Message:       "Failed to get repository analysis",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}
	defer rows.Close()

	analyses := []map[string]interface{}{}
	for rows.Next() {
		var analysis struct {
			ID              string                 `json:"id"`
			AnalysisType    string                 `json:"analysis_type"`
			AnalysisScope   string                 `json:"analysis_scope"`
			ResultData      map[string]interface{} `json:"result_data"`
			ConfidenceScore float64                `json:"confidence_score"`
			AnalyzedAt      time.Time              `json:"analyzed_at"`
			DurationMs      int                    `json:"duration_ms"`
			Status          string                 `json:"status"`
		}

		var resultDataBytes []byte

		err := rows.Scan(&analysis.ID, &analysis.AnalysisType, &analysis.AnalysisScope,
			&resultDataBytes, &analysis.ConfidenceScore, &analysis.AnalyzedAt,
			&analysis.DurationMs, &analysis.Status)
		if err != nil {
			continue
		}

		json.Unmarshal(resultDataBytes, &analysis.ResultData)

		analyses = append(analyses, map[string]interface{}{
			"id":               analysis.ID,
			"analysis_type":    analysis.AnalysisType,
			"analysis_scope":   analysis.AnalysisScope,
			"result_data":      analysis.ResultData,
			"confidence_score": analysis.ConfidenceScore,
			"analyzed_at":      analysis.AnalyzedAt,
			"duration_ms":      analysis.DurationMs,
			"status":           analysis.Status,
		})
	}

	result := map[string]interface{}{
		"repository_id": repoID,
		"analyses":      analyses,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// analyzeRepositoryHandler performs analysis on a repository
func analyzeRepositoryHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	repoID := chi.URLParam(r, "id")
	if repoID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Repository ID is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	var req struct {
		AnalysisType string                 `json:"analysis_type"`
		Parameters   map[string]interface{} `json:"parameters,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	if req.AnalysisType == "" {
		req.AnalysisType = "complexity"
	}

	// Validate analysis type
	validTypes := []string{"complexity", "dependencies", "security", "performance", "maintainability"}
	valid := false
	for _, t := range validTypes {
		if req.AnalysisType == t {
			valid = true
			break
		}
	}
	if !valid {
		WriteErrorResponse(w, &ValidationError{
			Field:   "analysis_type",
			Message: "Invalid analysis type",
			Code:    "invalid_analysis_type",
		}, http.StatusBadRequest)
		return
	}

	// Start background analysis
	analysisID := uuid.New().String()
	go performRepositoryAnalysis(analysisID, repoID, org.ID, req.AnalysisType, req.Parameters)

	result := map[string]interface{}{
		"analysis_id":   analysisID,
		"repository_id": repoID,
		"analysis_type": req.AnalysisType,
		"status":        "analysis_started",
		"message":       "Repository analysis has been initiated",
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(result)
}

// createRepositoryRelationshipHandler creates a relationship between repositories
func createRepositoryRelationshipHandler(w http.ResponseWriter, r *http.Request) {
	ctx := r.Context()
	org, err := getOrgFromContext(ctx)
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "context",
			Message:       "Failed to get organization from context",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	var req struct {
		SourceRepoID     string                 `json:"source_repo_id"`
		TargetRepoID     string                 `json:"target_repo_id"`
		RelationshipType string                 `json:"relationship_type"`
		Strength         float64                `json:"strength,omitempty"`
		Metadata         map[string]interface{} `json:"metadata,omitempty"`
	}

	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		WriteErrorResponse(w, &ValidationError{
			Field:   "body",
			Message: "Invalid JSON",
			Code:    "invalid_json",
		}, http.StatusBadRequest)
		return
	}

	// Validate required fields
	if req.SourceRepoID == "" || req.TargetRepoID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "source_repo_id,target_repo_id",
			Message: "Source and target repository IDs are required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	if req.RelationshipType == "" {
		req.RelationshipType = "depends_on"
	}

	if req.Strength == 0 {
		req.Strength = 0.5
	}

	// Verify repositories exist and belong to organization
	var count int
	db.QueryRow("SELECT COUNT(*) FROM repositories WHERE id IN ($1, $2) AND org_id = $3",
		req.SourceRepoID, req.TargetRepoID, org.ID).Scan(&count)

	if count != 2 {
		WriteErrorResponse(w, &ValidationError{
			Field:   "source_repo_id,target_repo_id",
			Message: "One or both repositories do not exist or access denied",
			Code:    "invalid_repository",
		}, http.StatusBadRequest)
		return
	}

	// Check for existing relationship
	var existingID string
	db.QueryRow(`
		SELECT id FROM repository_relationships
		WHERE source_repo_id = $1 AND target_repo_id = $2
	`, req.SourceRepoID, req.TargetRepoID).Scan(&existingID)

	if existingID != "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "source_repo_id,target_repo_id",
			Message: "Relationship already exists",
			Code:    "duplicate_relationship",
		}, http.StatusConflict)
		return
	}

	// Create relationship
	relationshipID := uuid.New().String()
	metadataJSON, _ := json.Marshal(req.Metadata)
	now := time.Now()

	_, err = db.Exec(`
		INSERT INTO repository_relationships (
			id, source_repo_id, target_repo_id, relationship_type, strength,
			metadata, discovered_at, last_updated, confidence_score
		) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
	`, relationshipID, req.SourceRepoID, req.TargetRepoID, req.RelationshipType,
		req.Strength, metadataJSON, now, now, 0.8)

	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "relationship_create",
			Message:       "Failed to create repository relationship",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	relationship := RepositoryRelationship{
		ID:               relationshipID,
		SourceRepoID:     req.SourceRepoID,
		TargetRepoID:     req.TargetRepoID,
		RelationshipType: req.RelationshipType,
		Strength:         req.Strength,
		Metadata:         req.Metadata,
		DiscoveredAt:     now,
		LastUpdated:      now,
		ConfidenceScore:  0.8,
	}

	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(relationship)
}

// Cross-Repository Analysis Functions

func performRepositorySync(repoID, orgID string) {
	// Update sync status to running
	db.Exec("UPDATE repositories SET sync_status = 'running', last_synced_at = NOW() WHERE id = $1", repoID)

	// Simulate sync process (in real implementation, this would sync with GitHub/GitLab/etc.)
	time.Sleep(2 * time.Second)

	// Update sync status to completed
	db.Exec("UPDATE repositories SET sync_status = 'completed' WHERE id = $1", repoID)
}

func performRepositoryAnalysis(analysisID, repoID, orgID, analysisType string, parameters map[string]interface{}) {
	startTime := time.Now()

	// Update analysis status
	_, err := db.Exec(`
		INSERT INTO repository_analysis (
			id, repo_id, analysis_type, analysis_scope, status, analyzed_at
		) VALUES ($1, $2, $3, $4, $5, $6)
	`, analysisID, repoID, analysisType, "single", "running", startTime)

	if err != nil {
		fmt.Printf("Failed to create analysis record: %v\n", err)
		return
	}

	// Perform analysis based on type
	var resultData map[string]interface{}
	var confidenceScore float64

	switch analysisType {
	case "complexity":
		resultData, confidenceScore = analyzeRepositoryComplexity(repoID)
	case "dependencies":
		resultData, confidenceScore = analyzeRepositoryDependencies(repoID)
	case "security":
		resultData, confidenceScore = analyzeRepositorySecurity(repoID)
	case "performance":
		resultData, confidenceScore = analyzeRepositoryPerformance(repoID)
	case "maintainability":
		resultData, confidenceScore = analyzeRepositoryMaintainability(repoID)
	default:
		resultData = map[string]interface{}{"error": "Unknown analysis type"}
		confidenceScore = 0.0
	}

	duration := time.Since(startTime)

	// Update analysis with results
	resultDataJSON, _ := json.Marshal(resultData)
	_, err = db.Exec(`
		UPDATE repository_analysis
		SET result_data = $1, confidence_score = $2, duration_ms = $3, status = $4
		WHERE id = $5
	`, resultDataJSON, confidenceScore, duration.Milliseconds(), "completed", analysisID)

	if err != nil {
		fmt.Printf("Failed to update analysis results: %v\n", err)
	}
}

// Analysis functions (simplified implementations)
func analyzeRepositoryComplexity(repoID string) (map[string]interface{}, float64) {
	return map[string]interface{}{
		"cyclomatic_complexity": 15.7,
		"cognitive_complexity":  12.3,
		"maintainability_index": 78.5,
		"lines_of_code":         15420,
		"functions_count":       234,
	}, 0.85
}

func analyzeRepositoryDependencies(repoID string) (map[string]interface{}, float64) {
	return map[string]interface{}{
		"direct_dependencies":      12,
		"transitive_dependencies":  45,
		"outdated_dependencies":    3,
		"security_vulnerabilities": 1,
		"dependency_health_score":  82.3,
	}, 0.78
}

func analyzeRepositorySecurity(repoID string) (map[string]interface{}, float64) {
	return map[string]interface{}{
		"vulnerabilities_found": 2,
		"critical_issues":       0,
		"high_issues":           1,
		"medium_issues":         1,
		"security_score":        87.5,
	}, 0.92
}

func analyzeRepositoryPerformance(repoID string) (map[string]interface{}, float64) {
	return map[string]interface{}{
		"performance_score":      79.3,
		"slow_functions":         5,
		"memory_leaks":           0,
		"bottlenecks_identified": 3,
	}, 0.71
}

func analyzeRepositoryMaintainability(repoID string) (map[string]interface{}, float64) {
	return map[string]interface{}{
		"technical_debt_ratio": 18.7,
		"code_duplication":     12.3,
		"test_coverage":        73.8,
		"documentation_score":  65.2,
	}, 0.88
}

// Cross-Repository Network Analysis

func buildRepositoryNetwork(orgID string) (*RepositoryNetwork, error) {
	network := &RepositoryNetwork{
		Repositories:  make(map[string]*Repository),
		Relationships: []RepositoryRelationship{},
		Dependencies:  []CrossRepoDependency{},
		IsConnected:   true,
		AnalysisDate:  time.Now(),
	}

	// Get all repositories
	rows, err := db.Query(`
		SELECT id, name, full_name, description, url, language, size_bytes,
		       stars_count, forks_count, watchers_count, is_private, is_archived,
		       is_template, is_fork, parent_repo_id, created_at, updated_at
		FROM repositories
		WHERE org_id = $1 AND is_archived = false
	`, orgID)

	if err != nil {
		return nil, err
	}
	defer rows.Close()

	for rows.Next() {
		var repo Repository
		rows.Scan(&repo.ID, &repo.Name, &repo.FullName, &repo.Description, &repo.URL,
			&repo.Language, &repo.SizeBytes, &repo.StarsCount, &repo.ForksCount,
			&repo.WatchersCount, &repo.IsPrivate, &repo.IsArchived, &repo.IsTemplate,
			&repo.IsFork, &repo.ParentRepoID, &repo.CreatedAt, &repo.UpdatedAt)

		network.Repositories[repo.ID] = &repo
	}

	// Get relationships
	relRows, err := db.Query(`
		SELECT id, source_repo_id, target_repo_id, relationship_type, strength,
		       metadata, discovered_at, last_updated, confidence_score
		FROM repository_relationships
		WHERE source_repo_id IN (SELECT id FROM repositories WHERE org_id = $1)
	`, orgID)

	if err != nil {
		return nil, err
	}
	defer relRows.Close()

	for relRows.Next() {
		var rel RepositoryRelationship
		var metadataBytes []byte

		relRows.Scan(&rel.ID, &rel.SourceRepoID, &rel.TargetRepoID, &rel.RelationshipType,
			&rel.Strength, &metadataBytes, &rel.DiscoveredAt, &rel.LastUpdated, &rel.ConfidenceScore)

		json.Unmarshal(metadataBytes, &rel.Metadata)
		network.Relationships = append(network.Relationships, rel)
	}

	// Get cross-repo dependencies
	depRows, err := db.Query(`
		SELECT id, source_repo_id, target_repo_id, dependency_type, package_name,
		       version_constraint, source_file_path, source_line_number, confidence_score, last_detected, is_active
		FROM cross_repo_dependencies
		WHERE source_repo_id IN (SELECT id FROM repositories WHERE org_id = $1)
	`, orgID)

	if err != nil {
		return nil, err
	}
	defer depRows.Close()

	for depRows.Next() {
		var dep CrossRepoDependency
		depRows.Scan(&dep.ID, &dep.SourceRepoID, &dep.TargetRepoID, &dep.DependencyType,
			&dep.PackageName, &dep.VersionConstraint, &dep.SourceFilePath, &dep.SourceLineNumber,
			&dep.ConfidenceScore, &dep.LastDetected, &dep.IsActive)

		network.Dependencies = append(network.Dependencies, dep)
	}

	// Calculate centrality scores
	network.CentralityScores = calculateRepositoryCentrality(network)

	// Identify clusters
	network.Clusters = identifyRepositoryClusters(network)

	return network, nil
}

func calculateRepositoryCentrality(network *RepositoryNetwork) map[string]float64 {
	scores := make(map[string]float64)

	// Simple degree centrality calculation
	for repoID := range network.Repositories {
		degree := 0

		// Count incoming relationships
		for _, rel := range network.Relationships {
			if rel.TargetRepoID == repoID {
				degree += int(rel.Strength * 10)
			}
		}

		// Count outgoing relationships
		for _, rel := range network.Relationships {
			if rel.SourceRepoID == repoID {
				degree += int(rel.Strength * 10)
			}
		}

		// Count dependencies
		for _, dep := range network.Dependencies {
			if dep.SourceRepoID == repoID || dep.TargetRepoID == repoID {
				degree += 5
			}
		}

		scores[repoID] = float64(degree)
	}

	return scores
}

func identifyRepositoryClusters(network *RepositoryNetwork) [][]string {
	clusters := [][]string{}

	// Simple clustering based on shared dependencies (simplified algorithm)
	repoMap := make(map[string]bool)
	for repoID := range network.Repositories {
		repoMap[repoID] = true
	}

	visited := make(map[string]bool)

	for repoID := range repoMap {
		if !visited[repoID] {
			cluster := findConnectedRepositories(repoID, network, visited)
			if len(cluster) > 1 {
				clusters = append(clusters, cluster)
			}
		}
	}

	return clusters
}

func findConnectedRepositories(startRepoID string, network *RepositoryNetwork, visited map[string]bool) []string {
	cluster := []string{startRepoID}
	visited[startRepoID] = true

	queue := []string{startRepoID}

	for len(queue) > 0 {
		current := queue[0]
		queue = queue[1:]

		// Find connected repositories through relationships
		for _, rel := range network.Relationships {
			var connected string
			if rel.SourceRepoID == current {
				connected = rel.TargetRepoID
			} else if rel.TargetRepoID == current {
				connected = rel.SourceRepoID
			}

			if connected != "" && !visited[connected] {
				visited[connected] = true
				cluster = append(cluster, connected)
				queue = append(queue, connected)
			}
		}

		// Find connected repositories through dependencies
		for _, dep := range network.Dependencies {
			var connected string
			if dep.SourceRepoID == current {
				connected = dep.TargetRepoID
			} else if dep.TargetRepoID == current {
				connected = dep.SourceRepoID
			}

			if connected != "" && !visited[connected] {
				visited[connected] = true
				cluster = append(cluster, connected)
				queue = append(queue, connected)
			}
		}
	}

	return cluster
}

// Cross-Repository Impact Analysis

func analyzeCrossRepositoryImpact(changedRepoID, changeType string, orgID string) (*CrossRepoImpactAnalysis, error) {
	analysis := &CrossRepoImpactAnalysis{
		ID:                   uuid.New().String(),
		RepositoryID:         changedRepoID,
		ChangeDescription:    fmt.Sprintf("Change in repository: %s", changeType),
		AffectedRepositories: []RepositoryImpact{},
		AnalyzedAt:           time.Now(),
	}

	// Build repository network
	network, err := buildRepositoryNetwork(orgID)
	if err != nil {
		return nil, err
	}

	// Find directly affected repositories
	directlyAffected := findDirectlyAffectedRepositories(changedRepoID, network)

	// Analyze impact for each affected repository
	for _, repoID := range directlyAffected {
		if repo, exists := network.Repositories[repoID]; exists {
			impact := analyzeRepositoryImpact(repo, changedRepoID, network)
			analysis.AffectedRepositories = append(analysis.AffectedRepositories, impact)
		}
	}

	// Build dependency chain
	analysis.DependencyChain = findDependencyChain(changedRepoID, network)

	// Calculate overall risk level
	analysis.RiskLevel = calculateOverallRiskLevel(analysis.AffectedRepositories)

	// Generate risk factors
	analysis.RiskFactors = identifyCrossRepoRiskFactors(analysis, network)

	// Generate mitigation strategies
	analysis.MitigationStrategies = generateCrossRepoMitigationStrategies(analysis)

	// Calculate estimated impact time
	analysis.EstimatedImpactTime = calculateCrossRepoImpactTime(analysis.AffectedRepositories)

	// Calculate confidence score
	analysis.ConfidenceScore = calculateCrossRepoConfidence(analysis, network)

	return analysis, nil
}

func findDirectlyAffectedRepositories(changedRepoID string, network *RepositoryNetwork) []string {
	affected := make(map[string]bool)

	// Find repositories that depend on the changed repository
	for _, rel := range network.Relationships {
		if rel.SourceRepoID == changedRepoID {
			affected[rel.TargetRepoID] = true
		}
	}

	// Find repositories that the changed repository depends on
	for _, rel := range network.Relationships {
		if rel.TargetRepoID == changedRepoID {
			affected[rel.SourceRepoID] = true
		}
	}

	// Find repositories connected through dependencies
	for _, dep := range network.Dependencies {
		if dep.SourceRepoID == changedRepoID {
			affected[dep.TargetRepoID] = true
		}
		if dep.TargetRepoID == changedRepoID {
			affected[dep.SourceRepoID] = true
		}
	}

	// Convert map to slice
	result := make([]string, 0, len(affected))
	for repoID := range affected {
		result = append(result, repoID)
	}

	return result
}

func analyzeRepositoryImpact(repo *Repository, changedRepoID string, network *RepositoryNetwork) RepositoryImpact {
	impact := RepositoryImpact{
		RepositoryID:   repo.ID,
		RepositoryName: repo.Name,
		ImpactType:     "indirect",
		Severity:       "low",
		Description:    "Affected by changes in dependent repository",
		TimeImpact:     1, // Default 1 day
		Confidence:     0.7,
	}

	// Determine relationship type and severity
	for _, rel := range network.Relationships {
		if (rel.SourceRepoID == repo.ID && rel.TargetRepoID == changedRepoID) ||
			(rel.TargetRepoID == repo.ID && rel.SourceRepoID == changedRepoID) {

			switch rel.RelationshipType {
			case "depends_on":
				impact.ImpactType = "direct_dependency"
				impact.Severity = "medium"
				impact.Description = "Direct dependency relationship affected"
				impact.TimeImpact = 3
				impact.Confidence = 0.9
			case "imports_from":
				impact.ImpactType = "code_dependency"
				impact.Severity = "high"
				impact.Description = "Code import relationship affected"
				impact.TimeImpact = 5
				impact.Confidence = 0.95
			case "shares_interface":
				impact.ImpactType = "interface_dependency"
				impact.Severity = "high"
				impact.Description = "Shared interface affected"
				impact.TimeImpact = 7
				impact.Confidence = 0.85
			}

			// Build propagation path
			impact.PropagationPath = []string{changedRepoID, repo.ID}
			break
		}
	}

	// Check for dependency relationships
	for _, dep := range network.Dependencies {
		if (dep.SourceRepoID == repo.ID && dep.TargetRepoID == changedRepoID) ||
			(dep.TargetRepoID == repo.ID && dep.SourceRepoID == changedRepoID) {

			impact.ImpactType = "dependency_chain"
			if impact.Severity == "low" {
				impact.Severity = "medium"
			}
			impact.Description = fmt.Sprintf("Dependency chain affected: %s", dep.PackageName)
			impact.TimeImpact += 2
			impact.Confidence = math.Max(impact.Confidence, 0.8)
		}
	}

	return impact
}

func findDependencyChain(startRepoID string, network *RepositoryNetwork) []CrossRepoDependency {
	chain := []CrossRepoDependency{}

	// Find all dependency paths starting from the changed repository
	for _, dep := range network.Dependencies {
		if dep.SourceRepoID == startRepoID {
			chain = append(chain, dep)
		}
	}

	return chain
}

func calculateOverallRiskLevel(affectedRepos []RepositoryImpact) string {
	if len(affectedRepos) == 0 {
		return "low"
	}

	highCount := 0
	criticalCount := 0

	for _, impact := range affectedRepos {
		switch impact.Severity {
		case "high":
			highCount++
		case "critical":
			criticalCount++
		}
	}

	if criticalCount > 0 {
		return "critical"
	} else if highCount > len(affectedRepos)/2 {
		return "high"
	} else if highCount > 0 {
		return "medium"
	}

	return "low"
}

func identifyCrossRepoRiskFactors(analysis *CrossRepoImpactAnalysis, network *RepositoryNetwork) []string {
	risks := []string{}

	// Check for cascade effects
	if len(analysis.AffectedRepositories) > 5 {
		risks = append(risks, "High number of affected repositories - potential cascade failure")
	}

	// Check for critical repository impact
	criticalImpact := false
	for _, impact := range analysis.AffectedRepositories {
		if impact.Severity == "critical" {
			criticalImpact = true
			break
		}
	}
	if criticalImpact {
		risks = append(risks, "Critical repositories affected - business continuity at risk")
	}

	// Check for long dependency chains
	if len(analysis.DependencyChain) > 3 {
		risks = append(risks, "Long dependency chains increase propagation risk")
	}

	// Check for central repository impact
	for repoID, score := range network.CentralityScores {
		if score > 50 { // High centrality
			for _, impact := range analysis.AffectedRepositories {
				if impact.RepositoryID == repoID {
					risks = append(risks, fmt.Sprintf("Central repository '%s' affected - widespread impact expected", impact.RepositoryName))
					break
				}
			}
		}
	}

	return risks
}

func generateCrossRepoMitigationStrategies(analysis *CrossRepoImpactAnalysis) []string {
	strategies := []string{}

	if analysis.RiskLevel == "critical" {
		strategies = append(strategies, "Immediate rollback consideration for critical changes")
		strategies = append(strategies, "Activate incident response team")
		strategies = append(strategies, "Prepare communication plan for stakeholders")
	}

	if analysis.RiskLevel == "high" {
		strategies = append(strategies, "Parallel testing across affected repositories")
		strategies = append(strategies, "Staged deployment with rollback points")
		strategies = append(strategies, "Increased monitoring during deployment")
	}

	strategies = append(strategies, "Coordinate deployment across repository teams")
	strategies = append(strategies, "Prepare contingency plans for each affected repository")
	strategies = append(strategies, "Document change impact for future reference")

	return strategies
}

func calculateCrossRepoImpactTime(affectedRepos []RepositoryImpact) int {
	totalTime := 0
	for _, impact := range affectedRepos {
		totalTime += impact.TimeImpact
	}

	// Apply parallelization factor (some work can be done simultaneously)
	if len(affectedRepos) > 1 {
		totalTime = int(float64(totalTime) * 0.7) // 30% reduction for parallel work
	}

	return totalTime
}

func calculateCrossRepoConfidence(analysis *CrossRepoImpactAnalysis, network *RepositoryNetwork) float64 {
	confidence := 0.5 // Base confidence

	// Higher confidence with more relationship data
	if len(network.Relationships) > 0 {
		confidence += 0.2
	}

	if len(network.Dependencies) > 0 {
		confidence += 0.2
	}

	// Higher confidence for direct relationships
	directCount := 0
	for _, impact := range analysis.AffectedRepositories {
		if impact.ImpactType == "direct_dependency" {
			directCount++
		}
	}

	if directCount > 0 {
		confidence += 0.1
	}

	return math.Min(confidence, 1.0)
}

// =============================================================================
// Phase 5: API Versioning, Rate Limiting, and Documentation
// =============================================================================

// apiVersionMiddleware handles API versioning and deprecation
func apiVersionMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Extract version from header or URL path
		version := r.Header.Get("X-API-Version")

		// Check URL path for version (e.g., /api/v1/...)
		if version == "" {
			pathParts := strings.Split(strings.Trim(r.URL.Path, "/"), "/")
			if len(pathParts) >= 2 && pathParts[0] == "api" && strings.HasPrefix(pathParts[1], "v") {
				version = pathParts[1]
			}
		}

		// Default to v1 if no version specified
		if version == "" {
			version = "v1"
		}

		// Check version status and add deprecation warnings
		versionInfo, err := getVersionInfo(version)
		if err != nil {
			WriteErrorResponse(w, &DatabaseError{
				Operation:     "version_check",
				Message:       "Failed to check API version",
				OriginalError: err,
			}, http.StatusInternalServerError)
			return
		}

		if versionInfo == nil {
			// Get all supported versions for error message
			supportedVersions, _ := getSupportedVersions()
			versionStrings := make([]string, len(supportedVersions))
			for i, v := range supportedVersions {
				versionStrings[i] = v.Version
			}

			WriteErrorResponse(w, &ValidationError{
				Field:   "version",
				Message: fmt.Sprintf("API version '%s' is not supported. Supported versions: %s", version, strings.Join(versionStrings, ", ")),
				Code:    "unsupported_version",
			}, http.StatusBadRequest)
			return
		}

		// Add version to request context
		ctx := context.WithValue(r.Context(), "api_version", version)
		ctx = context.WithValue(ctx, "api_version_info", versionInfo)
		r = r.WithContext(ctx)

		// Add version header to response
		w.Header().Set("X-API-Version", version)

		// Add deprecation warnings if applicable
		if versionInfo.Status == "deprecated" {
			w.Header().Set("X-API-Deprecated", "true")
			w.Header().Set("X-API-Deprecated-Message", fmt.Sprintf("API version %s is deprecated", version))

			if versionInfo.SunsetDate != nil {
				w.Header().Set("X-API-Sunset-Date", versionInfo.SunsetDate.Format(time.RFC3339))
			}

			if versionInfo.DeprecatedBy != "" {
				w.Header().Set("X-API-Deprecated-By", versionInfo.DeprecatedBy)
			}

			// Add deprecation warning to response body (for JSON responses)
			w.Header().Set("Warning", fmt.Sprintf(`299 sentinel.dev "API version %s is deprecated"`, version))
		}

		if versionInfo.Status == "sunset" {
			w.Header().Set("X-API-Sunset", "true")
			WriteErrorResponse(w, &ValidationError{
				Field:   "version",
				Message: fmt.Sprintf("API version '%s' has reached end-of-life. Please migrate to %s", version, versionInfo.DeprecatedBy),
				Code:    "version_sunset",
			}, http.StatusGone)
			return
		}

		// Check for deprecated endpoints
		endpointInfo, err := getEndpointInfo(r.URL.Path, r.Method, version)
		if err == nil && endpointInfo != nil && endpointInfo.Deprecated {
			w.Header().Set("X-API-Endpoint-Deprecated", "true")

			deprecationMsg := fmt.Sprintf("Endpoint %s %s is deprecated in version %s",
				r.Method, r.URL.Path, version)

			if endpointInfo.ReplacementPath != "" {
				deprecationMsg += fmt.Sprintf(". Use %s instead", endpointInfo.ReplacementPath)
				w.Header().Set("X-API-Endpoint-Replacement", endpointInfo.ReplacementPath)
			}

			w.Header().Set("Warning", fmt.Sprintf(`299 sentinel.dev "%s"`, deprecationMsg))
		}

		next.ServeHTTP(w, r)
	})
}

// rateLimitMiddleware implements rate limiting
func rateLimitMiddleware(next http.Handler) http.Handler {
	// Simple in-memory rate limiter (in production, use Redis or similar)
	limits := make(map[string]*RateLimitInfo)
	var mu sync.RWMutex

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Get client identifier (IP address for now)
		clientIP := getClientIP(r)

		// Get current rate limit info
		now := time.Now()
		limit := RateLimit{
			Requests: 100,         // 100 requests
			Window:   time.Minute, // per minute
			Burst:    10,          // burst allowance
		}

		key := clientIP + ":" + r.Method + ":" + r.URL.Path

		mu.Lock()
		info, exists := limits[key]
		if !exists || now.After(info.ResetTime) {
			// Reset or create new limit info
			info = &RateLimitInfo{
				Limit:      limit.Requests,
				Remaining:  limit.Requests - 1,
				ResetTime:  now.Add(limit.Window),
				WindowSize: "1m",
			}
			limits[key] = info
		} else {
			// Check if limit exceeded
			if info.Remaining <= 0 {
				mu.Unlock()
				w.Header().Set("X-RateLimit-Limit", strconv.Itoa(info.Limit))
				w.Header().Set("X-RateLimit-Remaining", "0")
				w.Header().Set("X-RateLimit-Reset", strconv.FormatInt(info.ResetTime.Unix(), 10))
				w.Header().Set("Retry-After", strconv.Itoa(int(time.Until(info.ResetTime).Seconds())))

				rateLimitErr := &models.RateLimitError{
					Message:    "Rate limit exceeded",
					RetryAfter: int(time.Until(info.ResetTime).Seconds()),
					ResetTime:  info.ResetTime,
				}
				WriteErrorResponse(w, rateLimitErr, http.StatusTooManyRequests)
				return
			}

			// Decrement remaining requests
			info.Remaining--
		}
		mu.Unlock()

		// Add rate limit headers
		w.Header().Set("X-RateLimit-Limit", strconv.Itoa(info.Limit))
		w.Header().Set("X-RateLimit-Remaining", strconv.Itoa(info.Remaining))
		w.Header().Set("X-RateLimit-Reset", strconv.FormatInt(info.ResetTime.Unix(), 10))

		next.ServeHTTP(w, r)
	})
}

// cacheMiddleware implements response caching
func cacheMiddleware(next http.Handler) http.Handler {
	cache := make(map[string]*CacheEntry)
	var mu sync.RWMutex

	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		// Only cache GET requests
		if r.Method != http.MethodGet {
			next.ServeHTTP(w, r)
			return
		}

		// Generate cache key
		key := generateCacheKey(r)

		// Check cache
		mu.RLock()
		entry, exists := cache[key]
		mu.RUnlock()

		if exists && time.Now().Before(entry.ExpiresAt) {
			// Check if client has current version
			if ifNoneMatch := r.Header.Get("If-None-Match"); ifNoneMatch != "" && ifNoneMatch == entry.ETag {
				w.WriteHeader(http.StatusNotModified)
				return
			}

			// Return cached response
			for k, v := range entry.Headers {
				w.Header().Set(k, v)
			}
			w.Header().Set("X-Cache", "HIT")
			w.Header().Set("ETag", entry.ETag)
			w.WriteHeader(entry.StatusCode)
			w.Write(entry.Response)
			return
		}

		// Capture response for caching
		cw := &cachedResponseWriter{
			ResponseWriter: w,
			statusCode:     http.StatusOK,
			body:           &bytes.Buffer{},
		}

		next.ServeHTTP(cw, r)

		// Cache successful GET responses for 5 minutes
		if cw.statusCode >= 200 && cw.statusCode < 300 {
			entry := &CacheEntry{
				Key:        key,
				Response:   cw.body.Bytes(),
				Headers:    make(map[string]string),
				StatusCode: cw.statusCode,
				CreatedAt:  time.Now(),
				ExpiresAt:  time.Now().Add(5 * time.Minute),
				ETag:       fmt.Sprintf(`"%x"`, md5.Sum(cw.body.Bytes())),
			}

			// Copy relevant headers
			for k, v := range w.Header() {
				if shouldCacheHeader(k) {
					entry.Headers[k] = v[0]
				}
			}

			mu.Lock()
			cache[key] = entry
			mu.Unlock()

			w.Header().Set("X-Cache", "MISS")
			w.Header().Set("ETag", entry.ETag)
		}
	})
}

// cachedResponseWriter captures response data for caching
type cachedResponseWriter struct {
	http.ResponseWriter
	statusCode int
	body       *bytes.Buffer
}

func (cw *cachedResponseWriter) WriteHeader(code int) {
	cw.statusCode = code
	cw.ResponseWriter.WriteHeader(code)
}

func (cw *cachedResponseWriter) Write(data []byte) (int, error) {
	cw.body.Write(data)
	return cw.ResponseWriter.Write(data)
}

// getClientIP extracts client IP address from request
func getClientIP(r *http.Request) string {
	// Check X-Forwarded-For header first
	if xff := r.Header.Get("X-Forwarded-For"); xff != "" {
		// Take the first IP if there are multiple
		ips := strings.Split(xff, ",")
		return strings.TrimSpace(ips[0])
	}

	// Check X-Real-IP header
	if xri := r.Header.Get("X-Real-IP"); xri != "" {
		return xri
	}

	// Fall back to remote address
	ip, _, err := net.SplitHostPort(r.RemoteAddr)
	if err != nil {
		return r.RemoteAddr
	}
	return ip
}

// generateCacheKey creates a unique cache key for the request
func generateCacheKey(r *http.Request) string {
	// Include method, path, and relevant query parameters
	key := r.Method + ":" + r.URL.Path

	// Add sorted query parameters
	if len(r.URL.Query()) > 0 {
		params := make([]string, 0, len(r.URL.Query()))
		for k, v := range r.URL.Query() {
			params = append(params, k+"="+strings.Join(v, ","))
		}
		sort.Strings(params)
		key += "?" + strings.Join(params, "&")
	}

	// Include API version if present
	if version := r.Context().Value("api_version"); version != nil {
		key += ":v" + version.(string)
	}

	return fmt.Sprintf("%x", md5.Sum([]byte(key)))
}

// shouldCacheHeader determines if a header should be cached
func shouldCacheHeader(header string) bool {
	cacheableHeaders := map[string]bool{
		"Content-Type":   true,
		"Content-Length": true,
		"Last-Modified":  true,
		"ETag":           true,
		"Cache-Control":  true,
	}

	_, shouldCache := cacheableHeaders[header]
	return shouldCache
}

// API Versioning Handlers

// getAPIVersionsHandler returns supported API versions
func getAPIVersionsHandler(w http.ResponseWriter, r *http.Request) {
	versions, err := getSupportedVersions()
	if err != nil {
		WriteErrorResponse(w, &DatabaseError{
			Operation:     "versions_get",
			Message:       "Failed to get API versions",
			OriginalError: err,
		}, http.StatusInternalServerError)
		return
	}

	// If no versions in database, return default v1
	if len(versions) == 0 {
		versions = []APIVersion{
			{
				Version:   "v1",
				Status:    "active",
				Changelog: "Initial stable API version with full feature set",
			},
		}
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"versions": versions,
		"current":  "v1",
	})
}

// getAPIDocumentationHandler generates and returns API documentation
func getAPIDocumentationHandler(w http.ResponseWriter, r *http.Request) {
	version := r.URL.Query().Get("version")
	if version == "" {
		version = "v1"
	}

	docs := generateAPIDocumentation(version)

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(docs)
}

// getAPIStatsHandler returns API usage statistics
func getAPIStatsHandler(w http.ResponseWriter, r *http.Request) {
	// Parse time range
	timeRange := r.URL.Query().Get("range")
	if timeRange == "" {
		timeRange = "1h"
	}

	// In a real implementation, this would query actual metrics
	// For now, return mock data
	stats := APIStats{
		TotalRequests: 15420,
		RequestsByMethod: map[string]int64{
			"GET":    12000,
			"POST":   2500,
			"PUT":    800,
			"DELETE": 120,
		},
		RequestsByPath: map[string]int64{
			"/api/v1/tasks":        5000,
			"/api/v1/repositories": 3500,
			"/api/v1/documents":    2800,
			"/api/v1/analysis":     4120,
		},
		ErrorsByType: map[string]int64{
			"validation_error": 45,
			"not_found":        120,
			"database_error":   15,
			"rate_limit":       8,
		},
		ResponseTimeAvg: 245.7,
		ResponseTimeP95: 1200.0,
		ResponseTimeP99: 3500.0,
		TimeRange:       timeRange,
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(stats)
}

// getRateLimitStatusHandler returns current rate limit status
func getRateLimitStatusHandler(w http.ResponseWriter, r *http.Request) {
	// This is a simplified implementation
	// In production, you'd check actual rate limit status
	info := RateLimitInfo{
		Limit:      100,
		Remaining:  85,
		ResetTime:  time.Now().Add(time.Minute),
		WindowSize: "1m",
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(info)
}

// healthCheckHandler provides comprehensive health check
func healthCheckHandler(w http.ResponseWriter, r *http.Request) {
	health := map[string]interface{}{
		"status":    "healthy",
		"timestamp": time.Now(),
		"version":   "v1.0.0",
		"uptime":    "24h 30m", // Would be calculated from actual uptime
		"services": map[string]interface{}{
			"database": map[string]interface{}{
				"status":  "healthy",
				"latency": "12ms",
			},
			"cache": map[string]interface{}{
				"status":   "healthy",
				"hit_rate": "87.5%",
			},
			"storage": map[string]interface{}{
				"status":    "healthy",
				"used":      "2.4GB",
				"available": "47.6GB",
			},
		},
		"metrics": map[string]interface{}{
			"active_connections": 23,
			"total_requests":     15420,
			"error_rate":         "0.012%",
		},
	}

	// Check database connectivity
	if err := db.Ping(); err != nil {
		health["status"] = "unhealthy"
		if services, ok := health["services"].(map[string]interface{}); ok {
			if database, ok := services["database"].(map[string]interface{}); ok {
				database["status"] = "unhealthy"
				database["error"] = err.Error()
			}
		}
		w.WriteHeader(http.StatusServiceUnavailable)
	} else {
		w.WriteHeader(http.StatusOK)
	}

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(health)
}

// generateAPIDocumentation creates comprehensive API documentation
func generateAPIDocumentation(version string) *APIDocumentation {
	docs := &APIDocumentation{
		Title:       "Sentinel Hub API",
		Version:     version,
		Description: "Comprehensive API for code analysis, task management, and repository intelligence",
		BaseURL:     "https://api.sentinel.dev",
		GeneratedAt: time.Now(),
	}

	// Define API versions
	docs.Versions = []APIVersion{
		{
			Version:   "v1",
			Status:    "active",
			Changelog: "Initial stable API version with full feature set",
		},
	}

	// Define security schemes
	docs.Security = []APISecurity{
		{
			Type:        "Bearer Token",
			Description: "JWT Bearer token authentication",
			Scheme:      "Bearer",
		},
		{
			Type:        "API Key",
			Description: "API key authentication",
			Scheme:      "X-API-Key",
		},
	}

	// Collect all endpoints (this would be more comprehensive in production)
	docs.Endpoints = []APIEndpoint{
		// Task Management
		{
			Path:        "/api/v1/tasks",
			Method:      "GET",
			Description: "List tasks with filtering and pagination",
			Version:     "v1",
			Deprecated:  false,
			Tags:        []string{"tasks", "management"},
			Parameters: []APIParam{
				{Name: "status", Type: "string", Required: false, Description: "Filter by task status", Location: "query"},
				{Name: "limit", Type: "integer", Required: false, Description: "Maximum number of results", Location: "query"},
			},
			Responses: []APIResponse{
				{StatusCode: 200, Description: "Successful response", Schema: map[string]interface{}{"type": "array", "items": map[string]interface{}{"$ref": "#/components/schemas/Task"}}},
				{StatusCode: 400, Description: "Invalid parameters"},
			},
		},
		{
			Path:        "/api/v1/tasks",
			Method:      "POST",
			Description: "Create a new task",
			Version:     "v1",
			Deprecated:  false,
			Tags:        []string{"tasks", "management"},
			Parameters: []APIParam{
				{Name: "body", Type: "Task", Required: true, Description: "Task object", Location: "body"},
			},
			Responses: []APIResponse{
				{StatusCode: 201, Description: "Task created successfully"},
				{StatusCode: 400, Description: "Invalid task data"},
			},
		},
		// Repository Management
		{
			Path:        "/api/v1/repositories",
			Method:      "GET",
			Description: "List repositories with filtering",
			Version:     "v1",
			Deprecated:  false,
			Tags:        []string{"repositories", "management"},
			Parameters: []APIParam{
				{Name: "language", Type: "string", Required: false, Description: "Filter by programming language", Location: "query"},
				{Name: "limit", Type: "integer", Required: false, Description: "Maximum number of results", Location: "query"},
			},
			Responses: []APIResponse{
				{StatusCode: 200, Description: "Successful response"},
			},
		},
		// Document Processing
		{
			Path:        "/api/v1/documents/upload",
			Method:      "POST",
			Description: "Upload and process a document",
			Version:     "v1",
			Deprecated:  false,
			Tags:        []string{"documents", "processing"},
			Parameters: []APIParam{
				{Name: "file", Type: "file", Required: true, Description: "Document file to upload", Location: "form"},
				{Name: "type", Type: "string", Required: false, Description: "Document type override", Location: "form"},
			},
			Responses: []APIResponse{
				{StatusCode: 202, Description: "Document upload accepted for processing"},
				{StatusCode: 400, Description: "Invalid file or parameters"},
			},
		},
		// Analysis
		{
			Path:        "/api/v1/repositories/{id}/analyze",
			Method:      "POST",
			Description: "Run analysis on a repository",
			Version:     "v1",
			Deprecated:  false,
			Tags:        []string{"analysis", "repositories"},
			Parameters: []APIParam{
				{Name: "id", Type: "string", Required: true, Description: "Repository ID", Location: "path"},
				{Name: "analysis_type", Type: "string", Required: true, Description: "Type of analysis to run", Location: "body"},
			},
			Responses: []APIResponse{
				{StatusCode: 200, Description: "Analysis completed"},
				{StatusCode: 202, Description: "Analysis started (async)"},
			},
		},
		// System endpoints
		{
			Path:        "/api/v1/health",
			Method:      "GET",
			Description: "System health check",
			Version:     "v1",
			Deprecated:  false,
			Tags:        []string{"system", "monitoring"},
			Responses: []APIResponse{
				{StatusCode: 200, Description: "System is healthy"},
				{StatusCode: 503, Description: "System is unhealthy"},
			},
		},
	}

	return docs
}

// =============================================================================
// EMERGENCY STUBS - Phase 1 Compilation Fixes
// These are minimal implementations to enable compilation
// TODO: Implement proper error monitoring system in Phase 3
// =============================================================================

// ErrorSeverity represents error severity levels
type ErrorSeverity int

const (
	ErrorSeverityInfo ErrorSeverity = iota
	ErrorSeverityLow
	ErrorSeverityMedium
	ErrorSeverityHigh
	ErrorSeverityCritical
)

// ErrorClassification represents comprehensive error classification data
type ErrorClassification struct {
	Severity      ErrorSeverity          `json:"severity"`
	Category      string                 `json:"category"`
	Recovery      string                 `json:"recovery_strategy"`
	Retryable     bool                   `json:"retryable"`
	UserVisible   bool                   `json:"user_visible"`
	Context       map[string]interface{} `json:"context,omitempty"`
	Suggestions   []string               `json:"suggestions,omitempty"`
	RelatedErrors []string               `json:"related_errors,omitempty"`
	ErrorCode     int                    `json:"error_code"`
	Timestamp     time.Time              `json:"timestamp"`
	RequestID     string                 `json:"request_id,omitempty"`
	ToolName      string                 `json:"tool_name,omitempty"`
}

// getErrorDashboardData returns basic error dashboard data
func getErrorDashboardData() map[string]interface{} {
	return map[string]interface{}{
		"total_errors":  0,
		"error_rate":    0.0,
		"top_errors":    []string{},
		"error_trends":  map[string]int{},
		"system_health": "unknown",
		"last_updated":  time.Now().Format(time.RFC3339),
	}
}

// analyzeErrorPatterns returns basic error pattern analysis
func analyzeErrorPatterns(timeWindow time.Duration) map[string]interface{} {
	return map[string]interface{}{
		"patterns":        []string{},
		"recommendations": []string{},
		"severity_trends": map[string]int{},
		"error_clusters":  []map[string]interface{}{},
		"time_window":     timeWindow.String(),
	}
}

// classifyMCPError classifies an MCP error (stub implementation)
func classifyMCPError(code int, message string, originalErr error) ErrorClassification {
	return ErrorClassification{
		Severity:    ErrorSeverityMedium,
		Category:    "unknown",
		Recovery:    "manual_intervention",
		Retryable:   false,
		UserVisible: true,
		Context:     map[string]interface{}{"code": code, "message": message},
		Suggestions: []string{"Check error logs for more details"},
		ErrorCode:   code,
		Timestamp:   time.Now(),
	}
}

// getErrorStatistics returns basic error statistics
func getErrorStatistics() map[string]interface{} {
	return map[string]interface{}{
		"total_by_severity": map[string]int{"critical": 0, "high": 0, "medium": 0, "low": 0},
		"total_by_category": map[string]int{},
		"time_window":       "24h",
		"generated_at":      time.Now().Format(time.RFC3339),
	}
}

// trackError records an error for monitoring (stub implementation)
func trackError(classification ErrorClassification) {
	// Stub implementation - in production this would aggregate errors
	// TODO: Implement proper error tracking in Phase 3
}

// getSeverityLabel returns a string label for error severity
func getSeverityLabel(severity ErrorSeverity) string {
	switch severity {
	case ErrorSeverityCritical:
		return "critical"
	case ErrorSeverityHigh:
		return "high"
	case ErrorSeverityMedium:
		return "medium"
	case ErrorSeverityLow:
		return "low"
	case ErrorSeverityInfo:
		return "info"
	default:
		return "unknown"
	}
}

// calculateOverallSystemHealth calculates overall system health score (stub)
func calculateOverallSystemHealth() float64 {
	return 85.0 // Healthy system
}

// countTotalBottlenecks counts total performance bottlenecks (stub)
func countTotalBottlenecks() int {
	return 2 // Low number of bottlenecks
}

// getCurrentDegradationState returns current system degradation state (stub)
func getCurrentDegradationState() string {
	return "healthy"
}

// CircuitState represents circuit breaker state
type CircuitState string

const (
	CircuitClosed   CircuitState = "closed"
	CircuitOpen     CircuitState = "open"
	CircuitHalfOpen CircuitState = "half_open"
)

// CircuitBreaker represents a circuit breaker instance
type CircuitBreaker struct {
	Name             string
	State            CircuitState
	FailureCount     int
	LastFailure      time.Time
	SuccessCount     int
	FailureThreshold int
	RecoveryTimeout  time.Duration
	mutex            sync.RWMutex
}

var circuitBreakers = make(map[string]*CircuitBreaker)
var circuitBreakerMutex sync.RWMutex

// getCircuitBreaker returns a circuit breaker for a tool (stub)
func getCircuitBreaker(toolName string) *CircuitBreaker {
	circuitBreakerMutex.RLock()
	defer circuitBreakerMutex.RUnlock()

	if cb, ok := circuitBreakers[toolName]; ok {
		return cb
	}

	// Return nil if not found
	return nil
}

// getAllFallbackStrategies returns all fallback strategies (stub)
func getAllFallbackStrategies() []map[string]interface{} {
	return []map[string]interface{}{
		{
			"tool":              "default",
			"fallback_enabled":  true,
			"fallback_strategy": "cache",
		},
	}
}

// getFallbackStrategy returns fallback strategy for a tool (stub)
func getFallbackStrategy(toolName string) map[string]interface{} {
	return map[string]interface{}{
		"tool":              toolName,
		"fallback_enabled":  true,
		"fallback_strategy": "cache",
	}
}

// getCircuitBreakerSummary returns circuit breaker summary (stub)
func getCircuitBreakerSummary() map[string]interface{} {
	return map[string]interface{}{
		"total":     5,
		"open":      0,
		"half_open": 1,
		"closed":    4,
	}
}

// getDocumentStatusHandler returns document processing status (stub)
func getDocumentStatusHandler(w http.ResponseWriter, r *http.Request) {
	docID := chi.URLParam(r, "id")
	if docID == "" {
		WriteErrorResponse(w, &ValidationError{
			Field:   "id",
			Message: "Document ID is required",
			Code:    "required",
		}, http.StatusBadRequest)
		return
	}

	// Stub implementation - in production this would query the database
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"document_id": docID,
		"status":      "completed",
		"progress":    100,
		"api_version": "v1",
	})
}

// getExtractedTextHandler returns extracted text from a document (stub)
func getExtractedTextHandler(w http.ResponseWriter, r *http.Request) {
	docID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"document_id":    docID,
		"extracted_text": "",
		"api_version":    "v1",
	})
}

// getKnowledgeItemsHandler returns knowledge items from a document (stub)
func getKnowledgeItemsHandler(w http.ResponseWriter, r *http.Request) {
	docID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"document_id":     docID,
		"knowledge_items": []interface{}{},
		"api_version":     "v1",
	})
}

// detectChangesHandler detects changes in a document (stub)
func detectChangesHandler(w http.ResponseWriter, r *http.Request) {
	docID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"document_id": docID,
		"changes":     []interface{}{},
		"api_version": "v1",
	})
}

// listDocumentsHandler lists documents for a project (stub)
func listDocumentsHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"documents":   []interface{}{},
		"total":       0,
		"api_version": "v1",
	})
}

// searchDocumentsHandler searches documents (stub)
func searchDocumentsHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"results":     []interface{}{},
		"total":       0,
		"api_version": "v1",
	})
}

// complexityAnalysisHandler performs code complexity analysis (stub)
func complexityAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"complexity":  map[string]interface{}{},
		"api_version": "v1",
	})
}

// deadCodeAnalysisHandler performs dead code analysis (stub)
func deadCodeAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"dead_code":   []interface{}{},
		"api_version": "v1",
	})
}

// dependencyAnalysisHandler performs dependency analysis (stub)
func dependencyAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"dependencies": []interface{}{},
		"api_version":  "v1",
	})
}

// typeSafetyAnalysisHandler performs type safety analysis (stub)
func typeSafetyAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"type_safety": map[string]interface{}{},
		"api_version": "v1",
	})
}

// performanceAnalysisHandler performs performance analysis (stub)
func performanceAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"performance": map[string]interface{}{},
		"api_version": "v1",
	})
}

// getTaskDependencyGraphHandler returns task dependency graph (stub)
func getTaskDependencyGraphHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":     taskID,
		"graph":       map[string]interface{}{},
		"api_version": "v1",
	})
}

// getTaskExecutionPlanHandler returns task execution plan (stub)
func getTaskExecutionPlanHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":     taskID,
		"plan":        map[string]interface{}{},
		"api_version": "v1",
	})
}

// createTaskDependencyHandler creates a task dependency (stub)
func createTaskDependencyHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":     taskID,
		"dependency":  map[string]interface{}{},
		"api_version": "v1",
	})
}

// deleteTaskDependencyHandler deletes a task dependency (stub)
func deleteTaskDependencyHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	dependencyID := chi.URLParam(r, "dependency_id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":       taskID,
		"dependency_id": dependencyID,
		"deleted":       true,
		"api_version":   "v1",
	})
}

// getTaskImpactAnalysisHandler returns task impact analysis (stub)
func getTaskImpactAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":     taskID,
		"impact":      map[string]interface{}{},
		"api_version": "v1",
	})
}

// analyzeTaskImpactHandler analyzes task impact (stub)
func analyzeTaskImpactHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":     taskID,
		"analysis":    map[string]interface{}{},
		"api_version": "v1",
	})
}

// getTaskChangeHistoryHandler returns task change history (stub)
func getTaskChangeHistoryHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":     taskID,
		"changes":     []interface{}{},
		"api_version": "v1",
	})
}

// getAllTaskChangesHandler returns all task changes (stub)
func getAllTaskChangesHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"changes":     []interface{}{},
		"total":       0,
		"api_version": "v1",
	})
}

// getImpactReportHandler returns impact report (stub)
func getImpactReportHandler(w http.ResponseWriter, r *http.Request) {
	taskID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"task_id":     taskID,
		"report":      map[string]interface{}{},
		"api_version": "v1",
	})
}

// getRepositoryNetworkHandler returns repository network (stub)
func getRepositoryNetworkHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"network":     map[string]interface{}{},
		"api_version": "v1",
	})
}

// getCrossRepoImpactAnalysisHandler returns cross-repo impact analysis (stub)
func getCrossRepoImpactAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analysis":    map[string]interface{}{},
		"api_version": "v1",
	})
}

// analyzeCrossRepoImpactHandler analyzes cross-repo impact (stub)
func analyzeCrossRepoImpactHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analysis":    map[string]interface{}{},
		"api_version": "v1",
	})
}

// listRepositoryRelationshipsHandler lists repository relationships (stub)
func listRepositoryRelationshipsHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"relationships": []interface{}{},
		"total":         0,
		"api_version":   "v1",
	})
}

// getRepositoryRelationshipsHandler returns repository relationships (stub)
func getRepositoryRelationshipsHandler(w http.ResponseWriter, r *http.Request) {
	repoID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"repository_id": repoID,
		"relationships": []interface{}{},
		"api_version":   "v1",
	})
}

// deleteRepositoryRelationshipHandler deletes a repository relationship (stub)
func deleteRepositoryRelationshipHandler(w http.ResponseWriter, r *http.Request) {
	repoID := chi.URLParam(r, "id")
	relationshipID := chi.URLParam(r, "relationship_id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"repository_id":   repoID,
		"relationship_id": relationshipID,
		"deleted":         true,
		"api_version":     "v1",
	})
}

// createCrossRepoDependencyHandler creates a cross-repo dependency (stub)
func createCrossRepoDependencyHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"dependency":  map[string]interface{}{},
		"api_version": "v1",
	})
}

// listCrossRepoDependenciesHandler lists cross-repo dependencies (stub)
func listCrossRepoDependenciesHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"dependencies": []interface{}{},
		"total":        0,
		"api_version":  "v1",
	})
}

// getRepositoryDependenciesHandler returns repository dependencies (stub)
func getRepositoryDependenciesHandler(w http.ResponseWriter, r *http.Request) {
	repoID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"repository_id": repoID,
		"dependencies":  []interface{}{},
		"api_version":   "v1",
	})
}

// deleteCrossRepoDependencyHandler deletes a cross-repo dependency (stub)
func deleteCrossRepoDependencyHandler(w http.ResponseWriter, r *http.Request) {
	dependencyID := chi.URLParam(r, "dependency_id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"dependency_id": dependencyID,
		"deleted":       true,
		"api_version":   "v1",
	})
}

// createCrossRepoAnalysisHandler creates cross-repo analysis (stub)
func createCrossRepoAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analysis":    map[string]interface{}{},
		"api_version": "v1",
	})
}

// listCrossRepoAnalysesHandler lists cross-repo analyses (stub)
func listCrossRepoAnalysesHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analyses":    []interface{}{},
		"total":       0,
		"api_version": "v1",
	})
}

// getCrossRepoAnalysisHandler returns cross-repo analysis (stub)
func getCrossRepoAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	analysisID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analysis_id": analysisID,
		"analysis":    map[string]interface{}{},
		"api_version": "v1",
	})
}

// getCrossRepoAnalysisStatusHandler returns cross-repo analysis status (stub)
func getCrossRepoAnalysisStatusHandler(w http.ResponseWriter, r *http.Request) {
	analysisID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analysis_id": analysisID,
		"status":      "completed",
		"api_version": "v1",
	})
}

// cancelCrossRepoAnalysisHandler cancels cross-repo analysis (stub)
func cancelCrossRepoAnalysisHandler(w http.ResponseWriter, r *http.Request) {
	analysisID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"analysis_id": analysisID,
		"cancelled":   true,
		"api_version": "v1",
	})
}

// analyzeRepositoryImpactHandler analyzes repository impact (stub)
func analyzeRepositoryImpactHandler(w http.ResponseWriter, r *http.Request) {
	repoID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"repository_id": repoID,
		"impact":        map[string]interface{}{},
		"api_version":   "v1",
	})
}

// getRepositoryImpactHandler returns repository impact (stub)
func getRepositoryImpactHandler(w http.ResponseWriter, r *http.Request) {
	repoID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"repository_id": repoID,
		"impact":        map[string]interface{}{},
		"api_version":   "v1",
	})
}

// getRepositoryCentralityHandler returns repository centrality (stub)
func getRepositoryCentralityHandler(w http.ResponseWriter, r *http.Request) {
	repoID := chi.URLParam(r, "id")
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"repository_id": repoID,
		"centrality":    map[string]interface{}{},
		"api_version":   "v1",
	})
}

// getRepositoryClustersHandler returns repository clusters (stub)
func getRepositoryClustersHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"clusters":    []interface{}{},
		"api_version": "v1",
	})
}

// getSystemHealthScore returns system health score (stub)
func getSystemHealthScore() float64 {
	return 92.5
}

// =============================================================================
// ADDITIONAL EMERGENCY STUBS - Phase 1 Compilation Fixes
// =============================================================================

// GetWorkflowExecution retrieves a workflow execution (stub implementation)
func GetWorkflowExecution(executionID string) (*models.WorkflowExecution, error) {
	return nil, fmt.Errorf("workflow execution retrieval not implemented")
}

// CancelWorkflowExecution cancels a workflow execution (stub implementation)
func CancelWorkflowExecution(executionID string) error {
	return fmt.Errorf("workflow execution cancellation not implemented")
}

// getPerformanceDashboard returns performance dashboard data (stub implementation)
func getPerformanceDashboard() map[string]interface{} {
	return map[string]interface{}{
		"total_requests":     0,
		"avg_response_time":  0.0,
		"error_rate":         0.0,
		"throughput":         0.0,
		"top_endpoints":      []string{},
		"performance_trends": map[string]interface{}{},
	}
}

// ToolResponse represents a tool response with error/result
type ToolResponse struct {
	Error  *models.MCPError       `json:"error,omitempty"`
	Result map[string]interface{} `json:"result,omitempty"`
}

// ToolResponse represents a tool response with error/result
type WorkflowExecution = models.WorkflowExecution

// ToolPerformanceMetrics represents performance metrics for a tool
type ToolPerformanceMetrics struct {
	TotalExecutions   int       `json:"total_executions"`
	AverageDuration   float64   `json:"average_duration_ms"`
	SuccessRate       float64   `json:"success_rate"`
	ErrorRate         float64   `json:"error_rate"`
	PerformanceScore  float64   `json:"performance_score"` // 0-100
	LastExecutionTime time.Time `json:"last_execution_time"`
}

// WorkflowPerformanceMetrics represents performance metrics for a workflow
type WorkflowPerformanceMetrics struct {
	TotalExecutions   int       `json:"total_executions"`
	AverageDuration   float64   `json:"average_duration_ms"`
	SuccessRate       float64   `json:"success_rate"`
	ErrorRate         float64   `json:"error_rate"`
	PerformanceScore  float64   `json:"performance_score"` // 0-100
	LastExecutionTime time.Time `json:"last_execution_time"`
}

// PerformanceMonitor represents the performance monitoring system
type PerformanceMonitor struct {
	SystemSnapshots []interface{}                          `json:"system_snapshots"`
	ToolMetrics     map[string]*ToolPerformanceMetrics     `json:"tool_metrics"`
	WorkflowMetrics map[string]*WorkflowPerformanceMetrics `json:"workflow_metrics"`
	Optimizations   map[string]interface{}                 `json:"optimizations"`
}

// handleToolPerformanceMetrics handles tool performance metrics (stub implementation)
func handleToolPerformanceMetrics(toolName string, args map[string]interface{}) ToolResponse {
	return ToolResponse{
		Result: map[string]interface{}{
			"message": "Tool performance metrics not implemented",
			"status":  "stub",
			"tool":    toolName,
			"args":    args,
		},
	}
}

// InternalServerError represents an internal server error
type InternalServerError struct {
	Message       string `json:"message"`
	OriginalError error  `json:"-"`
}

// Error returns the error message
func (e *InternalServerError) Error() string {
	return e.Message
}

// handleWorkflowPerformanceMetrics handles workflow performance metrics (stub implementation)
func handleWorkflowPerformanceMetrics(workflowID string, args map[string]interface{}) ToolResponse {
	return ToolResponse{
		Result: map[string]interface{}{
			"message":     "Workflow performance metrics not implemented",
			"status":      "stub",
			"workflow_id": workflowID,
			"args":        args,
		},
	}
}

// handlePerformanceOptimizationRecommendations handles performance optimization recommendations (stub implementation)
func handlePerformanceOptimizationRecommendations(args map[string]interface{}) ToolResponse {
	return ToolResponse{
		Result: map[string]interface{}{
			"message": "Performance optimization recommendations not implemented",
			"status":  "stub",
			"args":    args,
		},
	}
}

// handleAnalyzePerformanceBottlenecks analyzes performance bottlenecks (stub implementation)
func handleAnalyzePerformanceBottlenecks(args map[string]interface{}) ToolResponse {
	return ToolResponse{
		Result: map[string]interface{}{
			"message": "Performance bottleneck analysis not implemented",
			"status":  "stub",
			"args":    args,
		},
	}
}

// =============================================================================
// WORKFLOW MANAGEMENT STUBS - Phase 1 Compilation Fixes
// These are minimal implementations to enable compilation
// TODO: Implement proper workflow orchestration in Phase 4
// =============================================================================

// WorkflowStatus represents workflow execution status
type WorkflowStatus string

const (
	WorkflowStatusPending   WorkflowStatus = "pending"
	WorkflowStatusRunning   WorkflowStatus = "running"
	WorkflowStatusCompleted WorkflowStatus = "completed"
	WorkflowStatusFailed    WorkflowStatus = "failed"
	WorkflowStatusCancelled WorkflowStatus = "cancelled"
)

// WorkflowDefinition represents a workflow definition
type WorkflowDefinition struct {
	ID           string                 `json:"id"`
	Name         string                 `json:"name"`
	Description  string                 `json:"description"`
	Version      string                 `json:"version"`
	Steps        []WorkflowStep         `json:"steps"`
	InputSchema  map[string]interface{} `json:"input_schema"`
	OutputSchema map[string]interface{} `json:"output_schema"`
	CreatedAt    time.Time              `json:"created_at"`
	UpdatedAt    time.Time              `json:"updated_at"`
}

// WorkflowStep represents a step in a workflow
type WorkflowStep struct {
	ID         string                 `json:"id"`
	Name       string                 `json:"name"`
	ToolName   string                 `json:"tool_name"`
	Arguments  map[string]interface{} `json:"arguments"`
	DependsOn  []string               `json:"depends_on,omitempty"`
	Condition  string                 `json:"condition,omitempty"`
	Timeout    time.Duration          `json:"timeout,omitempty"`
	RetryCount int                    `json:"retry_count,omitempty"`
}

// WorkflowExecution is now defined in models/workflow.go

// ListWorkflows returns a list of workflows (stub implementation)
func ListWorkflows() []WorkflowDefinition {
	return []WorkflowDefinition{}
}

// GetWorkflow retrieves a workflow by ID (stub implementation)
func GetWorkflow(id string) (*WorkflowDefinition, error) {
	return nil, fmt.Errorf("workflow not found: %s", id)
}

// RegisterWorkflow registers a new workflow (stub implementation)
func RegisterWorkflow(workflow WorkflowDefinition) error {
	return fmt.Errorf("workflow registration not implemented")
}

// ExecuteWorkflow executes a workflow (stub implementation)
func ExecuteWorkflow(workflowID string, input map[string]interface{}, requestID string) (*models.WorkflowExecution, error) {
	return nil, fmt.Errorf("workflow execution not implemented")
}

// ListWorkflowExecutions returns workflow executions (stub implementation)
func ListWorkflowExecutions(workflowID string, status models.WorkflowStatus, limit int) []models.WorkflowExecution {
	return []WorkflowExecution{}
}
